{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Index Welcome to the Engineering Documentation site! This is an engineering-focused repository for docs, design, and other technical knowledge. This version of the site contains selectively published docs, derived from the version we deploy internally. Both versions are updated by the same Continuous Deployment pipeline, so changes will end up in both automatically. Please use the top nav or the search box to find what you\u2019re looking for.","title":"Index"},{"location":"#index","text":"Welcome to the Engineering Documentation site! This is an engineering-focused repository for docs, design, and other technical knowledge. This version of the site contains selectively published docs, derived from the version we deploy internally. Both versions are updated by the same Continuous Deployment pipeline, so changes will end up in both automatically. Please use the top nav or the search box to find what you\u2019re looking for.","title":"Index"},{"location":"Conventions/api-convention/","text":"API Design Conventions https://stackoverflow.com/c/atlastechnology/articles/55","title":"API Design Conventions"},{"location":"Conventions/api-convention/#api-design-conventions","text":"https://stackoverflow.com/c/atlastechnology/articles/55","title":"API Design Conventions"},{"location":"Conventions/coding-convention/","text":"C# Coding Conventions https://stackoverflow.com/c/atlastechnology/articles/56","title":"C# Coding Conventions"},{"location":"Conventions/coding-convention/#c-coding-conventions","text":"https://stackoverflow.com/c/atlastechnology/articles/56","title":"C# Coding Conventions"},{"location":"Conventions/docker-and-k8s-conventions/","text":"Docker and Kubernetes Conventions https://stackoverflow.com/c/atlastechnology/articles/57","title":"Docker and Kubernetes Conventions"},{"location":"Conventions/docker-and-k8s-conventions/#docker-and-kubernetes-conventions","text":"https://stackoverflow.com/c/atlastechnology/articles/57","title":"Docker and Kubernetes Conventions"},{"location":"Conventions/examples-constructor/","text":"Constructor Parameter Examples https://stackoverflow.com/c/atlastechnology/articles/56","title":"Constructor Parameter Examples"},{"location":"Conventions/examples-constructor/#constructor-parameter-examples","text":"https://stackoverflow.com/c/atlastechnology/articles/56","title":"Constructor Parameter Examples"},{"location":"Conventions/examples-readonly-property/","text":"Readonly Property https://stackoverflow.com/c/atlastechnology/articles/56","title":"Readonly Property"},{"location":"Conventions/examples-readonly-property/#readonly-property","text":"https://stackoverflow.com/c/atlastechnology/articles/56","title":"Readonly Property"},{"location":"Conventions/naming-convention/","text":"Naming Conventions To keep things consistent across various efforts and systems, a convention for the naming of various identifiers is important. This document lays them out clearly, by identifier type. Repository Naming Conventions Naming the repositories under a DevOps project has no explicit conventions. Instead, a requirement is that a convention be set by the project team(s). The decided convention should result in names that are consistent and concise. Solution Naming Conventions A \u201csolution\u201d here refers not just to Visual Studio solutions ( .sln ) but to any IDE/language environment where several modules are encapsulated by a single container. The naming convention for these containers is straightforward: <project-name>.<component-name> . The component-name part refers to which component in the system is represented by this collection of modules. Optionally, the company name can be prefixed as well, as in <company-name>.<project-name>.<component-name> . If selected, this should be consistent across all solutions in the entire DevOps project (even across repos). Some examples: Atlas.AlertingStudio.AuthAPI SelfServiceStudio.SchemaManagement Often, in more microservice-oriented systems, solutions will have fewer modules within them, which reduces the need for broadly applicable names that identify an entire component of a system. Module Naming Conventions When naming modules, use a convention similar to naming solutions: <project-name>.<component-name>.<module-name> . Again, company name can be prefixed but should be consistent across all solutions in the DevOps project if selected. You have the following 4 types of projects: API \u2014 Your interface that people use to reach your application. For a Service or Daemon you can rename this to Service . For example: Atlas.WorkQueue.Api or Atlas.WorkQueue.Services . Services \u2014 This is where rubber meets the road. Some people prefer to just skip the extension, which is fine. So e.g. Atlas.WorkQueue . Data \u2014 This is where your interface to data persistence lives. If you\u2019re using an http restful endpoint or whatever it\u2019s all the same; data is data, regardless of source. Contracts \u2014 This is where your interfaces, POCOs, and other shareable pieces live. Should take ZERO dependencies on anything that isn\u2019t provided by the framework. Tests \u2014 Tests. Use these to help inform the names of your modules. Examples: Atlas.AlertingStudio.AuthAPI.Service SelfServiceStudio.SchemaManagement.Contracts Atlas.AlertingStudio.AuthAPI.Tests Anti-Patterns in Module Naming DO NOT name your module any of the following: Helper or variations Frequently, helper classes are used as shortcuts or patches to smooth over problems with application structure. Additionally, calling something a \u201chelper\u201d doesn\u2019t provide any detail as to what the module is helping with and what that \u201chelp\u201d is meant to achieve. Utilities or variations These are almost always static classes whose names provide no documentation of intent. Very often, they indicate a failure to perform proper dependency injection, as taking a hard dependency on a static class goes against the principle of inversion of control. Shared or variations For example, naming a module SharedLib does nothing to imply the purpose of the module. Why is it being shared? What does it provide? Often, modules with names like these are being used to patch over separation of concerns issues by incorrectly or inappropriately sharing code between other modules. Usage of these names almost always indicates a failure in design, as referenced in the sub-points above. Code Object and Variable Naming Conventions These will differ between various programming languages. However, avoid certain patterns such as: Hungarian notation Other type-prefixed or postfixed names Identifiers should indicate intent/purpose, not language semantics, unless their purpose is specifically related to language semantics (e.g. ConvertRowToTuple<T>() ). Single-letter names, or heavily abbreviated identifiers Code is meant to be readable by humans C# conventions can be found here . Python conventions can be found here . For languages that do not have explicit conventions, follow general guidelines laid out here and reach out to your team lead(s) for more information and to create a convention. Anti-Patterns in Object Naming All of the rules that apply to module naming apply here as well.","title":"Naming Conventions"},{"location":"Conventions/naming-convention/#naming-conventions","text":"To keep things consistent across various efforts and systems, a convention for the naming of various identifiers is important. This document lays them out clearly, by identifier type.","title":"Naming Conventions"},{"location":"Conventions/naming-convention/#repository-naming-conventions","text":"Naming the repositories under a DevOps project has no explicit conventions. Instead, a requirement is that a convention be set by the project team(s). The decided convention should result in names that are consistent and concise.","title":"Repository Naming Conventions"},{"location":"Conventions/naming-convention/#solution-naming-conventions","text":"A \u201csolution\u201d here refers not just to Visual Studio solutions ( .sln ) but to any IDE/language environment where several modules are encapsulated by a single container. The naming convention for these containers is straightforward: <project-name>.<component-name> . The component-name part refers to which component in the system is represented by this collection of modules. Optionally, the company name can be prefixed as well, as in <company-name>.<project-name>.<component-name> . If selected, this should be consistent across all solutions in the entire DevOps project (even across repos). Some examples: Atlas.AlertingStudio.AuthAPI SelfServiceStudio.SchemaManagement Often, in more microservice-oriented systems, solutions will have fewer modules within them, which reduces the need for broadly applicable names that identify an entire component of a system.","title":"Solution Naming Conventions"},{"location":"Conventions/naming-convention/#module-naming-conventions","text":"When naming modules, use a convention similar to naming solutions: <project-name>.<component-name>.<module-name> . Again, company name can be prefixed but should be consistent across all solutions in the DevOps project if selected. You have the following 4 types of projects: API \u2014 Your interface that people use to reach your application. For a Service or Daemon you can rename this to Service . For example: Atlas.WorkQueue.Api or Atlas.WorkQueue.Services . Services \u2014 This is where rubber meets the road. Some people prefer to just skip the extension, which is fine. So e.g. Atlas.WorkQueue . Data \u2014 This is where your interface to data persistence lives. If you\u2019re using an http restful endpoint or whatever it\u2019s all the same; data is data, regardless of source. Contracts \u2014 This is where your interfaces, POCOs, and other shareable pieces live. Should take ZERO dependencies on anything that isn\u2019t provided by the framework. Tests \u2014 Tests. Use these to help inform the names of your modules. Examples: Atlas.AlertingStudio.AuthAPI.Service SelfServiceStudio.SchemaManagement.Contracts Atlas.AlertingStudio.AuthAPI.Tests","title":"Module Naming Conventions"},{"location":"Conventions/naming-convention/#anti-patterns-in-module-naming","text":"DO NOT name your module any of the following: Helper or variations Frequently, helper classes are used as shortcuts or patches to smooth over problems with application structure. Additionally, calling something a \u201chelper\u201d doesn\u2019t provide any detail as to what the module is helping with and what that \u201chelp\u201d is meant to achieve. Utilities or variations These are almost always static classes whose names provide no documentation of intent. Very often, they indicate a failure to perform proper dependency injection, as taking a hard dependency on a static class goes against the principle of inversion of control. Shared or variations For example, naming a module SharedLib does nothing to imply the purpose of the module. Why is it being shared? What does it provide? Often, modules with names like these are being used to patch over separation of concerns issues by incorrectly or inappropriately sharing code between other modules. Usage of these names almost always indicates a failure in design, as referenced in the sub-points above.","title":"Anti-Patterns in Module Naming"},{"location":"Conventions/naming-convention/#code-object-and-variable-naming-conventions","text":"These will differ between various programming languages. However, avoid certain patterns such as: Hungarian notation Other type-prefixed or postfixed names Identifiers should indicate intent/purpose, not language semantics, unless their purpose is specifically related to language semantics (e.g. ConvertRowToTuple<T>() ). Single-letter names, or heavily abbreviated identifiers Code is meant to be readable by humans C# conventions can be found here . Python conventions can be found here . For languages that do not have explicit conventions, follow general guidelines laid out here and reach out to your team lead(s) for more information and to create a convention.","title":"Code Object and Variable Naming Conventions"},{"location":"Conventions/naming-convention/#anti-patterns-in-object-naming","text":"All of the rules that apply to module naming apply here as well.","title":"Anti-Patterns in Object Naming"},{"location":"Conventions/python-coding-conventions/","text":"Python Coding Conventions Generally speaking, follow Pythonic coding styles and guidelines. This document is designed to point you in the right direction and propose a few approaches to coding to maintain consistency. Project Conventions Projects must be designed and organized in a specific way to ensure they are both docker compatible and they will continue to function going forward. PEP-518 lays out a new project format to describe project dependencies. In addition PEP-517 describes a very-likely to pass way to handle and build project dependencies. As part of adapting to the changing landscape we are changing from the pipenv tool to poetry . Pipenv no longer fits in the broader vision of how Python dependencies are managed and never provided much of a deployment vision, either. Poetry , on the other hand, maintains a somewhat opinioned vision of how to build and deploy Python applications and libraries both that is consistent with the future of Python (already integrates the pyproject.toml ) as well as intends to provide standardized hooks for build and deployment. Projects must be small and single-purposed. Projects should always be hosted in a git repository. All projects should follow the below structure replacing app with the name of the package: . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 subpackage \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 somefile.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 pyproject.toml Note that the pyproject sits one level above the actual code. This is very similar to, like, C/C++ projects or Go projects where you\u2019d typically have your project file sitting above the source files. Ideally this should be a more familiar and considerably easier to build format. Any Web API service should be exposed through FastAPI All developers should use poetry for environment creation and management. All docker images should build from the python-poetry or python-poetry-fastapi images in Harbor Packages should be deployed and pulled using the Azure Devops artifacts repository Sensitive data like passwords and users should be read out of configuration (secrets, files, environment variables) and not be checked in to source control. Naming and Style Conventions Follow the style guidelines as provided in PEP8 . For purposes of convention, prefer single quotes to double quotes in anything beyond multi-line strings. Any time in a project that doesn\u2019t follow convention then follow the general conventions of that project (e.g. Overlord). Classes vs. Modules vs. Packages A single file should not contain more than 1 public class Use classes to represent encapsulation of both behavior and state. Use classes as a mixin object to inject functionality in to another class. Use a single Module when you have a single piece of behavior. Use a Package to combine modules or packages based on domain. Avoid name-pollution by always having all python in a top-level Package . Favor generators, list comprehensions, and iterators over classes if it is clean to do so . Prefer readability and usability over dogmatic adherance to rules. Dependency Injection For classes, favor mixins for dependency injection. See here for a discussion on the topic . For functions, pass functions as parameters (service injection). Environment and Setup All packages should maintain compatibility with pip All python projects must be self-contained and may only rely on pip and python to run. A library should be just installable through poetry install {package} A non-docker runnable app should have a self-contained script that sets up the environment A docker app will not require anything special as long as it uses python-poetry or something based on python-poetry Cross Platform Assume python projects will be installed cross-platform. Always use os module functions to do things like path construction and file manipulation. Type Annotations Functions and methods should be annotated as much as possible Docstrings Docstrings are required for any code being exposed to anyone outside Atlas. Docstrings should be used for complex integrations with external tools. Docstrings are a nice to have for any internal code.","title":"Python Coding Conventions"},{"location":"Conventions/python-coding-conventions/#python-coding-conventions","text":"Generally speaking, follow Pythonic coding styles and guidelines. This document is designed to point you in the right direction and propose a few approaches to coding to maintain consistency.","title":"Python Coding Conventions"},{"location":"Conventions/python-coding-conventions/#project-conventions","text":"Projects must be designed and organized in a specific way to ensure they are both docker compatible and they will continue to function going forward. PEP-518 lays out a new project format to describe project dependencies. In addition PEP-517 describes a very-likely to pass way to handle and build project dependencies. As part of adapting to the changing landscape we are changing from the pipenv tool to poetry . Pipenv no longer fits in the broader vision of how Python dependencies are managed and never provided much of a deployment vision, either. Poetry , on the other hand, maintains a somewhat opinioned vision of how to build and deploy Python applications and libraries both that is consistent with the future of Python (already integrates the pyproject.toml ) as well as intends to provide standardized hooks for build and deployment. Projects must be small and single-purposed. Projects should always be hosted in a git repository. All projects should follow the below structure replacing app with the name of the package: . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 subpackage \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 somefile.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 pyproject.toml Note that the pyproject sits one level above the actual code. This is very similar to, like, C/C++ projects or Go projects where you\u2019d typically have your project file sitting above the source files. Ideally this should be a more familiar and considerably easier to build format. Any Web API service should be exposed through FastAPI All developers should use poetry for environment creation and management. All docker images should build from the python-poetry or python-poetry-fastapi images in Harbor Packages should be deployed and pulled using the Azure Devops artifacts repository Sensitive data like passwords and users should be read out of configuration (secrets, files, environment variables) and not be checked in to source control.","title":"Project Conventions"},{"location":"Conventions/python-coding-conventions/#naming-and-style-conventions","text":"Follow the style guidelines as provided in PEP8 . For purposes of convention, prefer single quotes to double quotes in anything beyond multi-line strings. Any time in a project that doesn\u2019t follow convention then follow the general conventions of that project (e.g. Overlord).","title":"Naming and Style Conventions"},{"location":"Conventions/python-coding-conventions/#classes-vs-modules-vs-packages","text":"A single file should not contain more than 1 public class Use classes to represent encapsulation of both behavior and state. Use classes as a mixin object to inject functionality in to another class. Use a single Module when you have a single piece of behavior. Use a Package to combine modules or packages based on domain. Avoid name-pollution by always having all python in a top-level Package . Favor generators, list comprehensions, and iterators over classes if it is clean to do so . Prefer readability and usability over dogmatic adherance to rules.","title":"Classes vs. Modules vs. Packages"},{"location":"Conventions/python-coding-conventions/#dependency-injection","text":"For classes, favor mixins for dependency injection. See here for a discussion on the topic . For functions, pass functions as parameters (service injection).","title":"Dependency Injection"},{"location":"Conventions/python-coding-conventions/#environment-and-setup","text":"All packages should maintain compatibility with pip All python projects must be self-contained and may only rely on pip and python to run. A library should be just installable through poetry install {package} A non-docker runnable app should have a self-contained script that sets up the environment A docker app will not require anything special as long as it uses python-poetry or something based on python-poetry","title":"Environment and Setup"},{"location":"Conventions/python-coding-conventions/#cross-platform","text":"Assume python projects will be installed cross-platform. Always use os module functions to do things like path construction and file manipulation.","title":"Cross Platform"},{"location":"Conventions/python-coding-conventions/#type-annotations","text":"Functions and methods should be annotated as much as possible","title":"Type Annotations"},{"location":"Conventions/python-coding-conventions/#docstrings","text":"Docstrings are required for any code being exposed to anyone outside Atlas. Docstrings should be used for complex integrations with external tools. Docstrings are a nice to have for any internal code.","title":"Docstrings"},{"location":"Misc/configure-workstation-aks/","text":"Steps to Configure Workstation for Azure Kubernetes (AKS) https://stackoverflow.com/c/atlastechnology/articles/50","title":"Steps to Configure Workstation for Azure Kubernetes (AKS)"},{"location":"Misc/configure-workstation-aks/#steps-to-configure-workstation-for-azure-kubernetes-aks","text":"https://stackoverflow.com/c/atlastechnology/articles/50","title":"Steps to Configure Workstation for Azure Kubernetes (AKS)"},{"location":"Misc/distributed-architecture-strategy/","text":"Distributed Architecture Strategy Much of our new design is oriented around a much more distributed architecture than we\u2019ve normally used at ATLAS. While we are a little late to the party, that also means we have the chance to learn from our forebears and not make the same mistakes. Many people think of microservices when they think of distributed architecture. While this is the primary implementation today, distributed architectures have been around since the dawn of computing. They can take many different forms, each with their own pros and cons. At ATLAS, we should be hesitant to refer to our version as \u201cmicroservices\u201d\u2014not out of dislike for the pattern, necessarily, but because the term brings several implications with it. Being realistic, a full-on microservice-oriented architecture will be challenging for us to achieve simply due to our staffing limitations. Throughout this document (and any time we discuss architecture going forward), the terms \u201cmicroservices\u201d and \u201cdistributed architecture\u201d will be used fairly interchangeably. Documentation should emphasize the ideal architecture, even if in practice we must make some sacrifices and compromises. Table of Contents Distributed Architecture Overview Important Terms Microservices Example When to use Distributed Architecture Why Distributed Architecture? Granularity Granularization Example Documentation Documentation Centralization Documentation Automation Other Automation Documentation Prioritization Application Design Philosophy The 10-Hops Rule Fault Tolerance Avoiding Retry Storms Communication Contracts Deployment Tactics and Service Management Service-Level Agreements (SLAs) and Metrics Good SLAs vs. Bad SLAs SLAs and Microservices The Stoplight Strategy The Stoplight Strategy In Practice CI and CD Example CI and CD Workflow Summary Important Terms In the scope of this document, there are some terms we should set definitions for to avoid confusion. Granularity : a measure of the degree of separation, isolation, and individualization given to units of a system. A system can be said to be highly granular when its implementation involves many small, distinct units working together. Service : any discrete, resident application whose purpose is to react in some functional way to requests or events. (This is called out to denote a difference between what is considered a \u201cservice\u201d here\u2014just the conceptual object\u2014and the more concrete patterns we usually take for granted when we discuss services.) Robustness : the measure of a process\u2019s ability to cope with erroneous input or faults gracefully. Uptime of scale : Rather than the literal uptime of a given process, uptime of scale is the healthiness of a distributed system as measured by its adherence to steady state over a window of time. In other words, the uptime of scale is the measurement of a given system\u2019s ability to sustainably maintain its scale. This differs from availability in that a service can still be available while experiencing a low uptime of scale . Microservices Example A microservice-oriented architecture might propose that the operation \u201crun a report\u201d (as it exists in current ATLAS, newer designs notwithstanding) consists of several distinct services. It may have services for: Loading metadata and compiling the report (e.g. AdHocs) Performing validation ( IsValid ) Performing SQL replacements ( RunDynamicSql ) Executing report SQL Executing post-processing Performing Excel exporting Delivering results Each of these services are currently just steps of the monolithic report pipeline. However, due to the monolithic design, each of these steps has innumerable cross-cutting concerns. Worse, this logic has far-reaching and omniscient dependencies throughout that service. As a result, the entire report pipeline incurs the risk of regression even if only a single step needs to change. Additionally, even a transient failure at any single step causes the entire pipeline to fail. Exactly how granular your services should go is a subject of hot debate in the field still to this day. One could easily imagine each of those steps being further broken into smaller services: a service for delivering FTP results, a service for delivering email results, a service for executing BI reports, a service for executing AdHoc reports\u2014the list goes on. This is part of the hesitation to use the word \u201cmicroservices\u201d when describing our distributed architecture. While there could be value in going more granular with these services, that value should be measured during design and balanced against the increase in complexity and overhead. When to use Distributed Architecture Distributed architecture is a great solution to many problems, but it does not solve every problem. Certainly, it isn\u2019t the right solution at all for some problems. These distributed service patterns, like microservices, bring with them a decent amount of overhead and complexity. If you select them to solve your design problems, you are making a trade-off. The amount of benefit you gain can be conceptually represented as a \u201cmultiplier\u201d that depends on your problem. Some problems have huge multipliers\u2014basically anything that needs to scale quickly and widely, or anything which needs highly isolated parallelism. Some problems have near-zero multipliers, or even negative multipliers. It is very important, before deciding to go with microservices or similar architectures, that you evaluate your problem\u2019s multiplier. Read the Why Distributed Architecture? section to help figure out how much your particular engineering problem will benefit from distribution. As an example: relatively simple applications, temporary solutions, quick fixes, dirty hacks, and some client applications have very low multipliers. From a mindset perspective, it\u2019s probably wise to always begin with a more monolithic design. Only break it down when you discover the advantages of doing so outweigh the disadvantages (more on that strategy later). That said, don\u2019t be afraid of choosing distributed architecture where it makes sense. This section exists more to warn you against using it for trivial applications or forcing it to apply to problems which resist it. There is no mandate that says \u201call projects going forward must be designed using a distributed architecture,\u201d but we now have the infrastructure to support it more robustly than ever before. Instead, this document should serve as a helpful teaching tool and guide for why you might want to use these patterns, and how to use them successfully. Why Distributed Architecture? Detractors of microservices as a pattern often cite that the complexity of the pattern isn\u2019t worth the gains. The claim is that this complexity increases exponentially as the number of services increases, yet in order to \u201cdo it right,\u201d one must create highly granular services\u2014which, of course, leads to high complexity. It\u2019s certainly irrefutable that system complexity grows exponentially as the number of services increases. However, the granularity of the system is a variable that can be tweaked to avoid complexity. As with any architecture, you measure the strengths and the weaknesses and choose the right one for the job. Distribution is not the correct solution for every problem, but it\u2019s very effective at solving certain types of problems. Here are some of the benefits of choosing a distributed architecture. If your business problem aligns well with these benefits, it increases that conceptual multiplier . These are the reasons you might choose this architecture over others: Reduction of friction in development and deployment Improves the development and debugging experience by isolating effort into a single module Tends to make deployments deterministic and far simpler, more robust Enables Continuous Deployment, which ensures a fast stream of improvements and a quick time-to-fix for defects CD is challenging with monolithic applications because deployment takes so long and regressions can be very broad. A single microservice can build and deploy in seconds, and issues are usually isolated to that one service In our ecosystem, we\u2019re looking at time improvements measured in hours for many defects; the test, deploy, re-test cycle for a fix is significantly expedited Zero-downtime CD is possible, if the infrastructure is designed correctly Isolation of responsibilities and concerns at the module level Lends to better designs, more future-proof APIs, and fewer defects Allows a much greater ability to test modules individually, which increases code quality and robustness Scalability Makes our business more flexible Allows us to alleviate bottlenecks and improve customer experience as needed with almost no spin-up time Reduces friction between dev and ops teams Sharing of components and services Greatly speeds development and enhances products with less dev effort Multiplies improvements across all participating products Infrastructure and language freedom Services can be written in the best language for the job By design, a microservice should be as infrastructure-independent as possible to increase its motility Reduces cost to the business Separation into distinct services leads to more complete applications Microservices are first-class applications with their own resiliency, high-availability strategies, fault tolerance, error handling, and so on This not only makes applications more resilient, it also tends to improve visibility and logging/metrics Enables cross-functional, smaller teams to be more productive Without monoliths full of omniscient dependencies, it\u2019s much easier for any developer to work on any piece of the puzzle This reduces costs and increases developer utilization Reduction in business risk due to better facilitation of agile development Since each piece of an application is discrete, agile teams can produce easily deliverable units Modules that change as a result of agile feedback loops are easily replaced with minimal lost effort Granularity This is it: the million dollar question. How granular should your services be? How much should you break up functional units into individual services versus grouping them at higher levels of abstraction? Sadly, there is no easy answer. Instead, for every use case, you\u2019ll need to evaluate the advantages and disadvantages to find a good balance point. Advantages: Every level of granularity is another unit of scaling, high-availability, and logical separation Each granular service introduces another level of reusability that other services can opt into Each granular service is easily self-contained, increasing efficiency, robustness, and project management flexibility Application failures tend to be isolated to single small, replaceable, easily fixable units Service failures almost never cause system downtime once the functional units are granular enough Parts of an application will continue to operate even if others are down Disadvantages: Every level of granularity increases complexity, eventually to the point of making the architecture incomprehensible Even with extensive documentation, building a mental map of a highly-granular microservice-oriented application requires a great effort Each granular service is another item that must be managed, maintained, developed, and owned Each granular service introduces additional overhead on a request, especially in sequential operations Application failures are more challenging to track down due to the high number of interconnecting parts The guideline for granularity is pretty simple, though it\u2019s more philosophical than scientific. Make every unit as granular as you can, so long as you are able to amplify the advantages and minimize the disadvantages. If going more granular no longer amplifies the advantages, or raises the disadvantages to an uncomfortable level, your unit is granular enough. One could imagine, especially as we first dip our toes into the waters of distributed architecture, we\u2019ll be more uncomfortable than we need to be about the disadvantages. While we definitely must be mindful of them, we must also remember this is a new paradigm for us; we\u2019ll need to change the way we think. As you\u2019ll see later, there are tools available to help us mitigate the disadvantages. Granularization Example Earlier, the monolithic process of running a report in current ATLAS was broken down into 7 services. What if it was further broken down and made more granular? Here\u2019s how that might look: Metadata services: AdHoc Metadata Service User Report Metadata Service BI Report Metadata Service Validation services: AdHoc validation service BI Report validation service Pre-processing services: Tokenization service SQL transformation service Object filter service AdHoc compilation service Report Execution service Post-processing service Formatting services: Excel formatting service PDF formatting service Access formatting service Delivery services: Web delivery service Email delivery service SharePoint delivery service FTP delivery service Coordinator service We\u2019ve gone from 1 monolith, to 7 services, to 19 services. Is there value in making the process so fine-grained? Let\u2019s revisit the advantages. We can now finely control exactly how much of every kind of work we can perform. We run far more BI reports, for example, so we could ensure we have far more BI Report Metadata service instances than AdHoc Metadata instances. Reporting Studio and other projects could use whichever pieces of this pipeline work for them. One project could use all of it, except it might not allow exporting to Access or delivery via FTP. It may instead add cloud storage delivery. It could use a separate coordinator that knows how to integrate the services for its use case, but still consumes many of these same services. One Email delivery service could crash, but there are at least two instances running at all times. Other delivery types are unaffected. All FTP delivery services could simultaneously crash, but other delivery services keep functioning. The FTP service fix for the crash affects only the FTP service instances. Each of these smaller services could be developed by a different engineer. So long as the contracts between services are well-defined (and they must be no matter how granular the scale), the developer\u2019s scope of work is greatly narrowed. The small unit can be more easily and completely tested. Sounds amazing so far. It\u2019s super scalable, robust, smooth to develop, and highly reusable. This design checks all of the boxes, right? Well, let\u2019s take a look at the disadvantages too. There are nineteen services. Keeping track of what each of them do, where they live, how they\u2019re deployed, their communication contracts, the health of each set of instances, and where along the path a given request currently lies are engineering problems all their own. A service had to be introduced (the coordinator) to manage the lifecycle of a request, further increasing complexity. This example scenario can occur: \u201cAn FTP service instance just crashed.\u201d What happened to the work it was doing? It probably got re-queued. \u201cNow every FTP service instance has crashed.\u201d Which message killed the FTP services? The logging and correlation metadata had better be very strong; otherwise, prepare to spend hours digging around trying to figure out what happened. \u201cI found the request that killed the FTP instances, but I need to debug through the code to figure out the fix.\u201d You must either spin up nineteen services on your local machine to step through the process in its entirety\u2026 \u2026turn on some kind of remote debugging (which is insecure and not going to happen)\u2026 \u2026or grab the input for just this particular service, stand it alone up on your workstation, and feed the message through manually. The latter option will almost always be the correct answer. Each of these services introduces a communication overhead . A given request in this design is likely to go through 9 hops before finishing. Assuming a network latency of 5ms and a general API server latency of 5ms, that\u2019s 10ms per hop. We\u2019ve added almost 100ms in latency to this request just for communication. Each of these services introduces a resource overhead . A given service instance probably needs a minimum 100 MB of RAM dedicated to it just for framework overhead and runtime. Given that we want to have many instances alive for distributing workloads and maintaining a high availability, let\u2019s assume we have a total of 100 instances running between all 19 services. That\u2019s almost 10 GB of RAM dedicated to overhead. Each of these services introduces a development overhead . Getting started on each service involves the creation of a new repository, and in some cases, a new project through the PMO. Someone has to own this project and manage its deployments. There is always some amount of boilerplate code or configuration for setting up a service, even trivial services like a basic website serving plain HTML. Without any additional tools to help, this sounds rough. However, Kubernetes was created to help mitigate these disadvantages. It is the key to minimizing them. With Kubernetes, we can let go of the need to know and manage where each service lives. We can define how they\u2019re deployed via script and get deterministic deployments. We can easily monitor the health of the deployment. We can minimize network overhead and cap resource overhead. K8s alone helps remove a lot of the pain from this 19-service design. On top of that, there are several other tools and methodologies we can employ to reduce the impact, such as serverless (functional) paradigms. Is the 19-service design worth choosing, then? Well, realistically, the best design is probably somewhere between the 7-service and 19-service ones. Splitting validation into two services probably does not amplify the microservice advantages in practice. The metadata services aren\u2019t likely to benefit from the scalability of being so granular. Reports today don\u2019t even support FTP or SharePoint delivery. No matter the scale, though, you\u2019re going to run into some of these roadblocks eventually. The remainder of this article is going to help you address the disadvantages and roadblocks of microservices at scale. We will cover those tools, methodologies, and patterns that make developing microservices much more fool-proof and allow us to squeeze every last drop of value out of them. Documentation Unsurprisingly, the number one strategy to deal with the unwieldy nature of microservices is to produce reliable documentation. Lack of documentation is a serious business risk that doesn\u2019t get enough attention no matter the architecture. Not only is the chance of introducing regressions into existing systems higher, but the tribal knowledge that went into developing and maintaining legacy applications is often lost forever. Documentation helps during development as well: the more informed the team members are about the design, APIs, and philosophy behind them, the better decisions they will make during development. It makes developers more efficient, less stressed, and more cohesive. The ugly, unavoidable truth is that documentation is expensive. Producing it\u2014especially when a product or application is being developed by a small and/or tight-knit team\u2014is often unattractive. Frankly, even going through a formal design process at all is sometimes expensive, as developers spend tens of hours expressing on paper what they feel they already know in their minds. Being given the task of writing documentation is seen as \u201cgetting stuck\u201d with it; no one wants to do it. Worst of all, it requires constant maintenance to keep current, as the only thing more harmful than no documentation is bad documentation. Given that it\u2019s so expensive and cumbersome, yet so important to the success of distributed applications, how can we reduce the cost and still achieve a high quality of documentation? The solution is simple: centralization , automation , and prioritization . Documentation Centralization The most aggravating thing about keeping documentation current is that it usually lives in multiple places. Code lives in Git, but documentation often lives in some combination of OneNote, SharePoint, and a Word doc in an e-mail someone sent you 6 months ago. As a result, you find yourself flipping between several different places trying to keep track of them all. This wastes significant time, usually tens of minutes per instance as you have to rediscover where everything lives. If documentation were centralized and organized, however, it would be easier to update and effortless to reference. To make sure that the documentation is not only discoverable, but free of administrative roadblocks, here are the recommendations for centralizing documentation: Documentation is published to a central site. All engineers in the company have access to the entire site. The docs are not walled off by teams. This allows people to answer their own questions where possible. The published documents live in a location wholly dedicated to presenting documentation and nothing else. Any existing docs should be migrated to the site over time. The pages are logically ordered by projects, domain, and other meaningful categories that minimize navigation time. Documentation is centrally searchable. At a company of our scale, it should be reasonable to centralize all technical documentation into a single location. If the company grows significantly, the sheer amount of documentation could become impossible for any organization scheme to manage. At that point (and only at that point) we can consider breaking the documentation out into logical sub-units. Documentation Automation You\u2019ll notice that this very document lives in a Git repository. It is automatically published via CD to a documentation site, and it\u2019s written in Markdown. Each of these is critically important to the upkeep of this document. Storing documentation in Git : Storing docs in Git allows several people to edit the same document and merge changes together trivially. It enables deployment integrations. It provides free version control. Everyone\u2019s been in a situation where the design document was \u201cProduct Design V4 Final - Final.docx\u201d and it\u2019s not helpful. Git is a toolset many developers are already familiar with. While Git itself is not the requirement, it happens to fulfill these requirements very well. Automatic, continuous deployment to the central site : Documentation is a living thing. It evolves, especially during development. Making sure the latest version is always available from an authoritative location is critical to the success of the documentation effort. Decisions made against an outdated copy of the documentation are bad decisions by default. Markdown : Markdown itself isn\u2019t the requirement. The true requirement is that documentation be written in a universal, easy-to-write text-based markup language. It must have low friction and lots of tooling in its ecosystem, but still support enough features to be useful. HTML is another example option. Although, one could argue that it fails to meet the \u201ceasy-to-write\u201d requirement for someone just trying to hash out some docs. Markdown is the best of both worlds: HTML support when you need it, easy and intuitive syntax when you don\u2019t. Markdown\u2019s ecosystem is also how we\u2019re able to so effortlessly publish the documentation site. In this case, the tool mkdocs generates a site with top-nav and left-nav automatically prepared for you. The particular combination of Git + TFS deployment + Markdown isn\u2019t necessarily the best option. However, it is how this document was published, and this document was published using the same principles that will make other documentation efforts successful. Many products exist which provide the same advantages (e.g. Confluence); feel free to use them. Just keep centralization in mind: the docs need to be in one place. Other Automation Aside from the automatic deployment of documents to a centralized location, there are other things we can automate to make doc maintenance simpler. Some of these tools we\u2019re already familiar with and some of them will be new to us, but each of them is critical to sustainable docs. Most important, probably, is the documentation of code itself. Self-documenting code is already a staple of good engineering. When documentation must be written, however, putting the documentation in the code itself is natural for developers. It\u2019s more likely to get updated when the functionality changes. Taking it one step further, publishing it automatically can be achieved via several tools (often built into modern IDEs and compilers, like JavaDoc and C#\u2019s XML documentation). The automatically-generated documentation from code comments is great for internal reference, but what about externally-facing docs? Sometimes the internal docs work for external consumers as well, but often you\u2019ll want to add some additional elements to API documentation, such as sample code or visualizations. This is where tools like Swagger UI come into play. Often, in cases like this where external customers will be consuming the documentation, it\u2019s also wise to treat the docs as a deliverable themselves rather than a byproduct of development. You\u2019ll want to devote some time to polishing them up and making sure they\u2019re accurate; then, rely on automation to keep them up-to-date. Documentation Prioritization You\u2019re probably already practicing centralization and automation to some degree. The real trick to help documentation efforts be successful is this one: prioritization. Specifically, acknowledging that some forms of documentation are inherently less valuable than others, and should be given less effort. Before we can discuss priority, let\u2019s look at the distinct varieties of docs. These definitions aren\u2019t exactly standardized, but should certainly be agreeable. Architectural documentation \u2014 lays out the concept behind the systems, how they work together, and what the system should achieve. Ensures that the system meets the project requirements. Usually purely conceptual. Design documentation \u2014 concretely defines the patterns and idioms to be used in the implementation of the architectural design. Dives into the details rarely, but still paints a solid picture of what the final code will look like. Often combines the conceptual design with practical solutions. Implementation design \u2014 The collection of interfaces, code modules, services, and solutions that will be created to reflect the design documentation. Usually purely practical. Code documentation \u2014 comments on methods, parameters, and modules that expose intent and usage guidelines for code. API documentation \u2014 details the purpose and parameters of an API for internal or external consumers, including the API\u2019s communication models. Sometimes, the API documentation and the code documentation are the same thing. There are certainly more, but these are the most common and most relevant to our discussion. This philosophy prioritizes documentation in this order: Architectural documentation and API documentation. Design documentation. Code documentation. Implementation design. Implementation design is the least valuable type of documentation. Implementation can change constantly, quickly, and sometimes unpredictably (e.g. due to hotfixes). Keeping an implementation design current is essentially equal effort to its corresponding development. It\u2019s not worth spending the time to carefully document the code so closely, even after the fact. Code documentation is very important, perhaps the most important type of documentation for a product\u2019s technical success. However, it falls low on the list of priorities because it should be happening anyway . No extra effort should be expended to create it. If developers are not documenting their code well, or not writing self-documenting code, actions should be taken to correct the deficiency. When done well, it also serves much the same purpose as the implementation design. Developers should be able to trivially reverse-engineer the meaning and purpose of any component or unit. Design documentation and architectural documentation are important because they both capture intangible elements of the design that cannot be easily discovered otherwise. Tribal knowledge, business decisions, design philosophies, and historical context are all made available through these documents. Essentially, they act as the rubric by which the implementation is measured. In an imaginary worst case scenario where the working implementation was lost, you should only need these two documents to produce a new one. It might not be exactly the same, but it will meet the same requirements. Of the two, design documentation is less important than the architectural documentation only because the design can change without affecting the architecture. Imagine the design docs as an implementation of an interface in C#, where the interface is the architecture. API documentation , then, is the only purely-practical piece of documentation that holds a high priority. Much of this effort is spent making sure that your software, which is meant to be consumable since it has an API, is easy to integrate. Maintenance effort spent here, unlike in implementation design, is always worthwhile as it directly contributes to the quality of the product. Additionally, a well-designed API should change fairly rarely (even though the implementation might change constantly), minimizing the upkeep. Further reducing the upkeep are tools that can automatically generate API documentation or assist in creating it, which is never possible for implementation design. Application Design Philosophy In the distributed architecture world, it helps to think of application design a little differently. Often, we tend to think of applications as expressions of a product or feature set. Especially when coming from a monolithic n-Tier Architecture background, it\u2019s easy to see an application as similar to an engineer itself: when given a problem, it figures out how to solve the entire problem and then solves it. For distributed architectures, it\u2019s better to instead think of each service as part of an assembly line. The service is responsible for taking the input (let\u2019s say a car door) and producing an output (attaching the door to the car). Its only responsibility is to take doors and attach them to cars. In fact, it doesn\u2019t even really care about the car or the door; it just attaches door-like things to objects on the expected attachment points. If the attachment points don\u2019t exist on either the \u201cdoor\u201d or the \u201ccar\u201d, there\u2019s a problem. As we\u2019ll learn later on, even the attachment points can be somewhat flexible. If the door has fewer attachment points than the car, but they still line up, the service might attach them anyway. If the door has more attachment points than the car, but the most important points still line up and the door still fits, the service might attach them anyway. This ensures that we don\u2019t need to retrain the service (i.e. rewrite code) when the door or car change slightly. The service can continue trucking along. The 10-Hops Rule The 10-Hops Rule is a guideline invented to help ensure overhead is minimized in microservice designs. The name is pretty self-evident to the philosophy: try to keep the satisfaction of any given user-level request below 10 service hops. This is just a guideline. Going over 10 hops doesn\u2019t immediately cause the entire design to fall apart like a bad game of Jenga. However, remember that every hop adds some amount of latency overhead\u201410ms is a good rule of thumb to help with planning. By the time you get to 10 hops, you\u2019ve added 1/10th of a second to the processing time of every user-level request simply through service communication. The 10-Hop Rule also helps control complexity. If a single request must pass through 50 services on its way to completion, keeping track of where that request is at along the line becomes much more challenging. Keep in mind that we\u2019re talking about 10 sequential hops. When you\u2019ve got some services talking asynchronously to each other, or one service talking to many services at once, you can determine the overhead using a sort of tiered diagram, as shown below. Note that the third tier\u2019s overhead isn\u2019t 60ms - service 2 and service 3 finished communicating before service 4, so only 40ms were spent waiting at that tier. To put it simply, each tier has as much overhead as the slowest service in that tier. Complexity , however, is still measured at the total number of services required to satisfy the request. Fault Tolerance One of the bits of wisdom learned from our forerunners in the microservices space is the importance of building tolerant systems. While tools like Kubernetes allow us to basically infinitely maintain a system through anything but severe crash loops, that doesn\u2019t mean a given service\u2019s design should depend on such behavior. Every piece of a system\u2019s puzzle should still be a complete application; that\u2019s one of the benefits of a distributed system. Measuring the success of a distributed system by looking at single unit uptimes is a mistake. However, in aggregate, the uptime of the fleet contributes to the system\u2019s uptime of scale . In other words, if your services are constantly crashing and having to start up again, you\u2019re wasting processing time and potentially impacting metrics. This is the lesson others have learned: ensuring that the system\u2019s units are tolerant of faults and errors is critical to maintaining a healthy ecosystem. There are, of course, times when the application must crash. This should usually be mitigated to infrastructure-level errors that are unrecoverable. Otherwise, the top priority is making sure that the service is able to recover cleanly from these sorts of concerns: Bad or malformed input Erroneous requests Common runtime exceptions Temporary resource unavailability (and other transient faults) Service-to-service contract mismatches Tolerance doesn\u2019t just mean recovering from issues without crashing or corrupting internal state. Tolerance can also mean being lenient on how many problems can exist in an input while still being actionable, for example. The objective is to ensure that small changes in service-to-service communication, transient faults, miscommunications, file I/O issues, and such do not cause misbehaviors in the application. Additionally, we want to ensure that compatibility remains high. APIs should be designed in such a way to facilitate evolution without requiring changes on consumers. System infrastructure changes (for example, changing the URI of a resource) should be mitigated through tools like Kubernetes, where they can be abstracted from the beginning, or obviated through a no-deploy central configuration mechanism (again, perhaps in Kubernetes). Avoiding Retry Storms You might have read the previous section and thought, \u201cGreat! I can make sure all my service-to-service requests are tolerant trivially. I\u2019ll just add a retry to the request!\u201d Congratulations, you just introduced a flaw that could take down the entire system\u2014maybe the entire data center. The difficulty in distributed systems is the scale: you might have 100 instances of a given service running, all getting requests at the same time. These 100 instances may, themselves, depend on another fleet of services, and them another set, and so on. If Service F becomes unresponsive for some reason\u2014say a network resource temporarily becomes unavailable or something like that\u2014then you end up with a traffic jam at that service. All of Service E backs up now, followed by Service D, C, B, and eventually A. It becomes a standstill. \u201cThat\u2019s OK,\u201d you say. \u201cAs soon as the network resource becomes available again, the jam will resolve.\u201d Perhaps that\u2019s true, except that Service E is programmed to retry its accesses to Service F. It turns out that Service D also retries its accesses to Service E, and Service C does the same with Service D. A lot of applications like to use ~1 second as a retry wait between attempts\u2026 Suddenly you have an exponential explosion of network traffic, as every service in the entire ecosystem starts retrying at the exact same time over and over. Services crash entirely because they starve themselves of network resources, infrastructure dies due to excessive load, and you more or less DDoS the data center from the inside. This is a retry storm , similar to the classic Thundering Herd Problem . Thankfully, it\u2019s easily mitigated. Principles of avoiding retry storms: Know when to implement retries at all. As recommended by Microsoft\u2019s general guidance on retries (for Azure, but applicable to any distributed system), it\u2019s not usually wise to implement retries at every service along the path of fulfilling a request. In many cases, the onus of retrying should be on one service\u2014usually the one most user-facing\u2014or the client application itself. This removes several aggravating factors from the retry storm scenario. Never infinitely retry. Eventually, allow the service to fail the request. The threshold for this will differ based on use case, but there comes a time that the request can simply no longer be fulfilled. Usually, this is implemented as a cap on the maximum number of retries. Never retry at the same interval between each attempt. Always use some sort of back-off strategy to ensure that your retries become less and less as the issue persists. Introduce some randomization to your retry intervals. That way not every instance backs-off and retries at the same time (in the case where they all run into issues at similar times). Consider implementation of the Circuit Breaker pattern instead of na\u00efve retries. This can be especially valuable when communicating with external services where failure is more likely to occur and be unrecoverable (as far as we\u2019re concerned). However, it can also hold value within ATLAS when communicating with highly-shared services like ATLAS Platform functionality\u2014your application should more or less behave as though those services are external. These mitigations cannot completely eliminate retry storms. No matter what, they will occur to some degree any time the system ends up in a bottleneck state and the bottleneck becomes clogged. By following these guidelines, however, the impact can be minimized. Communication Contracts One of the pesky recurring problems of software design is the management of communication contracts between services. Whether it\u2019s an evolving API, a code-level interface, or the response model from a service, it seems like a common source of regressions whenever two applications need to talk to each other. Since distributed architectures are built entirely on the concept of applications talking to each other, they can sometimes be especially prone to problems arising from miscommunication. There is no \u201cmagic bullet\u201d solution to the issue of communication between applications. In each instance, for both the API into an application and the responses coming out, one must exercise caution and diligence in design. As a general philosophy around how to design your communication contracts, try to follow Postel\u2019s Law. An implementation should be conservative in its sending behavior, and liberal in its receiving behavior. \u2014 Jon Postel To break this philosophy into more digestible chunks: Ensure your communication is as brief, small, and relevant as possible. This also means making sure your APIs are designed appropriately, such that each call is responsible for as little as possible. Responses from services should be very concise and contain only the immediate response to the request. Where possible, use schema-less models. Serializing and deserializing JSON or XML objects seems like a natural fit for service communication, but those formats enforce a schema that must be adhered to. If the schema is not met, the object will not deserialize, breaking the contract. This often causes minor changes to have a ripple effect throughout an ecosystem; even correcting a typo in an API spec can be impossible due to the massive number of consuming applications. One example schema-less pattern is the TolerantReader pattern . Where possible, avoid discrete versioning. (For example, moving from /api/v1/ to /api/v2/ is a discrete version change.) Never mark a new version for every change to the API. Creating a new API version implies that the old version must be supported\u2014not just individual pieces of it, but the entire contract. This requires the creation and enforcement of SLAs around its lifetime, deprecation, and eventual discontinuation. In the meanwhile, you likely incur technical debt supporting two different contracts. Versioning is painful for consumers. It forces them to adopt a new contract which, even with good documentation or release notes, requires thorough review and potential downstream overhauls. It discourages adoption of your API. That said, there are some scenarios where overhauls of your contract are required. In those scenarios, versioning can make sense. Probably best practice is to leave the API open for versioning (e.g. /api/v1/ ) but only take advantage of it when absolutely required. Utilize abstractions to remove dependencies on the contract details. This allows your logic to be ignorant of contract changes, except in the single source of translation (where the contract object becomes a DTO). Be explicitly backwards compatible\u2026 If you must advance your API version to v2, for example, create an SLA that dictates how long you will continue to support the v1 API. Ensure API consumers are informed of this SLA so that appropriate accommodations can (eventually) be made. \u2026but not forever. One of the flaws of supporting a highly-tolerant system for a long time is the accrual of technical debt in supporting many layers of backwards compatibility. Once a contract has been obsoleted, make and adhere to a plan to remove it entirely. Disable it programmatically, if possible, to reduce maintenance cost. When the API must change, try to make only additions. Given the above guidelines, adding to an API will not break existing consumers even if the new endpoints obsolete the old ones. Remember that the obsolete endpoints should be removed at a planned, communicated time. Have a clear definition of the required attributes in a contract, and concern yourself only with those. To put it another way, your code may not be the only consumer of the service, and therefore the response may contain properties you don\u2019t recognize. Discard them. On the other hand, if your code requires certain properties from the service\u2019s response that aren\u2019t supplied, and those properties were previously part of the contract, you have an error scenario. Or, for the same reasons, the API endpoints may support properties or parameters that your logic isn\u2019t aware of. Ignore them. If the service requires that your call provide attributes/parameters it can\u2019t possibly supply, you have a contract issue. It only introduces regressions to attempt to force contract compatibility with \u201cfake\u201d parameters or null values (unless the API docs specifically indicate null as a way to opt-out of that parameter). Even if it works, you\u2019re taking dependencies on incidental behaviors, which violates any contract. The service owner is not required to uphold incidental behaviors, which means your consuming logic is doomed to fail. Instead, where possible, reach out to the owner of the service and figure out how to reconcile the contract. If no reconciliation is possible, you\u2019ll need to either legitimately acquire the missing attributes/parameters (perhaps from another service) or find a different service to fulfill your request. Obviously, there can be exceptions. This is a set of guidelines, not rules. Just remember that every compromise on this philosophy increases the chance of regression and the cost of maintenance in the communication between applications. Deployment Tactics and Service Management Creating an application is about more than just building the systems. You also must design and implement the deployment and management strategies for the application. Organizations (including ours) often have methodologies for these DevOps tasks whether they have dedicated DevOps engineers or not, but with distributed architectures it\u2019s critically important that these existing strategies are rethought. The rigors of these systems are often more complex, although not necessarily more challenging, than monolithic systems. It is ultimately more beneficial to discuss the health of a distributed system in terms of uptime of scale rather than traditional metrics. It is strongly recommended that each project team have at least one member fulfilling a DevOps role to design, build, manage, and monitor the deployment of complex distributed applications. However, this isn\u2019t always feasible for a number of reasons. As a result, the entire development team for a project backed by microservices should be fully aware of these tactics, strategies, and patterns for successfully administering and managing the application. To be successful, everyone must participate in DevOps. Service-Level Agreements (SLAs) and Metrics Especially when dealing with services at a scope typical of microservice architecture, it is necessary to clearly define SLAs for systems. Without solid lines for \u201csuccess\u201d and \u201cfailure,\u201d these states are open to interpretation both from customers and internal stakeholders. Inevitably, your team will find itself constantly failing as the goal posts move based on shifting priorities, customer happiness or unhappiness, and internal perception. Establishing these contracts (in the abstract sense, though sometimes also literally) allows you to take control of the success and failure states and base them on concrete, measurable metrics. SLAs are comprised of one or more service-level objectives (SLOs). Your ability to base the SLOs on actual metrics ties directly to your system\u2019s ability to measure them. Thus, it\u2019s incredibly important to make recording metrics a first-class item of your design. Even if the metric isn\u2019t used as part of an SLA, record it anyway. The more data you have about the operation, state, and throughput of a system, the more information you have to diagnose issues. Good SLAs vs. Bad SLAs The successful implementation of SLAs requires that they meet several criteria. The only meaningful SLA is one that actually has value and can be measured; if it can\u2019t be measured or it can\u2019t be met, it\u2019s not useful. SLA requirements: It must be achievable through normal maintenance and upkeep. In other words, the SLA must not require over-allocating staff or working overtime in order to meet it. It must be achievable and reasonably future-proof with infrastructure that exists at the time of the agreement. An SLA that requires overtime, over-allocation, or lacks proper infrastructure is doomed to fail, and therefore not a valid SLA. SLOs must be measurable. The metrics must be mathematically significant and defined within the agreement. \u201cThe customer must be kept happy\u201d is not a valid SLA because it\u2019s not measurable. \u201cCustomers must rate our service at least a B+ on surveys\u201d is not a valid SLA because it\u2019s not mathematically significant; there is no meaning to the measurement. \u201cThe queuing system must be capable of handling peak load\u201d is not a valid SLA because while peak load is measurable, and throughput rates are mathematically significant, neither of these thresholds are defined in the agreement. Not defining boundaries and thresholds in the agreements leads to scalability problems\u2014peak load will naturally increase over time given company growth, creating an inevitability where the system will fail to meet SLA. This is why you tend to see SLAs around uptimes and MTBF (mean time between failure) measurements: the target number is measurable, significant, and made part of the agreement. It must be honored. Obvious? Yes, but it should be reinforced: defining an SLA that will never be upheld creates friction on all fronts, wastes significant effort, and spends trust. Trust is the most valuable currency we have with both internal and external stakeholders. Spending it frivolously is not wise. It is better to have no SLA than to define one you can never (or will never) honor. The responsible team must be held accountable for failures to meet SLA. Given rule #1\u2014that the SLA is achievable\u2014if the team fails to meet SLA, actions must be taken. This does not necessarily imply punitive action; rather, a plan must be formulated to bring the service back into compliance and executed immediately. Some sort of post-examination of the violation should be conducted. Perhaps the environment has changed such that the SLA is no longer maintainable, which should be easily validated with metrics. Often, simple scaling or software tweaks can bring a service back into compliance. Possibly, the team doesn\u2019t have enough resources or people to meet SLA. Sometimes, a defect in the software may cause a violation of SLA. A fix should be identified, tested, and fast-tracked into deployment. Depending on the frequency of this type of violation, a broader look should be taken at the team\u2019s quality practices. Adjustments may need to be made to methodology. It is possible that the team simply failed to be responsible. Assuming that the company is hiring strong staff with good priorities, this should be fairly rare. It\u2019s often mixed up with one of the other issues due to a poor root cause analysis, though it certainly does occur on its own. These scenarios will be handled by the unit\u2019s usual method of addressing underperformance. It must provide business value. Creating an SLA around 99.999% uptime and a six-month MTBF is great if customers are paying to justify that level of service. Adhering to those standards, however, requires herculean effort and resources. If customers don\u2019t need it (or aren\u2019t paying for it), why waste the time, money, and effort providing it? This isn\u2019t to say that only customer-facing SLAs are important. Internal SLAs are just as important, if not more so since they directly prop up the external ones. Instead, focus on determining the level of service you need to guarantee, and build SLAs on that. SLAs and Microservices This section on SLAs exists in this document because of the intrinsic link between the upkeep and management of a highly distributed system and the need for clear SLAs. By definition, the parts of a distributed and/or modular system will fail. Applying the usual monolithic architecture mentality to these failures will cause nothing but overreactions and negative perceptions. At sufficient scale, all distributed systems are some degree of \u201cdown\u201d at all times. The good news is: that\u2019s fine. It may be concerning to plan for a system where failures are commonplace, but part of the philosophy here is that failures are commonplace in every system, distributed or not. In microservices, you have the opportunity to plan for them and mitigate them architecturally. The highly-available, widely-scaled, and fault-tolerant nature of these services means that individual failures are of no concern. Instead, you watch for systemic issues and measure health in terms of uptime of scale \u2014this is why we must have strong SLAs. They help provide a measurable criteria for health and success. This measurable criteria can be called the steady state of intended behavior or throughput. As it turns out, the steady state and your system\u2019s SLAs go hand-in-hand; they\u2019re intrinsically bound. The primary way to identify systemic issues at scale is to compare the current state against the steady state. The Stoplight Strategy To help with the management and monitoring of our distributed systems, a strategy has been devised to amplify the discoverability of systemic issues. This is the Stoplight Strategy. The idea is simple: a given system\u2019s state can be represented depending on how closely it is currently adhering to steady state. Our existing monolithic system is either up or down, but distributed architectures have a whole gradient of grey area in between. As alluded to earlier, most distributed systems at scale are always some degree of \u201cdown\u201d . The definitions of success and failure have to change. This leads us into a metric-based, SLA-oriented model of defining green , yellow , and red system states. It is critically important that we define the thresholds, SLOs, and criteria for a system\u2019s green , yellow , and red states early and concretely. They must be reevaluated often in order to keep in sync with the current SLAs and demands. Green state is when we meet or exceed the established expected performance, uptime of scale, latencies, throughputs, etc. of the system. All SLOs are being met or exceeded. Once we know our green state, we can start to define our yellow state. The yellow state is an operational state, but may be suffering from factors that impact user experience. Examples include high latencies, dangerously low uptime of scale, or an abnormal number of errors across the system in a given time window. The yellow state is incredibly important because it indicates two things: Failure is imminent There is an opportunity to improve the state of the system before failure occurs Red state occurs when some part of the system is fully down, or at least one SLO is not being met. The Stoplight Strategy In Practice Important to keep in mind is that a green state doesn\u2019t necessarily indicate that the system is 100% healthy. It only indicates that the system is exceeding the criteria we\u2019ve established, and that\u2019s OK. We might still look at other metrics to evaluate whether there are concerns to be investigated, but assuming the green state is well-defined, the system is working as expected. Any concerns found should not be critical, though it is wise to be on the lookout for trends that will lead the system into a yellow state eventually. On the other hand, being in yellow state means that investigations should begin immediately. While the state is not critical yet, it usually indicates that it will become critical . If a system is in the red state, it should be considered completely down, even if it might still be servicing some requests. The intentions behind this strategy are almost more important than the strategy itself: A clear guideline and goal for establishing the metrics and monitoring around a system An implied dashboard that monitors the state and metrics of many systems using the same green / yellow / red indicators, even if the criteria differs between them, to provide an \u201cinfrastructure at a glance\u201d view A simple way to report on the current status of any given system for any stakeholders, internal or external You might notice that this system bears many similarities to something like Azure\u2019s Service Status page. This is fully intentional. The two strategies serve the same purpose, though for slightly different audiences. In fact, there\u2019s a lot of value in providing an internal version with greater detail and a version for customers that gives a different, simplified perspective. A simple stoplight-inspired indicator of state will never be sufficient to thoroughly monitor system states. It is not intended to be the only metric of health, or the only touchpoint of support. However, it does provide an easy-to-understand and quick reaction pivot point for all involved engineers. One of the problems in many of our existing systems is that we have a hard time saying\u2014at even a basic level\u2014whether they\u2019re healthy or not. We almost completely lack visibility. We have difficulty reacting quickly to changes in state, because there is no human-readable way to get state at a glance. This indicator helps by acting as a canary for potential issues. CI and CD CI (Continuous Integration) and CD (Continuous Deployment, in this case) are vital practices to maintaining a distributed application. A quick definition for each to help set context: Continuous Integration : developers push code to a central repository, from which the commits are immediately built and tested (via automated unit tests) to ensure valid and well-integrated code has been committed. Continuous Deployment : once validated by the CI process, the new version of the application is published and automatically deployed into production, most often via zero-downtime strategies. This is different from continuous delivery , where the application is built, validated, and published, but not yet deployed into production. It must be deployable, but the deployment occurs manually. Together, these processes help us control the complexity of distributed architectures. By employing CI, we ensure that each unit of the application is always in a good state\u2014that is, not completely broken or poorly integrated. Regressions are mitigated by automated testing. Most development teams use some form of CI today; there is likely no value in going further preaching its benefits. Continuous deployment is a more challenging process, but it too ensures that the application\u2019s units remain in a good state. The continuous delivery strategy is part of continuous deployment : the units must be easily deliverable and always deployable. However, going the extra step into automatically deploying units as soon as they\u2019re ready provides a ton of value, assuming your infrastructure and design is capable of zero-downtime deployments . The time-to-value for a given change is significantly reduced. Fixes deploy much faster, and improvements are delivered to users as soon as possible. The process overhead of deployment is removed entirely. There is an argument that the overhead is simply moved elsewhere in the workflow, usually manifesting as holding commits into master until they\u2019re ready for deployment. This is more desirable than delaying the deployment in later stages, as the roadblocks can be removed in case of emergency. Fixes can deploy as fast as code can be committed, instead of however long it takes a human to find the latest build, ensure it\u2019s valid, and initiate a manual deployment. The master branch always represents the deployed code. This allows teams to know, trivially, the state of the production environment, without having to accommodate for whether the code has been deployed yet or not. In a monolithic environment this is a non-issue, but in a distributed environment it becomes a morass of confusion. Better application design is encouraged in order to support zero-downtime deployments. Specifically, this encourages designs that are completely stateless, which has tons of knock-on benefits in terms of resilience and scalability. It all but requires a faster, more efficient, and lower friction database design, often leaning on databases which deploy as part of the application or schema-less technologies that prevent roadblocks on shared databases. Continuous deployment acts as a sort of CI for your distributed system. The goal of CI is to be constantly validating that your commits integrate well with the codebase. Continuous deployment allows you to constantly validate that your commits integrate well with the entire system . In CI, you continuously commit to the central repository to shorten the window of time in which your work could have gone out of sync with the codebase\u2014e.g. due to others\u2019 commits. In CD, you continuously deploy units for the same reason: to shorten the window of time in which your work could have gone out of sync with the rest of the system\u2014e.g. due to changes to other units. Also similarly to CI, regressions are discovered immediately, instead of whenever the commit happens to deploy. This has been implied above, but each unit of your distributed application should be a separate CI/CD pipeline. In other words, each unit should be capable of building and deploying independently of other units. Thanks to tools like Kubernetes and Docker, this becomes significantly more straightforward. Docker allows each unit to dictate its own build process in a Dockerfile. Kubernetes makes it simple for it to declaratively define its deployment in a .yaml . As these are (mostly) universal concepts, we no longer need to rely as much on extensive custom build and deploy scripts in a system like TFS. For monolithic applications, it isn\u2019t the worst thing in the world to write custom build/deploy scripts, but at distributed scale it\u2019s impossible to maintain. Example CI and CD Workflow Here\u2019s an example of how our CI/CD workflow might look. This isn\u2019t exactly how our actual workflow will look, but it should help demonstrate the concepts. A developer named John Doe makes changes to the code in a branch. Let\u2019s call it john.doe/feature-branch for example\u2019s sake. John commits code to john.doe/feature-branch during active development. If necessary, John has a path on the Docker registry (e.g. Harbor) for pushing images manually for testing, though they cannot push to certain protected paths like the root. A Test environment exists as well for manually testing deployments and for testing units (or whole applications) at scale without affecting Production resources. After development and initial testing, a pull request is submitted to merge john.doe/feature-branch into master . The PR contains not just code changes, but potentially build and deploy changes as well. The repository\u2019s policy requires that the DevOps group approve the PR if the build and/or deployments change, in addition to the usual approvals. The DevOps group and the usual suspects approve the PR. It merges into master . The build server detects a change to master and initiates the CI process. The image is built and automated tests are executed. Assuming the build and tests pass, the new image is pushed to the registry under a specific version number. After the image is pushed to the registry, it is vulnerability-scanned. Assuming the scan completes successfully, the CI process is complete. The image is tagged as latest in the registry. Now the CD process begins. The build server initiates a deployment of the Kubernetes .yaml . Since the DevOps group signed off on the PR, the deployment should be safe. Using k8s mechanisms for upgrading deployments, the existing pods are rolled off one-by-one, with new ones created in their place. Assuming the deployment enters a good state before a configured timeout passes, the CD process is complete. The new unit can be considered fully deployed now. Summary After a pretty solid amount of research and evaluation, this document contains the current philosophy around ATLAS\u2019s approach to distributed and microservice-oriented architectures. This philosophy will change with time and experience, but it represents our idealistic strategy at the time of writing. As mentioned in the first section, we always want to design the \u201ccorrect\u201d implementation first. Make compromises only when the correct design is unachievable, and make them as small as possible. If you begin designing from a compromising mindset, your design will not reach the heights it may have otherwise been able to reach despite limitations and setbacks. The collection of strategies around implementing distributed architectures are a first draft. These, too, will expand and evolve as we gain experience. Perhaps the idea of a \u201cstoplight\u201d system of measuring system health is too na\u00efve to bring value, or perhaps it\u2019s too complex for the target audience even as presented. Maybe it turns out that the Circuit Breaker pattern just doesn\u2019t work for us. It\u2019s possible that continuous deployment turns out incompatible with our team structures. Most likely, we will add powerful solutions we discover along the way. At no point should this document be considered final or even authoritative. The document is, however, a set of guidelines and philosophy that should help everyone work into this new way of thinking. This is its only true objective. Putting yourself into the mindset for successfully creating a distributed system\u2014that is, avoiding the common pitfalls and long-term pain points that often plague distributed systems built by monolithic teams\u2014requires more than a basic understanding of the tools. It requires a philosophical and fundamental shift in many of the assumptions and basic foundations that we build up after years of experience. As alluded to before, distributed systems have existed since the birth of networked computing. Today they\u2019re just easier than ever before to create and manage. The tools we have now make designs practical that were once infeasible.","title":"Distributed Architecture Strategy"},{"location":"Misc/distributed-architecture-strategy/#distributed-architecture-strategy","text":"Much of our new design is oriented around a much more distributed architecture than we\u2019ve normally used at ATLAS. While we are a little late to the party, that also means we have the chance to learn from our forebears and not make the same mistakes. Many people think of microservices when they think of distributed architecture. While this is the primary implementation today, distributed architectures have been around since the dawn of computing. They can take many different forms, each with their own pros and cons. At ATLAS, we should be hesitant to refer to our version as \u201cmicroservices\u201d\u2014not out of dislike for the pattern, necessarily, but because the term brings several implications with it. Being realistic, a full-on microservice-oriented architecture will be challenging for us to achieve simply due to our staffing limitations. Throughout this document (and any time we discuss architecture going forward), the terms \u201cmicroservices\u201d and \u201cdistributed architecture\u201d will be used fairly interchangeably. Documentation should emphasize the ideal architecture, even if in practice we must make some sacrifices and compromises.","title":"Distributed Architecture Strategy"},{"location":"Misc/distributed-architecture-strategy/#table-of-contents","text":"Distributed Architecture Overview Important Terms Microservices Example When to use Distributed Architecture Why Distributed Architecture? Granularity Granularization Example Documentation Documentation Centralization Documentation Automation Other Automation Documentation Prioritization Application Design Philosophy The 10-Hops Rule Fault Tolerance Avoiding Retry Storms Communication Contracts Deployment Tactics and Service Management Service-Level Agreements (SLAs) and Metrics Good SLAs vs. Bad SLAs SLAs and Microservices The Stoplight Strategy The Stoplight Strategy In Practice CI and CD Example CI and CD Workflow Summary","title":"Table of Contents"},{"location":"Misc/distributed-architecture-strategy/#important-terms","text":"In the scope of this document, there are some terms we should set definitions for to avoid confusion. Granularity : a measure of the degree of separation, isolation, and individualization given to units of a system. A system can be said to be highly granular when its implementation involves many small, distinct units working together. Service : any discrete, resident application whose purpose is to react in some functional way to requests or events. (This is called out to denote a difference between what is considered a \u201cservice\u201d here\u2014just the conceptual object\u2014and the more concrete patterns we usually take for granted when we discuss services.) Robustness : the measure of a process\u2019s ability to cope with erroneous input or faults gracefully. Uptime of scale : Rather than the literal uptime of a given process, uptime of scale is the healthiness of a distributed system as measured by its adherence to steady state over a window of time. In other words, the uptime of scale is the measurement of a given system\u2019s ability to sustainably maintain its scale. This differs from availability in that a service can still be available while experiencing a low uptime of scale .","title":"Important Terms"},{"location":"Misc/distributed-architecture-strategy/#microservices-example","text":"A microservice-oriented architecture might propose that the operation \u201crun a report\u201d (as it exists in current ATLAS, newer designs notwithstanding) consists of several distinct services. It may have services for: Loading metadata and compiling the report (e.g. AdHocs) Performing validation ( IsValid ) Performing SQL replacements ( RunDynamicSql ) Executing report SQL Executing post-processing Performing Excel exporting Delivering results Each of these services are currently just steps of the monolithic report pipeline. However, due to the monolithic design, each of these steps has innumerable cross-cutting concerns. Worse, this logic has far-reaching and omniscient dependencies throughout that service. As a result, the entire report pipeline incurs the risk of regression even if only a single step needs to change. Additionally, even a transient failure at any single step causes the entire pipeline to fail. Exactly how granular your services should go is a subject of hot debate in the field still to this day. One could easily imagine each of those steps being further broken into smaller services: a service for delivering FTP results, a service for delivering email results, a service for executing BI reports, a service for executing AdHoc reports\u2014the list goes on. This is part of the hesitation to use the word \u201cmicroservices\u201d when describing our distributed architecture. While there could be value in going more granular with these services, that value should be measured during design and balanced against the increase in complexity and overhead.","title":"Microservices Example"},{"location":"Misc/distributed-architecture-strategy/#when-to-use-distributed-architecture","text":"Distributed architecture is a great solution to many problems, but it does not solve every problem. Certainly, it isn\u2019t the right solution at all for some problems. These distributed service patterns, like microservices, bring with them a decent amount of overhead and complexity. If you select them to solve your design problems, you are making a trade-off. The amount of benefit you gain can be conceptually represented as a \u201cmultiplier\u201d that depends on your problem. Some problems have huge multipliers\u2014basically anything that needs to scale quickly and widely, or anything which needs highly isolated parallelism. Some problems have near-zero multipliers, or even negative multipliers. It is very important, before deciding to go with microservices or similar architectures, that you evaluate your problem\u2019s multiplier. Read the Why Distributed Architecture? section to help figure out how much your particular engineering problem will benefit from distribution. As an example: relatively simple applications, temporary solutions, quick fixes, dirty hacks, and some client applications have very low multipliers. From a mindset perspective, it\u2019s probably wise to always begin with a more monolithic design. Only break it down when you discover the advantages of doing so outweigh the disadvantages (more on that strategy later). That said, don\u2019t be afraid of choosing distributed architecture where it makes sense. This section exists more to warn you against using it for trivial applications or forcing it to apply to problems which resist it. There is no mandate that says \u201call projects going forward must be designed using a distributed architecture,\u201d but we now have the infrastructure to support it more robustly than ever before. Instead, this document should serve as a helpful teaching tool and guide for why you might want to use these patterns, and how to use them successfully.","title":"When to use Distributed Architecture"},{"location":"Misc/distributed-architecture-strategy/#why-distributed-architecture","text":"Detractors of microservices as a pattern often cite that the complexity of the pattern isn\u2019t worth the gains. The claim is that this complexity increases exponentially as the number of services increases, yet in order to \u201cdo it right,\u201d one must create highly granular services\u2014which, of course, leads to high complexity. It\u2019s certainly irrefutable that system complexity grows exponentially as the number of services increases. However, the granularity of the system is a variable that can be tweaked to avoid complexity. As with any architecture, you measure the strengths and the weaknesses and choose the right one for the job. Distribution is not the correct solution for every problem, but it\u2019s very effective at solving certain types of problems. Here are some of the benefits of choosing a distributed architecture. If your business problem aligns well with these benefits, it increases that conceptual multiplier . These are the reasons you might choose this architecture over others: Reduction of friction in development and deployment Improves the development and debugging experience by isolating effort into a single module Tends to make deployments deterministic and far simpler, more robust Enables Continuous Deployment, which ensures a fast stream of improvements and a quick time-to-fix for defects CD is challenging with monolithic applications because deployment takes so long and regressions can be very broad. A single microservice can build and deploy in seconds, and issues are usually isolated to that one service In our ecosystem, we\u2019re looking at time improvements measured in hours for many defects; the test, deploy, re-test cycle for a fix is significantly expedited Zero-downtime CD is possible, if the infrastructure is designed correctly Isolation of responsibilities and concerns at the module level Lends to better designs, more future-proof APIs, and fewer defects Allows a much greater ability to test modules individually, which increases code quality and robustness Scalability Makes our business more flexible Allows us to alleviate bottlenecks and improve customer experience as needed with almost no spin-up time Reduces friction between dev and ops teams Sharing of components and services Greatly speeds development and enhances products with less dev effort Multiplies improvements across all participating products Infrastructure and language freedom Services can be written in the best language for the job By design, a microservice should be as infrastructure-independent as possible to increase its motility Reduces cost to the business Separation into distinct services leads to more complete applications Microservices are first-class applications with their own resiliency, high-availability strategies, fault tolerance, error handling, and so on This not only makes applications more resilient, it also tends to improve visibility and logging/metrics Enables cross-functional, smaller teams to be more productive Without monoliths full of omniscient dependencies, it\u2019s much easier for any developer to work on any piece of the puzzle This reduces costs and increases developer utilization Reduction in business risk due to better facilitation of agile development Since each piece of an application is discrete, agile teams can produce easily deliverable units Modules that change as a result of agile feedback loops are easily replaced with minimal lost effort","title":"Why Distributed Architecture?"},{"location":"Misc/distributed-architecture-strategy/#granularity","text":"This is it: the million dollar question. How granular should your services be? How much should you break up functional units into individual services versus grouping them at higher levels of abstraction? Sadly, there is no easy answer. Instead, for every use case, you\u2019ll need to evaluate the advantages and disadvantages to find a good balance point. Advantages: Every level of granularity is another unit of scaling, high-availability, and logical separation Each granular service introduces another level of reusability that other services can opt into Each granular service is easily self-contained, increasing efficiency, robustness, and project management flexibility Application failures tend to be isolated to single small, replaceable, easily fixable units Service failures almost never cause system downtime once the functional units are granular enough Parts of an application will continue to operate even if others are down Disadvantages: Every level of granularity increases complexity, eventually to the point of making the architecture incomprehensible Even with extensive documentation, building a mental map of a highly-granular microservice-oriented application requires a great effort Each granular service is another item that must be managed, maintained, developed, and owned Each granular service introduces additional overhead on a request, especially in sequential operations Application failures are more challenging to track down due to the high number of interconnecting parts The guideline for granularity is pretty simple, though it\u2019s more philosophical than scientific. Make every unit as granular as you can, so long as you are able to amplify the advantages and minimize the disadvantages. If going more granular no longer amplifies the advantages, or raises the disadvantages to an uncomfortable level, your unit is granular enough. One could imagine, especially as we first dip our toes into the waters of distributed architecture, we\u2019ll be more uncomfortable than we need to be about the disadvantages. While we definitely must be mindful of them, we must also remember this is a new paradigm for us; we\u2019ll need to change the way we think. As you\u2019ll see later, there are tools available to help us mitigate the disadvantages.","title":"Granularity"},{"location":"Misc/distributed-architecture-strategy/#granularization-example","text":"Earlier, the monolithic process of running a report in current ATLAS was broken down into 7 services. What if it was further broken down and made more granular? Here\u2019s how that might look: Metadata services: AdHoc Metadata Service User Report Metadata Service BI Report Metadata Service Validation services: AdHoc validation service BI Report validation service Pre-processing services: Tokenization service SQL transformation service Object filter service AdHoc compilation service Report Execution service Post-processing service Formatting services: Excel formatting service PDF formatting service Access formatting service Delivery services: Web delivery service Email delivery service SharePoint delivery service FTP delivery service Coordinator service We\u2019ve gone from 1 monolith, to 7 services, to 19 services. Is there value in making the process so fine-grained? Let\u2019s revisit the advantages. We can now finely control exactly how much of every kind of work we can perform. We run far more BI reports, for example, so we could ensure we have far more BI Report Metadata service instances than AdHoc Metadata instances. Reporting Studio and other projects could use whichever pieces of this pipeline work for them. One project could use all of it, except it might not allow exporting to Access or delivery via FTP. It may instead add cloud storage delivery. It could use a separate coordinator that knows how to integrate the services for its use case, but still consumes many of these same services. One Email delivery service could crash, but there are at least two instances running at all times. Other delivery types are unaffected. All FTP delivery services could simultaneously crash, but other delivery services keep functioning. The FTP service fix for the crash affects only the FTP service instances. Each of these smaller services could be developed by a different engineer. So long as the contracts between services are well-defined (and they must be no matter how granular the scale), the developer\u2019s scope of work is greatly narrowed. The small unit can be more easily and completely tested. Sounds amazing so far. It\u2019s super scalable, robust, smooth to develop, and highly reusable. This design checks all of the boxes, right? Well, let\u2019s take a look at the disadvantages too. There are nineteen services. Keeping track of what each of them do, where they live, how they\u2019re deployed, their communication contracts, the health of each set of instances, and where along the path a given request currently lies are engineering problems all their own. A service had to be introduced (the coordinator) to manage the lifecycle of a request, further increasing complexity. This example scenario can occur: \u201cAn FTP service instance just crashed.\u201d What happened to the work it was doing? It probably got re-queued. \u201cNow every FTP service instance has crashed.\u201d Which message killed the FTP services? The logging and correlation metadata had better be very strong; otherwise, prepare to spend hours digging around trying to figure out what happened. \u201cI found the request that killed the FTP instances, but I need to debug through the code to figure out the fix.\u201d You must either spin up nineteen services on your local machine to step through the process in its entirety\u2026 \u2026turn on some kind of remote debugging (which is insecure and not going to happen)\u2026 \u2026or grab the input for just this particular service, stand it alone up on your workstation, and feed the message through manually. The latter option will almost always be the correct answer. Each of these services introduces a communication overhead . A given request in this design is likely to go through 9 hops before finishing. Assuming a network latency of 5ms and a general API server latency of 5ms, that\u2019s 10ms per hop. We\u2019ve added almost 100ms in latency to this request just for communication. Each of these services introduces a resource overhead . A given service instance probably needs a minimum 100 MB of RAM dedicated to it just for framework overhead and runtime. Given that we want to have many instances alive for distributing workloads and maintaining a high availability, let\u2019s assume we have a total of 100 instances running between all 19 services. That\u2019s almost 10 GB of RAM dedicated to overhead. Each of these services introduces a development overhead . Getting started on each service involves the creation of a new repository, and in some cases, a new project through the PMO. Someone has to own this project and manage its deployments. There is always some amount of boilerplate code or configuration for setting up a service, even trivial services like a basic website serving plain HTML. Without any additional tools to help, this sounds rough. However, Kubernetes was created to help mitigate these disadvantages. It is the key to minimizing them. With Kubernetes, we can let go of the need to know and manage where each service lives. We can define how they\u2019re deployed via script and get deterministic deployments. We can easily monitor the health of the deployment. We can minimize network overhead and cap resource overhead. K8s alone helps remove a lot of the pain from this 19-service design. On top of that, there are several other tools and methodologies we can employ to reduce the impact, such as serverless (functional) paradigms. Is the 19-service design worth choosing, then? Well, realistically, the best design is probably somewhere between the 7-service and 19-service ones. Splitting validation into two services probably does not amplify the microservice advantages in practice. The metadata services aren\u2019t likely to benefit from the scalability of being so granular. Reports today don\u2019t even support FTP or SharePoint delivery. No matter the scale, though, you\u2019re going to run into some of these roadblocks eventually. The remainder of this article is going to help you address the disadvantages and roadblocks of microservices at scale. We will cover those tools, methodologies, and patterns that make developing microservices much more fool-proof and allow us to squeeze every last drop of value out of them.","title":"Granularization Example"},{"location":"Misc/distributed-architecture-strategy/#documentation","text":"Unsurprisingly, the number one strategy to deal with the unwieldy nature of microservices is to produce reliable documentation. Lack of documentation is a serious business risk that doesn\u2019t get enough attention no matter the architecture. Not only is the chance of introducing regressions into existing systems higher, but the tribal knowledge that went into developing and maintaining legacy applications is often lost forever. Documentation helps during development as well: the more informed the team members are about the design, APIs, and philosophy behind them, the better decisions they will make during development. It makes developers more efficient, less stressed, and more cohesive. The ugly, unavoidable truth is that documentation is expensive. Producing it\u2014especially when a product or application is being developed by a small and/or tight-knit team\u2014is often unattractive. Frankly, even going through a formal design process at all is sometimes expensive, as developers spend tens of hours expressing on paper what they feel they already know in their minds. Being given the task of writing documentation is seen as \u201cgetting stuck\u201d with it; no one wants to do it. Worst of all, it requires constant maintenance to keep current, as the only thing more harmful than no documentation is bad documentation. Given that it\u2019s so expensive and cumbersome, yet so important to the success of distributed applications, how can we reduce the cost and still achieve a high quality of documentation? The solution is simple: centralization , automation , and prioritization .","title":"Documentation"},{"location":"Misc/distributed-architecture-strategy/#documentation-centralization","text":"The most aggravating thing about keeping documentation current is that it usually lives in multiple places. Code lives in Git, but documentation often lives in some combination of OneNote, SharePoint, and a Word doc in an e-mail someone sent you 6 months ago. As a result, you find yourself flipping between several different places trying to keep track of them all. This wastes significant time, usually tens of minutes per instance as you have to rediscover where everything lives. If documentation were centralized and organized, however, it would be easier to update and effortless to reference. To make sure that the documentation is not only discoverable, but free of administrative roadblocks, here are the recommendations for centralizing documentation: Documentation is published to a central site. All engineers in the company have access to the entire site. The docs are not walled off by teams. This allows people to answer their own questions where possible. The published documents live in a location wholly dedicated to presenting documentation and nothing else. Any existing docs should be migrated to the site over time. The pages are logically ordered by projects, domain, and other meaningful categories that minimize navigation time. Documentation is centrally searchable. At a company of our scale, it should be reasonable to centralize all technical documentation into a single location. If the company grows significantly, the sheer amount of documentation could become impossible for any organization scheme to manage. At that point (and only at that point) we can consider breaking the documentation out into logical sub-units.","title":"Documentation Centralization"},{"location":"Misc/distributed-architecture-strategy/#documentation-automation","text":"You\u2019ll notice that this very document lives in a Git repository. It is automatically published via CD to a documentation site, and it\u2019s written in Markdown. Each of these is critically important to the upkeep of this document. Storing documentation in Git : Storing docs in Git allows several people to edit the same document and merge changes together trivially. It enables deployment integrations. It provides free version control. Everyone\u2019s been in a situation where the design document was \u201cProduct Design V4 Final - Final.docx\u201d and it\u2019s not helpful. Git is a toolset many developers are already familiar with. While Git itself is not the requirement, it happens to fulfill these requirements very well. Automatic, continuous deployment to the central site : Documentation is a living thing. It evolves, especially during development. Making sure the latest version is always available from an authoritative location is critical to the success of the documentation effort. Decisions made against an outdated copy of the documentation are bad decisions by default. Markdown : Markdown itself isn\u2019t the requirement. The true requirement is that documentation be written in a universal, easy-to-write text-based markup language. It must have low friction and lots of tooling in its ecosystem, but still support enough features to be useful. HTML is another example option. Although, one could argue that it fails to meet the \u201ceasy-to-write\u201d requirement for someone just trying to hash out some docs. Markdown is the best of both worlds: HTML support when you need it, easy and intuitive syntax when you don\u2019t. Markdown\u2019s ecosystem is also how we\u2019re able to so effortlessly publish the documentation site. In this case, the tool mkdocs generates a site with top-nav and left-nav automatically prepared for you. The particular combination of Git + TFS deployment + Markdown isn\u2019t necessarily the best option. However, it is how this document was published, and this document was published using the same principles that will make other documentation efforts successful. Many products exist which provide the same advantages (e.g. Confluence); feel free to use them. Just keep centralization in mind: the docs need to be in one place.","title":"Documentation Automation"},{"location":"Misc/distributed-architecture-strategy/#other-automation","text":"Aside from the automatic deployment of documents to a centralized location, there are other things we can automate to make doc maintenance simpler. Some of these tools we\u2019re already familiar with and some of them will be new to us, but each of them is critical to sustainable docs. Most important, probably, is the documentation of code itself. Self-documenting code is already a staple of good engineering. When documentation must be written, however, putting the documentation in the code itself is natural for developers. It\u2019s more likely to get updated when the functionality changes. Taking it one step further, publishing it automatically can be achieved via several tools (often built into modern IDEs and compilers, like JavaDoc and C#\u2019s XML documentation). The automatically-generated documentation from code comments is great for internal reference, but what about externally-facing docs? Sometimes the internal docs work for external consumers as well, but often you\u2019ll want to add some additional elements to API documentation, such as sample code or visualizations. This is where tools like Swagger UI come into play. Often, in cases like this where external customers will be consuming the documentation, it\u2019s also wise to treat the docs as a deliverable themselves rather than a byproduct of development. You\u2019ll want to devote some time to polishing them up and making sure they\u2019re accurate; then, rely on automation to keep them up-to-date.","title":"Other Automation"},{"location":"Misc/distributed-architecture-strategy/#documentation-prioritization","text":"You\u2019re probably already practicing centralization and automation to some degree. The real trick to help documentation efforts be successful is this one: prioritization. Specifically, acknowledging that some forms of documentation are inherently less valuable than others, and should be given less effort. Before we can discuss priority, let\u2019s look at the distinct varieties of docs. These definitions aren\u2019t exactly standardized, but should certainly be agreeable. Architectural documentation \u2014 lays out the concept behind the systems, how they work together, and what the system should achieve. Ensures that the system meets the project requirements. Usually purely conceptual. Design documentation \u2014 concretely defines the patterns and idioms to be used in the implementation of the architectural design. Dives into the details rarely, but still paints a solid picture of what the final code will look like. Often combines the conceptual design with practical solutions. Implementation design \u2014 The collection of interfaces, code modules, services, and solutions that will be created to reflect the design documentation. Usually purely practical. Code documentation \u2014 comments on methods, parameters, and modules that expose intent and usage guidelines for code. API documentation \u2014 details the purpose and parameters of an API for internal or external consumers, including the API\u2019s communication models. Sometimes, the API documentation and the code documentation are the same thing. There are certainly more, but these are the most common and most relevant to our discussion. This philosophy prioritizes documentation in this order: Architectural documentation and API documentation. Design documentation. Code documentation. Implementation design. Implementation design is the least valuable type of documentation. Implementation can change constantly, quickly, and sometimes unpredictably (e.g. due to hotfixes). Keeping an implementation design current is essentially equal effort to its corresponding development. It\u2019s not worth spending the time to carefully document the code so closely, even after the fact. Code documentation is very important, perhaps the most important type of documentation for a product\u2019s technical success. However, it falls low on the list of priorities because it should be happening anyway . No extra effort should be expended to create it. If developers are not documenting their code well, or not writing self-documenting code, actions should be taken to correct the deficiency. When done well, it also serves much the same purpose as the implementation design. Developers should be able to trivially reverse-engineer the meaning and purpose of any component or unit. Design documentation and architectural documentation are important because they both capture intangible elements of the design that cannot be easily discovered otherwise. Tribal knowledge, business decisions, design philosophies, and historical context are all made available through these documents. Essentially, they act as the rubric by which the implementation is measured. In an imaginary worst case scenario where the working implementation was lost, you should only need these two documents to produce a new one. It might not be exactly the same, but it will meet the same requirements. Of the two, design documentation is less important than the architectural documentation only because the design can change without affecting the architecture. Imagine the design docs as an implementation of an interface in C#, where the interface is the architecture. API documentation , then, is the only purely-practical piece of documentation that holds a high priority. Much of this effort is spent making sure that your software, which is meant to be consumable since it has an API, is easy to integrate. Maintenance effort spent here, unlike in implementation design, is always worthwhile as it directly contributes to the quality of the product. Additionally, a well-designed API should change fairly rarely (even though the implementation might change constantly), minimizing the upkeep. Further reducing the upkeep are tools that can automatically generate API documentation or assist in creating it, which is never possible for implementation design.","title":"Documentation Prioritization"},{"location":"Misc/distributed-architecture-strategy/#application-design-philosophy","text":"In the distributed architecture world, it helps to think of application design a little differently. Often, we tend to think of applications as expressions of a product or feature set. Especially when coming from a monolithic n-Tier Architecture background, it\u2019s easy to see an application as similar to an engineer itself: when given a problem, it figures out how to solve the entire problem and then solves it. For distributed architectures, it\u2019s better to instead think of each service as part of an assembly line. The service is responsible for taking the input (let\u2019s say a car door) and producing an output (attaching the door to the car). Its only responsibility is to take doors and attach them to cars. In fact, it doesn\u2019t even really care about the car or the door; it just attaches door-like things to objects on the expected attachment points. If the attachment points don\u2019t exist on either the \u201cdoor\u201d or the \u201ccar\u201d, there\u2019s a problem. As we\u2019ll learn later on, even the attachment points can be somewhat flexible. If the door has fewer attachment points than the car, but they still line up, the service might attach them anyway. If the door has more attachment points than the car, but the most important points still line up and the door still fits, the service might attach them anyway. This ensures that we don\u2019t need to retrain the service (i.e. rewrite code) when the door or car change slightly. The service can continue trucking along.","title":"Application Design Philosophy"},{"location":"Misc/distributed-architecture-strategy/#the-10-hops-rule","text":"The 10-Hops Rule is a guideline invented to help ensure overhead is minimized in microservice designs. The name is pretty self-evident to the philosophy: try to keep the satisfaction of any given user-level request below 10 service hops. This is just a guideline. Going over 10 hops doesn\u2019t immediately cause the entire design to fall apart like a bad game of Jenga. However, remember that every hop adds some amount of latency overhead\u201410ms is a good rule of thumb to help with planning. By the time you get to 10 hops, you\u2019ve added 1/10th of a second to the processing time of every user-level request simply through service communication. The 10-Hop Rule also helps control complexity. If a single request must pass through 50 services on its way to completion, keeping track of where that request is at along the line becomes much more challenging. Keep in mind that we\u2019re talking about 10 sequential hops. When you\u2019ve got some services talking asynchronously to each other, or one service talking to many services at once, you can determine the overhead using a sort of tiered diagram, as shown below. Note that the third tier\u2019s overhead isn\u2019t 60ms - service 2 and service 3 finished communicating before service 4, so only 40ms were spent waiting at that tier. To put it simply, each tier has as much overhead as the slowest service in that tier. Complexity , however, is still measured at the total number of services required to satisfy the request.","title":"The 10-Hops Rule"},{"location":"Misc/distributed-architecture-strategy/#fault-tolerance","text":"One of the bits of wisdom learned from our forerunners in the microservices space is the importance of building tolerant systems. While tools like Kubernetes allow us to basically infinitely maintain a system through anything but severe crash loops, that doesn\u2019t mean a given service\u2019s design should depend on such behavior. Every piece of a system\u2019s puzzle should still be a complete application; that\u2019s one of the benefits of a distributed system. Measuring the success of a distributed system by looking at single unit uptimes is a mistake. However, in aggregate, the uptime of the fleet contributes to the system\u2019s uptime of scale . In other words, if your services are constantly crashing and having to start up again, you\u2019re wasting processing time and potentially impacting metrics. This is the lesson others have learned: ensuring that the system\u2019s units are tolerant of faults and errors is critical to maintaining a healthy ecosystem. There are, of course, times when the application must crash. This should usually be mitigated to infrastructure-level errors that are unrecoverable. Otherwise, the top priority is making sure that the service is able to recover cleanly from these sorts of concerns: Bad or malformed input Erroneous requests Common runtime exceptions Temporary resource unavailability (and other transient faults) Service-to-service contract mismatches Tolerance doesn\u2019t just mean recovering from issues without crashing or corrupting internal state. Tolerance can also mean being lenient on how many problems can exist in an input while still being actionable, for example. The objective is to ensure that small changes in service-to-service communication, transient faults, miscommunications, file I/O issues, and such do not cause misbehaviors in the application. Additionally, we want to ensure that compatibility remains high. APIs should be designed in such a way to facilitate evolution without requiring changes on consumers. System infrastructure changes (for example, changing the URI of a resource) should be mitigated through tools like Kubernetes, where they can be abstracted from the beginning, or obviated through a no-deploy central configuration mechanism (again, perhaps in Kubernetes).","title":"Fault Tolerance"},{"location":"Misc/distributed-architecture-strategy/#avoiding-retry-storms","text":"You might have read the previous section and thought, \u201cGreat! I can make sure all my service-to-service requests are tolerant trivially. I\u2019ll just add a retry to the request!\u201d Congratulations, you just introduced a flaw that could take down the entire system\u2014maybe the entire data center. The difficulty in distributed systems is the scale: you might have 100 instances of a given service running, all getting requests at the same time. These 100 instances may, themselves, depend on another fleet of services, and them another set, and so on. If Service F becomes unresponsive for some reason\u2014say a network resource temporarily becomes unavailable or something like that\u2014then you end up with a traffic jam at that service. All of Service E backs up now, followed by Service D, C, B, and eventually A. It becomes a standstill. \u201cThat\u2019s OK,\u201d you say. \u201cAs soon as the network resource becomes available again, the jam will resolve.\u201d Perhaps that\u2019s true, except that Service E is programmed to retry its accesses to Service F. It turns out that Service D also retries its accesses to Service E, and Service C does the same with Service D. A lot of applications like to use ~1 second as a retry wait between attempts\u2026 Suddenly you have an exponential explosion of network traffic, as every service in the entire ecosystem starts retrying at the exact same time over and over. Services crash entirely because they starve themselves of network resources, infrastructure dies due to excessive load, and you more or less DDoS the data center from the inside. This is a retry storm , similar to the classic Thundering Herd Problem . Thankfully, it\u2019s easily mitigated. Principles of avoiding retry storms: Know when to implement retries at all. As recommended by Microsoft\u2019s general guidance on retries (for Azure, but applicable to any distributed system), it\u2019s not usually wise to implement retries at every service along the path of fulfilling a request. In many cases, the onus of retrying should be on one service\u2014usually the one most user-facing\u2014or the client application itself. This removes several aggravating factors from the retry storm scenario. Never infinitely retry. Eventually, allow the service to fail the request. The threshold for this will differ based on use case, but there comes a time that the request can simply no longer be fulfilled. Usually, this is implemented as a cap on the maximum number of retries. Never retry at the same interval between each attempt. Always use some sort of back-off strategy to ensure that your retries become less and less as the issue persists. Introduce some randomization to your retry intervals. That way not every instance backs-off and retries at the same time (in the case where they all run into issues at similar times). Consider implementation of the Circuit Breaker pattern instead of na\u00efve retries. This can be especially valuable when communicating with external services where failure is more likely to occur and be unrecoverable (as far as we\u2019re concerned). However, it can also hold value within ATLAS when communicating with highly-shared services like ATLAS Platform functionality\u2014your application should more or less behave as though those services are external. These mitigations cannot completely eliminate retry storms. No matter what, they will occur to some degree any time the system ends up in a bottleneck state and the bottleneck becomes clogged. By following these guidelines, however, the impact can be minimized.","title":"Avoiding Retry Storms"},{"location":"Misc/distributed-architecture-strategy/#communication-contracts","text":"One of the pesky recurring problems of software design is the management of communication contracts between services. Whether it\u2019s an evolving API, a code-level interface, or the response model from a service, it seems like a common source of regressions whenever two applications need to talk to each other. Since distributed architectures are built entirely on the concept of applications talking to each other, they can sometimes be especially prone to problems arising from miscommunication. There is no \u201cmagic bullet\u201d solution to the issue of communication between applications. In each instance, for both the API into an application and the responses coming out, one must exercise caution and diligence in design. As a general philosophy around how to design your communication contracts, try to follow Postel\u2019s Law. An implementation should be conservative in its sending behavior, and liberal in its receiving behavior. \u2014 Jon Postel To break this philosophy into more digestible chunks: Ensure your communication is as brief, small, and relevant as possible. This also means making sure your APIs are designed appropriately, such that each call is responsible for as little as possible. Responses from services should be very concise and contain only the immediate response to the request. Where possible, use schema-less models. Serializing and deserializing JSON or XML objects seems like a natural fit for service communication, but those formats enforce a schema that must be adhered to. If the schema is not met, the object will not deserialize, breaking the contract. This often causes minor changes to have a ripple effect throughout an ecosystem; even correcting a typo in an API spec can be impossible due to the massive number of consuming applications. One example schema-less pattern is the TolerantReader pattern . Where possible, avoid discrete versioning. (For example, moving from /api/v1/ to /api/v2/ is a discrete version change.) Never mark a new version for every change to the API. Creating a new API version implies that the old version must be supported\u2014not just individual pieces of it, but the entire contract. This requires the creation and enforcement of SLAs around its lifetime, deprecation, and eventual discontinuation. In the meanwhile, you likely incur technical debt supporting two different contracts. Versioning is painful for consumers. It forces them to adopt a new contract which, even with good documentation or release notes, requires thorough review and potential downstream overhauls. It discourages adoption of your API. That said, there are some scenarios where overhauls of your contract are required. In those scenarios, versioning can make sense. Probably best practice is to leave the API open for versioning (e.g. /api/v1/ ) but only take advantage of it when absolutely required. Utilize abstractions to remove dependencies on the contract details. This allows your logic to be ignorant of contract changes, except in the single source of translation (where the contract object becomes a DTO). Be explicitly backwards compatible\u2026 If you must advance your API version to v2, for example, create an SLA that dictates how long you will continue to support the v1 API. Ensure API consumers are informed of this SLA so that appropriate accommodations can (eventually) be made. \u2026but not forever. One of the flaws of supporting a highly-tolerant system for a long time is the accrual of technical debt in supporting many layers of backwards compatibility. Once a contract has been obsoleted, make and adhere to a plan to remove it entirely. Disable it programmatically, if possible, to reduce maintenance cost. When the API must change, try to make only additions. Given the above guidelines, adding to an API will not break existing consumers even if the new endpoints obsolete the old ones. Remember that the obsolete endpoints should be removed at a planned, communicated time. Have a clear definition of the required attributes in a contract, and concern yourself only with those. To put it another way, your code may not be the only consumer of the service, and therefore the response may contain properties you don\u2019t recognize. Discard them. On the other hand, if your code requires certain properties from the service\u2019s response that aren\u2019t supplied, and those properties were previously part of the contract, you have an error scenario. Or, for the same reasons, the API endpoints may support properties or parameters that your logic isn\u2019t aware of. Ignore them. If the service requires that your call provide attributes/parameters it can\u2019t possibly supply, you have a contract issue. It only introduces regressions to attempt to force contract compatibility with \u201cfake\u201d parameters or null values (unless the API docs specifically indicate null as a way to opt-out of that parameter). Even if it works, you\u2019re taking dependencies on incidental behaviors, which violates any contract. The service owner is not required to uphold incidental behaviors, which means your consuming logic is doomed to fail. Instead, where possible, reach out to the owner of the service and figure out how to reconcile the contract. If no reconciliation is possible, you\u2019ll need to either legitimately acquire the missing attributes/parameters (perhaps from another service) or find a different service to fulfill your request. Obviously, there can be exceptions. This is a set of guidelines, not rules. Just remember that every compromise on this philosophy increases the chance of regression and the cost of maintenance in the communication between applications.","title":"Communication Contracts"},{"location":"Misc/distributed-architecture-strategy/#deployment-tactics-and-service-management","text":"Creating an application is about more than just building the systems. You also must design and implement the deployment and management strategies for the application. Organizations (including ours) often have methodologies for these DevOps tasks whether they have dedicated DevOps engineers or not, but with distributed architectures it\u2019s critically important that these existing strategies are rethought. The rigors of these systems are often more complex, although not necessarily more challenging, than monolithic systems. It is ultimately more beneficial to discuss the health of a distributed system in terms of uptime of scale rather than traditional metrics. It is strongly recommended that each project team have at least one member fulfilling a DevOps role to design, build, manage, and monitor the deployment of complex distributed applications. However, this isn\u2019t always feasible for a number of reasons. As a result, the entire development team for a project backed by microservices should be fully aware of these tactics, strategies, and patterns for successfully administering and managing the application. To be successful, everyone must participate in DevOps.","title":"Deployment Tactics and Service Management"},{"location":"Misc/distributed-architecture-strategy/#service-level-agreements-slas-and-metrics","text":"Especially when dealing with services at a scope typical of microservice architecture, it is necessary to clearly define SLAs for systems. Without solid lines for \u201csuccess\u201d and \u201cfailure,\u201d these states are open to interpretation both from customers and internal stakeholders. Inevitably, your team will find itself constantly failing as the goal posts move based on shifting priorities, customer happiness or unhappiness, and internal perception. Establishing these contracts (in the abstract sense, though sometimes also literally) allows you to take control of the success and failure states and base them on concrete, measurable metrics. SLAs are comprised of one or more service-level objectives (SLOs). Your ability to base the SLOs on actual metrics ties directly to your system\u2019s ability to measure them. Thus, it\u2019s incredibly important to make recording metrics a first-class item of your design. Even if the metric isn\u2019t used as part of an SLA, record it anyway. The more data you have about the operation, state, and throughput of a system, the more information you have to diagnose issues.","title":"Service-Level Agreements (SLAs) and Metrics"},{"location":"Misc/distributed-architecture-strategy/#good-slas-vs-bad-slas","text":"The successful implementation of SLAs requires that they meet several criteria. The only meaningful SLA is one that actually has value and can be measured; if it can\u2019t be measured or it can\u2019t be met, it\u2019s not useful. SLA requirements: It must be achievable through normal maintenance and upkeep. In other words, the SLA must not require over-allocating staff or working overtime in order to meet it. It must be achievable and reasonably future-proof with infrastructure that exists at the time of the agreement. An SLA that requires overtime, over-allocation, or lacks proper infrastructure is doomed to fail, and therefore not a valid SLA. SLOs must be measurable. The metrics must be mathematically significant and defined within the agreement. \u201cThe customer must be kept happy\u201d is not a valid SLA because it\u2019s not measurable. \u201cCustomers must rate our service at least a B+ on surveys\u201d is not a valid SLA because it\u2019s not mathematically significant; there is no meaning to the measurement. \u201cThe queuing system must be capable of handling peak load\u201d is not a valid SLA because while peak load is measurable, and throughput rates are mathematically significant, neither of these thresholds are defined in the agreement. Not defining boundaries and thresholds in the agreements leads to scalability problems\u2014peak load will naturally increase over time given company growth, creating an inevitability where the system will fail to meet SLA. This is why you tend to see SLAs around uptimes and MTBF (mean time between failure) measurements: the target number is measurable, significant, and made part of the agreement. It must be honored. Obvious? Yes, but it should be reinforced: defining an SLA that will never be upheld creates friction on all fronts, wastes significant effort, and spends trust. Trust is the most valuable currency we have with both internal and external stakeholders. Spending it frivolously is not wise. It is better to have no SLA than to define one you can never (or will never) honor. The responsible team must be held accountable for failures to meet SLA. Given rule #1\u2014that the SLA is achievable\u2014if the team fails to meet SLA, actions must be taken. This does not necessarily imply punitive action; rather, a plan must be formulated to bring the service back into compliance and executed immediately. Some sort of post-examination of the violation should be conducted. Perhaps the environment has changed such that the SLA is no longer maintainable, which should be easily validated with metrics. Often, simple scaling or software tweaks can bring a service back into compliance. Possibly, the team doesn\u2019t have enough resources or people to meet SLA. Sometimes, a defect in the software may cause a violation of SLA. A fix should be identified, tested, and fast-tracked into deployment. Depending on the frequency of this type of violation, a broader look should be taken at the team\u2019s quality practices. Adjustments may need to be made to methodology. It is possible that the team simply failed to be responsible. Assuming that the company is hiring strong staff with good priorities, this should be fairly rare. It\u2019s often mixed up with one of the other issues due to a poor root cause analysis, though it certainly does occur on its own. These scenarios will be handled by the unit\u2019s usual method of addressing underperformance. It must provide business value. Creating an SLA around 99.999% uptime and a six-month MTBF is great if customers are paying to justify that level of service. Adhering to those standards, however, requires herculean effort and resources. If customers don\u2019t need it (or aren\u2019t paying for it), why waste the time, money, and effort providing it? This isn\u2019t to say that only customer-facing SLAs are important. Internal SLAs are just as important, if not more so since they directly prop up the external ones. Instead, focus on determining the level of service you need to guarantee, and build SLAs on that.","title":"Good SLAs vs. Bad SLAs"},{"location":"Misc/distributed-architecture-strategy/#slas-and-microservices","text":"This section on SLAs exists in this document because of the intrinsic link between the upkeep and management of a highly distributed system and the need for clear SLAs. By definition, the parts of a distributed and/or modular system will fail. Applying the usual monolithic architecture mentality to these failures will cause nothing but overreactions and negative perceptions. At sufficient scale, all distributed systems are some degree of \u201cdown\u201d at all times. The good news is: that\u2019s fine. It may be concerning to plan for a system where failures are commonplace, but part of the philosophy here is that failures are commonplace in every system, distributed or not. In microservices, you have the opportunity to plan for them and mitigate them architecturally. The highly-available, widely-scaled, and fault-tolerant nature of these services means that individual failures are of no concern. Instead, you watch for systemic issues and measure health in terms of uptime of scale \u2014this is why we must have strong SLAs. They help provide a measurable criteria for health and success. This measurable criteria can be called the steady state of intended behavior or throughput. As it turns out, the steady state and your system\u2019s SLAs go hand-in-hand; they\u2019re intrinsically bound. The primary way to identify systemic issues at scale is to compare the current state against the steady state.","title":"SLAs and Microservices"},{"location":"Misc/distributed-architecture-strategy/#the-stoplight-strategy","text":"To help with the management and monitoring of our distributed systems, a strategy has been devised to amplify the discoverability of systemic issues. This is the Stoplight Strategy. The idea is simple: a given system\u2019s state can be represented depending on how closely it is currently adhering to steady state. Our existing monolithic system is either up or down, but distributed architectures have a whole gradient of grey area in between. As alluded to earlier, most distributed systems at scale are always some degree of \u201cdown\u201d . The definitions of success and failure have to change. This leads us into a metric-based, SLA-oriented model of defining green , yellow , and red system states. It is critically important that we define the thresholds, SLOs, and criteria for a system\u2019s green , yellow , and red states early and concretely. They must be reevaluated often in order to keep in sync with the current SLAs and demands. Green state is when we meet or exceed the established expected performance, uptime of scale, latencies, throughputs, etc. of the system. All SLOs are being met or exceeded. Once we know our green state, we can start to define our yellow state. The yellow state is an operational state, but may be suffering from factors that impact user experience. Examples include high latencies, dangerously low uptime of scale, or an abnormal number of errors across the system in a given time window. The yellow state is incredibly important because it indicates two things: Failure is imminent There is an opportunity to improve the state of the system before failure occurs Red state occurs when some part of the system is fully down, or at least one SLO is not being met.","title":"The Stoplight Strategy"},{"location":"Misc/distributed-architecture-strategy/#the-stoplight-strategy-in-practice","text":"Important to keep in mind is that a green state doesn\u2019t necessarily indicate that the system is 100% healthy. It only indicates that the system is exceeding the criteria we\u2019ve established, and that\u2019s OK. We might still look at other metrics to evaluate whether there are concerns to be investigated, but assuming the green state is well-defined, the system is working as expected. Any concerns found should not be critical, though it is wise to be on the lookout for trends that will lead the system into a yellow state eventually. On the other hand, being in yellow state means that investigations should begin immediately. While the state is not critical yet, it usually indicates that it will become critical . If a system is in the red state, it should be considered completely down, even if it might still be servicing some requests. The intentions behind this strategy are almost more important than the strategy itself: A clear guideline and goal for establishing the metrics and monitoring around a system An implied dashboard that monitors the state and metrics of many systems using the same green / yellow / red indicators, even if the criteria differs between them, to provide an \u201cinfrastructure at a glance\u201d view A simple way to report on the current status of any given system for any stakeholders, internal or external You might notice that this system bears many similarities to something like Azure\u2019s Service Status page. This is fully intentional. The two strategies serve the same purpose, though for slightly different audiences. In fact, there\u2019s a lot of value in providing an internal version with greater detail and a version for customers that gives a different, simplified perspective. A simple stoplight-inspired indicator of state will never be sufficient to thoroughly monitor system states. It is not intended to be the only metric of health, or the only touchpoint of support. However, it does provide an easy-to-understand and quick reaction pivot point for all involved engineers. One of the problems in many of our existing systems is that we have a hard time saying\u2014at even a basic level\u2014whether they\u2019re healthy or not. We almost completely lack visibility. We have difficulty reacting quickly to changes in state, because there is no human-readable way to get state at a glance. This indicator helps by acting as a canary for potential issues.","title":"The Stoplight Strategy In Practice"},{"location":"Misc/distributed-architecture-strategy/#ci-and-cd","text":"CI (Continuous Integration) and CD (Continuous Deployment, in this case) are vital practices to maintaining a distributed application. A quick definition for each to help set context: Continuous Integration : developers push code to a central repository, from which the commits are immediately built and tested (via automated unit tests) to ensure valid and well-integrated code has been committed. Continuous Deployment : once validated by the CI process, the new version of the application is published and automatically deployed into production, most often via zero-downtime strategies. This is different from continuous delivery , where the application is built, validated, and published, but not yet deployed into production. It must be deployable, but the deployment occurs manually. Together, these processes help us control the complexity of distributed architectures. By employing CI, we ensure that each unit of the application is always in a good state\u2014that is, not completely broken or poorly integrated. Regressions are mitigated by automated testing. Most development teams use some form of CI today; there is likely no value in going further preaching its benefits. Continuous deployment is a more challenging process, but it too ensures that the application\u2019s units remain in a good state. The continuous delivery strategy is part of continuous deployment : the units must be easily deliverable and always deployable. However, going the extra step into automatically deploying units as soon as they\u2019re ready provides a ton of value, assuming your infrastructure and design is capable of zero-downtime deployments . The time-to-value for a given change is significantly reduced. Fixes deploy much faster, and improvements are delivered to users as soon as possible. The process overhead of deployment is removed entirely. There is an argument that the overhead is simply moved elsewhere in the workflow, usually manifesting as holding commits into master until they\u2019re ready for deployment. This is more desirable than delaying the deployment in later stages, as the roadblocks can be removed in case of emergency. Fixes can deploy as fast as code can be committed, instead of however long it takes a human to find the latest build, ensure it\u2019s valid, and initiate a manual deployment. The master branch always represents the deployed code. This allows teams to know, trivially, the state of the production environment, without having to accommodate for whether the code has been deployed yet or not. In a monolithic environment this is a non-issue, but in a distributed environment it becomes a morass of confusion. Better application design is encouraged in order to support zero-downtime deployments. Specifically, this encourages designs that are completely stateless, which has tons of knock-on benefits in terms of resilience and scalability. It all but requires a faster, more efficient, and lower friction database design, often leaning on databases which deploy as part of the application or schema-less technologies that prevent roadblocks on shared databases. Continuous deployment acts as a sort of CI for your distributed system. The goal of CI is to be constantly validating that your commits integrate well with the codebase. Continuous deployment allows you to constantly validate that your commits integrate well with the entire system . In CI, you continuously commit to the central repository to shorten the window of time in which your work could have gone out of sync with the codebase\u2014e.g. due to others\u2019 commits. In CD, you continuously deploy units for the same reason: to shorten the window of time in which your work could have gone out of sync with the rest of the system\u2014e.g. due to changes to other units. Also similarly to CI, regressions are discovered immediately, instead of whenever the commit happens to deploy. This has been implied above, but each unit of your distributed application should be a separate CI/CD pipeline. In other words, each unit should be capable of building and deploying independently of other units. Thanks to tools like Kubernetes and Docker, this becomes significantly more straightforward. Docker allows each unit to dictate its own build process in a Dockerfile. Kubernetes makes it simple for it to declaratively define its deployment in a .yaml . As these are (mostly) universal concepts, we no longer need to rely as much on extensive custom build and deploy scripts in a system like TFS. For monolithic applications, it isn\u2019t the worst thing in the world to write custom build/deploy scripts, but at distributed scale it\u2019s impossible to maintain.","title":"CI and CD"},{"location":"Misc/distributed-architecture-strategy/#example-ci-and-cd-workflow","text":"Here\u2019s an example of how our CI/CD workflow might look. This isn\u2019t exactly how our actual workflow will look, but it should help demonstrate the concepts. A developer named John Doe makes changes to the code in a branch. Let\u2019s call it john.doe/feature-branch for example\u2019s sake. John commits code to john.doe/feature-branch during active development. If necessary, John has a path on the Docker registry (e.g. Harbor) for pushing images manually for testing, though they cannot push to certain protected paths like the root. A Test environment exists as well for manually testing deployments and for testing units (or whole applications) at scale without affecting Production resources. After development and initial testing, a pull request is submitted to merge john.doe/feature-branch into master . The PR contains not just code changes, but potentially build and deploy changes as well. The repository\u2019s policy requires that the DevOps group approve the PR if the build and/or deployments change, in addition to the usual approvals. The DevOps group and the usual suspects approve the PR. It merges into master . The build server detects a change to master and initiates the CI process. The image is built and automated tests are executed. Assuming the build and tests pass, the new image is pushed to the registry under a specific version number. After the image is pushed to the registry, it is vulnerability-scanned. Assuming the scan completes successfully, the CI process is complete. The image is tagged as latest in the registry. Now the CD process begins. The build server initiates a deployment of the Kubernetes .yaml . Since the DevOps group signed off on the PR, the deployment should be safe. Using k8s mechanisms for upgrading deployments, the existing pods are rolled off one-by-one, with new ones created in their place. Assuming the deployment enters a good state before a configured timeout passes, the CD process is complete. The new unit can be considered fully deployed now.","title":"Example CI and CD Workflow"},{"location":"Misc/distributed-architecture-strategy/#summary","text":"After a pretty solid amount of research and evaluation, this document contains the current philosophy around ATLAS\u2019s approach to distributed and microservice-oriented architectures. This philosophy will change with time and experience, but it represents our idealistic strategy at the time of writing. As mentioned in the first section, we always want to design the \u201ccorrect\u201d implementation first. Make compromises only when the correct design is unachievable, and make them as small as possible. If you begin designing from a compromising mindset, your design will not reach the heights it may have otherwise been able to reach despite limitations and setbacks. The collection of strategies around implementing distributed architectures are a first draft. These, too, will expand and evolve as we gain experience. Perhaps the idea of a \u201cstoplight\u201d system of measuring system health is too na\u00efve to bring value, or perhaps it\u2019s too complex for the target audience even as presented. Maybe it turns out that the Circuit Breaker pattern just doesn\u2019t work for us. It\u2019s possible that continuous deployment turns out incompatible with our team structures. Most likely, we will add powerful solutions we discover along the way. At no point should this document be considered final or even authoritative. The document is, however, a set of guidelines and philosophy that should help everyone work into this new way of thinking. This is its only true objective. Putting yourself into the mindset for successfully creating a distributed system\u2014that is, avoiding the common pitfalls and long-term pain points that often plague distributed systems built by monolithic teams\u2014requires more than a basic understanding of the tools. It requires a philosophical and fundamental shift in many of the assumptions and basic foundations that we build up after years of experience. As alluded to before, distributed systems have existed since the birth of networked computing. Today they\u2019re just easier than ever before to create and manage. The tools we have now make designs practical that were once infeasible.","title":"Summary"},{"location":"Misc/kubernetes-deployment-guidelines/","text":"Kubernetes Deployment Guidelines https://stackoverflow.com/c/atlastechnology/articles/49","title":"Kubernetes Deployment Guidelines"},{"location":"Misc/kubernetes-deployment-guidelines/#kubernetes-deployment-guidelines","text":"https://stackoverflow.com/c/atlastechnology/articles/49","title":"Kubernetes Deployment Guidelines"},{"location":"Projects/database-and-schema-design-guidelines/","text":"Database and Schema Design Guidelines Important This document is a WIP. At time of writing, it\u2019s little more than rough thoughts. The document will be more fully developed in the coming weeks. These guidelines and considerations are a great starting place for understanding the conventions and practices we espouse. As with all guidance like this, however, there are definitely exceptions. Use your best judgement and discuss with DB Architecture to determine the correct path forward. DBMS Design/Architecture Considerations When selecting PostgreSQL as DBMS, use PostgreSQL 11. There are no known reasons to use 10 or below in our use cases. \u2026 Naming Conventions Identifier casing conventions: In SQL Server and Vertica, use PascalCase naming. In PostgreSQL, use lower_case_underscored naming. We don\u2019t typically use tbl_ or vw_ / view_ prefixes for tables or views. (Currently unknown if this guidance can apply to all DBMS platforms.) Schema names should be short but specific and readable. \u2026 Schema Design Guidance Tables should try to use a natural key as a primary key where possible, unless the applications has specific performance needs. In PostgreSQL, use identity instead of serial as identity is more compliant. If you use separate schemas, they should not be joined to each other except through a single specially designed join table. This helps enforce a separation of concerns between schemas and create an authoritative mapping of truth across separate conceptual domains (the schemas). Quantities are always a numeric -equivalent type, not an integer type. An identity column should never be a primary key. If you must use one, you must either have a unique key elsewhere or declare in design that the table has no requirements for uniqueness (e.g. a log table). \u2026 Database Code Conventions Never use SELECT * or equivalent in queries that are a part of application logic. Using it in \u201cad-hoc\u201d investigative queries or during debugging is fine, but it should never appear in production queries. \u2026","title":"Database and Schema Design Guidelines"},{"location":"Projects/database-and-schema-design-guidelines/#database-and-schema-design-guidelines","text":"Important This document is a WIP. At time of writing, it\u2019s little more than rough thoughts. The document will be more fully developed in the coming weeks. These guidelines and considerations are a great starting place for understanding the conventions and practices we espouse. As with all guidance like this, however, there are definitely exceptions. Use your best judgement and discuss with DB Architecture to determine the correct path forward.","title":"Database and Schema Design Guidelines"},{"location":"Projects/database-and-schema-design-guidelines/#dbms-designarchitecture-considerations","text":"When selecting PostgreSQL as DBMS, use PostgreSQL 11. There are no known reasons to use 10 or below in our use cases. \u2026","title":"DBMS Design/Architecture Considerations"},{"location":"Projects/database-and-schema-design-guidelines/#naming-conventions","text":"Identifier casing conventions: In SQL Server and Vertica, use PascalCase naming. In PostgreSQL, use lower_case_underscored naming. We don\u2019t typically use tbl_ or vw_ / view_ prefixes for tables or views. (Currently unknown if this guidance can apply to all DBMS platforms.) Schema names should be short but specific and readable. \u2026","title":"Naming Conventions"},{"location":"Projects/database-and-schema-design-guidelines/#schema-design-guidance","text":"Tables should try to use a natural key as a primary key where possible, unless the applications has specific performance needs. In PostgreSQL, use identity instead of serial as identity is more compliant. If you use separate schemas, they should not be joined to each other except through a single specially designed join table. This helps enforce a separation of concerns between schemas and create an authoritative mapping of truth across separate conceptual domains (the schemas). Quantities are always a numeric -equivalent type, not an integer type. An identity column should never be a primary key. If you must use one, you must either have a unique key elsewhere or declare in design that the table has no requirements for uniqueness (e.g. a log table). \u2026","title":"Schema Design Guidance"},{"location":"Projects/database-and-schema-design-guidelines/#database-code-conventions","text":"Never use SELECT * or equivalent in queries that are a part of application logic. Using it in \u201cad-hoc\u201d investigative queries or during debugging is fine, but it should never appear in production queries. \u2026","title":"Database Code Conventions"},{"location":"Projects/product-lifecycle-requirements/","text":"Product Lifecycle Requirements As projects begin, develop, and mature, they eventually blossom into full-fledged products. Along the way, there are several important steps to take that help ensure the product turns out solid. Too frequently ignored, however, is the follow-through once the product has matured. Continuing to manage the lifecycle of a product, even once it\u2019s \u201cdone\u201d, is just as important to its quality, and often the quality-of-life of your team as well. Table of Contents Looking for the TL;DR ? Here are some shortcuts to common reference sections: Product Versioning Requirements Support and Maintenance Checklist Documentation Requirements Product Evolution Requirements Full Table of Contents: Product Lifecycle Requirements Table of Contents Lifecycle != SDLC What is a Product? The Structure of this Document Reading the Requirements Versioning and Operation Lifecycle Public Version vs. Internal Version Choosing Versioned or Rolling Versioned Products During Design During Initial Development Upon Initial Release Subsequent Versions Discontinuing a Product Rolling Products During Design During Initial Development Upon Initial Release Subsequent Versions Discontinuing a Product Responsibility, Accountability, and Ownership Maintenance and Support Checklist The Importance of Documentation Product Growth The Importance of Innovation The Importance of Evolution Evolution Requirements Summary Lifecycle != SDLC While this document deals with the lifecycle of products at ATLAS, it is not a replacement for an understanding of Systems/Software Development Lifecycle (SDLC) approaches. It also does not prescribe an approach. Instead, it addresses how the company in particular develops and maintains its products. These requirements should be applicable to any SDLC process. This distinction is made because SDLC is a loaded term. Sometimes it can refer generically to any lifecycle approach, but often it refers to a very particular process that heavily promotes waterfall development. ATLAS is not a waterfall shop, and many aspects of the colloquial SDLC methodology don\u2019t fit well. As a result, the term \u201cSDLC\u201d will be avoided from here. What is a Product? It\u2019s easy to look at the front-end applications, which receive all the marketing and sales focus, and call those the products. From a business and sales perspective, that\u2019s probably true. Those are the products of the company . Engineering, however, has many more products than just those SKUs. This document uses the term product frequently and loosely, but what is a product? Within the context of these requirements, basically everything is a product. They apply to all efforts. While the \u201cproductness\u201d of some sub-modules or parts of larger applications might be debatable, assume that these requirements apply when in doubt. The Structure of this Document These requirements are broken into three major categories that encapsulate most of a product\u2019s lifecycle. The first section is about the operation of the product , specifically around managing the initial effort, ongoing development, and its relationship with its users throughout. The second section details matters of responsibility, accountability, and ownership of the product as it relates to the support and maintenance from both a business and technical level. Finally, the last section covers the growth, innovation, and upkeep of products\u2013how they\u2019re kept from stagnation and how technical debt is paid off. Reading the Requirements When interpreting the verbiage of the requirements, know that it was inspired by RFC 2119 . As a quick reference, requirements using must , shall , must not , and shall not are absolute. Should , should not , recommended , and not recommended are strongly encouraged but there may exist valid reasons to avoid them. May , may not , and optional are optional, but acknowledged as part of the problem space. In all cases, the letter of the item is less important than the spirit. Reach out to team leadership if there are questions around interpretation. Similarly, no matter how absolute the requirement in this document, nothing overrides the fact that business must continue to operate. These requirements are a tool to assist in cooperation and ongoing value-creation for our customers; they are not a book with which to beat others. Versioning and Operation Lifecycle The versioning and operational requirements laid down in this document are bisected into the categories of versioned products and rolling products . Neither of them necessarily reflect anything about the development or release methodology of the product itself, though obviously some practices tend in one direction or the other. Instead, the focus is on the two different sets of expectations that consumers of the products harbor. The requirements are guided by these expectations, and differ as a result. ATLAS uses a versioned approach most often for technical modules like code libraries, containers, and (rarely) some APIs. These are strongly versioned to provide a contract to their consumers. The contract serves as an interface guarantee; a statement that the product at version a.b.c does exactly what it does the same way every time, while version x.y.z may behave differently. On the other hand, ATLAS usually relies on a rolling approach for its front-end, customer-facing software. Here, the contract between developer and consumer is much softer. It\u2019s implied that in exchange for a live, hassle-free stream of updates, users receive new versions automatically and regularly\u2013usually without their explicit consent or action. These products still have versions, of course, but the version is not part of that developer/consumer contract. Instead, there is an understanding that the developers will do their due diligence to ensure smooth transitions between releases, since users cannot opt out. There are certainly crossovers between these domains. However, the important part is keeping in mind the consumer\u2019s expectations. One must design these into the product, as they will heavily affect its lifecycle. Public Version vs. Internal Version It\u2019s important to note that there is a clear distinction between the consumer-facing (\u201cpublic\u201d) version number of a product and its internal version as the development team knows it. No matter the approach, the internal version of a product must always uniquely represent a specific build of it\u2013or, more accurately, a reproducible point-in-time instance of it. The public version of the product may be the same as the internal version, but rarely will. The requirements in this document almost exclusively cover public versions. The public version number of the product is representative of its feature set rather than any given point-in-time instance. Rolling products must not have a prominent public version number. As discussed in later sections, the public version number is tied to a particular release rather than a build instance, and always requires a developer\u2019s hand to maintain the contract implied by the version number. For example, if a product follows Semantic Versioning , there is meaning behind each part of the version number that must be upheld. There\u2019s another version indicator that can exist as well: a marketing version number. Using Windows as an example, Windows 10 is not (internally or externally) version 10.x.y.z of the OS software. The 10 is just part of the marketing. These indicators are of little concern to the development team, though there are a few considerations. Choosing Versioned or Rolling When starting a new project, choosing the operational lifecycle strategy is key to developing and maintaining it successfully. Making this decision can be challenging, but there are a few guidelines to help. Versioned : \u2795 Creates a stable interface for consumers to rely on \u2795 Gives the dev team an easy path to introduce breaking changes when necessary \u2795 Allows clear communication around support windows \u2796 Tends to slow the pace of releases \u2796 Tends to stagnate consumers Consumers will not upgrade unless they are forced or convinced Forcing consumers to upgrade should come with a clear communication of deprecation to all consumers, a generous timeline for them to switch, and (for critical products) an out-of-band window to support those that cannot or will not upgrade. Convincing them requires publishing detailed change notes and, sometimes, marketing and promoting the new version (even internally) \u2796 Can be a burden on maintenance Depending on the users of the product and your relationship with them, you may be forced to support many versions simultaneously \u2796 Disadvantageous for service-oriented products Some products are inherently versioned. Code libraries are a prime example. Organizations occasionally hybridize the two approaches\u2013for example, automatically releasing nightly \u201cunstable\u201d versions or only making the latest version available\u2013but the product is still consumed at a specific version. The consumer usually still has to seek out and upgrade versions themselves, and in those scenarios most consumers are encouraged to wait for full releases. Even versioned products with auto-updaters are still versioned. Rolling : \u2795 Allows for an uninterrupted flow of new features to consumers \u2795 Smaller, non-breaking bug fixes can be applied as needed, enabling prompt resolution of issues \u2795 Only a single \u201clive\u201d version to worry about \u2795 Tends to accelerate the pace of releases \u2796 Communication around support windows lacks clear points of delineation \u2796 A deft approach to change is required from developers Since there is no ability to opt-in or opt-out of changes, any planned breaking change must be clearly communicated to all consumers They must be given a generous timeline to adapt, sometimes requiring training Every upgrade, enhancement, new feature, and regression goes live for all consumers This necessitates a more thorough testing methodology and more careful design\u2013not necessarily bad things, but certainly time-sinks Leads to strategies such as blue/green deployments; these strategies add complexity and overhead It often leads to strange workarounds to avoid introducing breaking changes alongside new functionality These workarounds/hacks more easily become permanent, due to the lack of clear delineation around support windows, which can eventually (if unmanaged) create large amounts of technical debt \u2796 Tends to increase burden on support teams/documentation Consumers are likely to find themselves left behind if they step away from your product for a time Even consistently engaged users are more likely to miss new functionality without strong update communication \u2796 Cannot cleanly apply to some product types Most SaaS products (including web applications) use a rolling model. Even their APIs are rolling, though often they\u2019re hybridized. In the common hybrid model, the consumer declares a version in their request to specify the desired functionality and compatibility. However, they\u2019re still forced to use the current interface; the version request is a function of the software rather than a fundamental part of the contract. It\u2019s an inversion of control; the user must ask to be allowed to tap into compatibility with old versions, rather than the user driving which version is used and the developer trying to shepherd them forward. Versioned Products During Design The intended consumer audience for the product must be set. Remember: a consumer might be a customer user, an internal user, or even another development group. No matter who the audience is, treat them as your consumer. Consumer expectations for lifecycle must be defined. If you\u2019re here, you\u2019ve probably already done this. The release mechanism must be established. How will consumers get new versions? SLAs around support must be quantified. What expectations should be set around how long a given version will be supported? What is the plan for enterprise-level support of out-of-band versions (if applicable)? Versioning rules for the product must be declared. Either they are defined in the design, or a reference is provided to the standard scheme used. One should strongly prefer Semantic Versioning over other schemes. No matter the versioning scheme chosen, it must meet these criteria : Some indicator of major version and minor version must be available at a minimum. Further indicators, such as patch numbers and tags to indicate development status (e.g. 3.0.4-beta ), are allowed. Additional indicators (patch numbers, etc.) must have their meaning and rules clearly defined in the design, especially as they relate to the major and minor version. Incrementing the minor version must never introduce known breaking changes. Incrementing the major version should reset the minor version. In other words, the minor version should always be a subset of the major version. The version indicators must always be mathematically comparable first and lexically comparable second. The comparison must linearly indicate the release history of the product relative to other versions. This is to address a common error. Version 3.10 must always be a later version than 3.2 when comparing version numbers\u2013specifically, 8 minor versions later. During Initial Development The parties responsible and accountable for the product\u2019s ongoing maintenance, development, and support must be established. Consumer-facing versioning is not important during initial development until the product is made available to consumers for the first time. Expectations should be set that the \u201cinterface contract\u201d does not begin until the product\u2019s initial full release. To clarify, in-development versions are likely to have breaking changes constantly. However, the responsibility is on the developers to ensure that whatever version consumers are likely to take dependencies on is the initial full release . In other words, don\u2019t hand consumers a mostly-functional version of the product and expect them to not start relying on it, no matter what the version number is. It\u2019s on you, the developer, to manage expectations. Internal versioning begins ASAP during initial development. The internal version indicator should not be prominently visible publicly. It must always be possible for maintainers/supporters to discover the internal version of the product in the wild (e.g. once deployed or installed somewhere). It is strongly encouraged to use a different versioning scheme for internal versions. Prefer ones that include a temporal element. Upon Initial Release The initial release shall be version 1.0 (or equivalent in your scheme). The full interface contract is now in effect. Version indicators must now fully adhere to the versioning rules laid out in the design, including the initial release. Subsequent Versions Changes in ownership of the ongoing support and maintenance must be continually well documented, and that ownership must always be clearly defined. Release versions shall be a concern of the release pipeline/process, and shall always reflect the consumer-facing (\u201cpublic\u201d) version indicator. No system shall automatically increment the major version of a product. Build systems shall not increment the major or minor version of a product. Only release processes can affect them. These requirements are in place to convey the meaning behind the version. No automated system is capable of determining what the versioning of the next iteration of the product should be. However, it is acceptable for a release system to, by default, increment a minor version. It should not be possible for a release to be created that generates an out-of-sequence, historically inaccurate, or repeat version number. All major and minor releases shall be communicated to consumers via appropriate means. New releases shall include change notes. These can be as simple as a summary of the whole update (\u201cfixed bugs with loading report options\u201d), or as complex as a full changelog. The format of these notes depends on the intended consumer of the product, but should tend toward being as informative as possible. Changing the versioning scheme for a product must occur only at new major versions. It is considered a breaking change. Major version releases should only be made when breaking changes are introduced. The development team shall support the entire current (latest) major version series. Example: if Product X is at version 3.4.1 and a consumer using version 3.1.8 finds a bug, this is a valid issue and must be supported. Note that \u201csupport\u201d does not mean that a special version 3.1.8-fix should be created. If the issue is already fixed in a later release, it is valid to ask the consumer to upgrade. If the issue still exists in the most recent release, it is valid to fix it in a future release. The development team shall not make the immediately previous major version series of the product unavailable. Example: Product X is at version 3.4.1 . The team then releases version 4.0.0 . It cannot make the 3.*.* series of the product unavailable. It could take the 2.*.* series offline if necessary. This requirement is in place to provide some level of security to consumers\u2019 downstream dependent systems (e.g. build pipelines). Although the 3.*.* series is still available in this example, availability does not necessarily imply support (see below). Versions prior to the initial release do not apply. There is one exception: when the software has ceased to function properly in uncorrectable ways outside of the dev team\u2019s control, it may be made unavailable. The development team shall support the most recent version of the immediately previous major version series of the product. For example, if the product is version 3.4 and then 4.0 is released, version 3.4 must continue to be supported. Version 3.3 may go unsupported. When version 5.0 releases, support may end for 3.4 . As deemed necessary by product leadership, this support can be sunset. Sunsetting the previous series is a double-edged sword: Setting a sunset date requires that support for the series continue through that date. This supersedes the requirement to only support the current and immediately previous series, so the team may end up supporting many series. A generous window must be given before end-of-life. Consumers must be notified of the sunset date using appropriate means. The product team may choose to support more versions than the minimum outlined here. If so, the policy must be documented alongside the versioning scheme. Fixes should almost always be made against the latest series. Additions shall always be made against the latest series. It is strongly discouraged to branch the product into two actively maintained series (like how Python 2 and Python 3 worked until 2020). However, in some rare and unusual cases, the product\u2019s leadership may choose to do this. Discontinuing a Product There is currently no set of requirements or guidelines for discontinuing a product. A process will need to be established before requirements can be created. Rolling Products These are often called \u201clive\u201d or \u201cservice\u201d products, and they represent a vast majority of the software consumers use today. During Design The intended consumer audience for the product must be set. Remember: a consumer might be a customer user, an internal user, or even another development group. No matter who the audience is, treat them as your consumer. Consumer expectations for lifecycle must be defined. If you\u2019re here, you\u2019ve probably already done this. The release mechanism must be established. How will consumers receive the live version? Any expectations or agreements around product features or interface details (especially relating to consumer design input, demands, legal agreements, or otherwise) must be well-documented so that future maintainers are aware. It should be decided what SLAs are needed around feature support and product/service availability. Since the consumers are not able to opt out of updating, features they may rely on must be protected by some kind of SLA, even if its only for internal accountability and confidence. Many rolling products are SaaS offerings; this implies that the product is only usable when the service is available and functioning as intended. Because of this, the product\u2019s availability must be protected by some kind of SLA, even if its only for internal reference and confidence. During Initial Development The parties responsible and accountable for the product\u2019s ongoing maintenance and support must be established. A strategy for smoothly transitioning into breaking changes must be developed. Despite the rolling nature of the product, it is not valid to break functionality without ensuring minimal consumer disruption. This strategy includes: Software design accommodations for changing requirements (so that a transition plan can exist later) Communication plans How are consumers to learn of changes to the product? What are the impacts? What is the value for them? When is the change expected? An approach to gate delivery of features or changes, such as feature flags or blue/green deployments The dev team must have a plan for determining consumer acceptance (e.g. UAT). This is true for versioned products as well, but becomes especially important for rolling products in subsequent releases. A rolling product, when it must take dependencies, should prefer to take dependencies on versioned products. Versioned products provide a level of reliability that rolling products cannot. Depending on them allows your live rolling product to better isolate issues and remove variables during troubleshooting. Internal versioning begins ASAP during initial development. The internal version indicator should not be prominently visible publicly. It must always be possible for maintainers/supporters to discover the internal version of the live product. Rolling products are strongly encouraged to either include a temporal element in their internal version, or be able to easily track backwards into one. This is important to help establish a historical reference of versions that otherwise may not exist if, for example, the internal version is based entirely on an arbitrary incrementing ID. Upon Initial Release The product shall not be referenced by a version indicator when communicating with consumers, with the exception of any marketing version applied to it. This includes technical consumers and internal (developer) consumers. This requirement exists to prevent internal versioning from becoming external versioning. Versioning a rolling product is a violation of its implicit contract with consumers and sets the wrong expectations with them. Changes, fixes, and feature additions can instead be referenced by release timeline or development milestone, depending on product leadership\u2019s preference. Subsequent Versions The development team must not introduce breaking changes without generous warning to consumers. For breaking changes, a transition plan must be developed to ensure as seamless an experience as possible. It should follow the strategy designed during initial development. Changes in ownership of the ongoing support and maintenance must be continually well documented, and that ownership must always be clearly defined. Changes to the consumers\u2019 expectations of the product, agreements (legal or otherwise), and design input over time must be continually well documented. New releases containing new features, important fixes, or impactful changes shall coincide with change communication to the consumer. This can be as simple as a summary of the whole update (\u201cfixed bugs with loading report options\u201d), or as complex as a full changelog. The format of these notes depends on the intended consumer of the product, but should tend toward being as informative as possible. The team should continue to support deprecated features for a generous period of time. The window should be large enough that even consumers using the product in unintended ways, who act on elongated timelines, and who require some coercion are able to adapt before the feature goes away. Security issues are an exception; features found to suffer from a security vulnerability can be removed immediately as a result. If the feature was slated for removal anyway, it can remain absent. If not, it should be reintroduced once the issue has been addressed. One should still notify consumers when these changes are made. The team should track metrics around feature utilization, especially around features that are currently or will soon become deprecated. This data should be used to inform transition plans and better target enhancements. The team must continue to prioritize consumer acceptance (UAT) for each new release, especially those with new features. A strategy must exist for correcting when breaking regressions are accidentally introduced. Whatever the strategy, it must include a catch-all safety, such as a full rollback, that ensures the product becomes fully available in an accepted state again. This is because consumers do not have the ability to opt out of updates that happen to be non-functional; some guarantee of restoration of service is required. The development team must not provide differently-versioned instances of the same product. In other words, even if multiple instances of the product are live at a given time, all instances must be the same internal version. This only applies to production environments, of course. Exceptions to this can be made, but only by company leadership. Supporting multiple versions of a rolling product turns it into a versioned product (but without the positive attributes normally associated with them). Support and maintenance costs are an order of magnitude higher. Discontinuing a Product There is currently no set of requirements or guidelines for discontinuing a product. A process will need to be established before requirements can be created. Responsibility, Accountability, and Ownership A product\u2019s lifecycle is about more than versioning new features and managing consumer expectations. The product must also be continually supported and maintained. It is not acceptable to have products that are unattended. Having a clear and complete understanding of who owns a product\u2019s lifecycle and what that ownership entails is critical to its continued success. Responsibility vs. Accountability It is important to differentiate between responsibility and accountability. The responsible party is the one tasked with the duty. The accountable party is the one held liable for the results. The two can be the same but aren\u2019t necessarily so. Maintenance and Support Checklist This section provides an easy checklist of maintenance and support questions that must be answered for every product. The answers to these questions must always be available in product documentation during all stages of its lifecycle. They help ensure everyone is aware of which team is responsible for the ongoing support and who is accountable for the product\u2019s continued success. The responsible and accountable party can shift over time, but it must always be clearly defined. Note that the separate pieces/services which make up a product may have separate answers to these questions . For example, a website product with a backing database might have one team monitoring the health of the site and another monitoring the health of the database. This is disadvantageous, but until a DevOps methodology is fully embraced, is sometimes required. Be sure that the separation is clearly denoted in documentation. Who is the product team? Who is the product owner? What roles make up the dev team and who is filling them? What other roles are dedicated to the product and who is assigned to them? Who is responsible/accountable for monitoring the product? Monitoring includes ensuring that the product is operating within acceptable parameters and meeting SLOs. It also includes proactive examination of metrics to anticipate future problems or concerns. Monitors are responsible for raising the alarm when the product deviates, or is forecasted to deviate, from its \u201cgreen\u201d state. Automated monitoring is encouraged, but a human must always be available\u2013either alongside the auto-monitoring or \u201con call\u201d\u2013to ensure its signals are observed and reacted to. The objective of the monitoring effort is to help ensure that the product continues to meet business needs at the real-time level. Some product types require no monitoring, for example if they are statically-delivered and \u201cpackaged\u201d applications, but this is becoming rare. Who is responsible/accountable for the customer support of the product? In other words, who engages with the consumers of the product around questions, issues, requests, and so on? This party must be assigned even when the consumer of the product is an internal audience. Who is responsible/accountable for the technical support of the product? In other words, who can answer technical questions about the workings of the product and its components? Who is responsible/accountable for the ongoing maintenance of the product infrastructure? These concerns include making sure database maintenance is performed, security patches are kept current, backups are valid and can be restored, log folders are cleaned, scaling is in line with demand, and so on. Who is responsible/accountable for the business alignment of the product? Almost always, this will be the product owner. The product owner will react to the business needs and express the alignment as stories and prioritization. Sometimes, a product owner does not exist. Even in this case, some party must be responsible for ensuring that the product moves forward and aligns with direction. The objective of this effort is to help ensure that the product continues to meet business objectives at a tactical and strategic level. Who is responsible/accountable for the technical upkeep and evolution (growth) of the product? The nature of entropy dictates that over time, all applications will decay unless they continue to evolve and, at minimum, keep up with their surrounding environment. This means that someone must be responsible for ensuring that the evolution of all supported products is represented in discussions around prioritization and value. Products left untouched for long periods of time accrue technical debt simply by existing. The knowledge necessary to support and maintain the product fades over time, and eventually even minor fixes become large efforts as the code must be relearned (often by someone new). The Importance of Documentation The original development team of a product will not always be its shepherds. Even if the product doesn\u2019t change hands directly, the team itself will change over time. The only way to preserve the decisions made about the product\u2013whether during initial design, original development, or as part of on-going growth\u2013is to document them. There must be a centralized location for all documentation related to the product\u2019s design and development. PMO-based documentation (project submissions, etc.) are encouraged to be located here as well but not required. Confidential legal artifacts, financial artifacts, and (hopefully a very rare scenario) tightly-held company secrets should not live here. Documentation shall cover technical and business decisions made, the operation of the product, dependencies, assumptions, and other intangibles. Product docs shall be available for any member of any development team within the company to view. This allows a cross-pollination of information, removes variables around security and access (which saves Ops time), and encourages a sharing of ideas and collaboration. It also saves time when the team responsible for the product changes. There is a window during the initial design of the product where documentation can be less widely available, to avoid exposing incorrect information. However, the docs should be made available ASAP. Secrets should not live in centralized documentation. This is especially true for passwords, authentication tokens, keys, and so on. Instead, secrets should live in a secret store (such as a key vault). Documents shall be allowed to evolve and change alongside the product as necessary. However, this evolution should happen within a change-tracked system, so that historical versions of documents can be retrieved. This requirement exists to ensure that knowledge-sharing documents are not encouraged to freeze and become outdated. An exception: documents which must freeze in order to represent authoritative or process-based finality (such as forms approved as part of project submission) are allowed to do so. All currently supported versions of the product must be covered by the documentation, either within a single set or by multiple parallel sets. Product Growth All products that are to be supported for any significant amount of time must continue to grow. We cannot allow products to stagnate or rot, even those products which are \u201cfinished\u201d. This section discusses how products should continue to innovate and evolve over time, even if they are already providing stable business value. The Importance of Innovation Warning This section covers philosophy and approach rather than direct requirements. It is still very important; it provides context to the requirements that come in the next section. Here\u2019s a very curt statement: A stagnant product leaves money on the table. It may be blunt, but it\u2019s a core philosophy of successful software companies. If you have a product you\u2019re still supporting/selling/distributing, letting it languish is robbing the company of potential value. It may not be literal dollars, but there are many factors contributing to \u201cvalue\u201d which can be represented either directly or indirectly by money. Actual money, in new sales contracts, add-ons, or ad-hoc purchases Expense which could have been reduced Opportunity cost Customer/user faith, trust, and interest Competitive advantage Like all things, software is subject to entropy. A new product is fresh and exciting, even something as simple as a development library. There is inherent momentum behind a new release. As that momentum dies down, however, users become bored and disinterested. This is one reason why you often see products like Facebook make large changes to user experience even though the existing one worked \u201cjust fine\u201d\u2013change keeps the momentum going. It keeps users engaged and gives you the opportunity to introduce new features and functionality which might lead to greater value. It\u2019s reasonable to react to this philosophy with some negativity. \u201cWhy make changes to a product which is bringing value and risk bringing less value?\u201d There is some validity to this reaction, certainly. Not all markets react the same way to stagnant products. However, a stagnant product\u2019s value often has a peculiar curve, as demonstrated by this rough graph. Post-release, the product\u2019s value skyrockets to its peak after a warm-up period. From there, without any further releases, the value holds briefly before beginning to taper off. Eventually, the value will near zero. Obviously, this is not hard data based on any particular case study; it\u2019s an abstraction of the philosophy. Most products will deviate from this on the micro level, but at the macro level, experience holds this to be accurate more often than not. On the other hand, here\u2019s the product\u2019s value with additional releases that bring new functionality, a refreshed user experience, or even (less sustainably) important bug fixes. Again, these graphs are not hard data, but this approach has been proven time and again by various companies. Innovation is its own value, and often holds the key to unlocking the long-term potential of the product. The Importance of Evolution A central part of product ownership is being committed to its technical advancement and upkeep. Innovation is very important, but tacking new features onto a product without keeping the underlying infrastructure and technology up-to-date is like adding rooms onto your house when the foundation is cracking. You might be successful at first, but without addressing the underlying issue, the house is doomed. The importance of evolution is to not just patch up the foundation as it starts to fall apart, but to proactively improve and enhance it so that you can more easily innovate. Perhaps a more familiar metaphor is technical debt . Through sheer entropy, code left unimproved for long periods of time will accrue debt. Quick fixes and hasty bolt-ons will act as loans, creating more debt. Eventually, if you don\u2019t pay it down, it will begin to compound on itself until the burden of it is too much to repay. Often, you\u2019re left with what seems like only a single option: technical bankruptcy. To help ensure our products don\u2019t fall into this cycle, there are a few basic requirements. These apply to both product owners and to product teams. Some are common sense, while others may seem a little extreme. Just remember that one of the primary ways Engineering can help the business grow is through innovation, and that innovation is much cheaper and easier when the foundation stays strong. Evolution Requirements Advancing and upkeeping a product comes in various forms. Redesign, re-architecture, and refactoring are the most common varieties of evolution. These are technical challenges to be overcome. However, product owners and stakeholders have responsibilities here as well: reevaluation (of business value and direction) and revision (of product roadmap). You may have heard it said that refactoring isn\u2019t worthwhile, and therefore assume none of these efforts are beneficial. This is 100% false. These efforts can be incredibly lucrative, so long as they are focused on bringing additional value to the business. In other words, refactoring for the sake of it has no value; refactoring to pay off technical debt or increase speed to ROI is incredibly worthy. These are the requirements of all products: \u201cWhen in Rome\u201d only applies to code style differences, not to design and architecture. Efforts made in existing modules should attempt to bring them up to current standards. An example of a code style difference where \u201cWhen in Rome\u201d can apply: using ALL_CAPS convention for constants in C# code instead of PascalCase . An example of a design difference that should be corrected: logging to file when the standard is to log to stdout / stderr and allow a log aggregator to collect them. In this example, value is brought not just by reducing maintenance costs (in reducing the number of code paths that must be supported), but also in unifying logging. This makes solving issues easier, as the logs are more accessible and discoverable. Prioritize evolution efforts by value, and be able to identify the value of each decision. If you cannot articulate the value, it may not be worth doing. The product team should prefer to refactor mercilessly in place rather than take on large, risky, discrete refactoring efforts. The product team must be empowered to pay down technical debt. Product owners must recognize the value of evolution and should allow for it to be planned as part of feature design, or, where necessary, as discrete tasks. It is always cheaper, from an expense and opportunity perspective, to pay down debt ASAP. Ultimately, however, the product owner makes the call on how the product should align with business objectives, as reflected in their decisions managing the backlog. Or, to put it in layman\u2019s terms, sometimes evolution will be deprioritized for innovation, and that\u2019s fine. Technical debt is like financial debt: it can be paid off with little additional cost if well-managed. It only becomes a problem when the debt spirals out of control. However, if the debt does spiral out of control, it represents a huge loss of value for the company. The company\u2019s technical stakeholders will hold accountable anyone (including themselves) whose actions lead to out-of-control technical debt or technical bankruptcy. The product team (including the product owner) shall keep technical debt well-managed. Products shall be reviewed annually by an interest group to evaluate the design, infrastructural, and architectural state of the product. The objective of this review shall be: To assess technical debt To determine what actions need to be taken to rectify issues and improve foundations over the next year To realign the technology with business goals/direction To drive adoption of supported/platform technology and (sustainable) growth The interest group must include (or include representatives from): Director over the product Product owner Product team lead and/or technical lead Ops Data Architecture Architecture The review must occur early enough to allow for budgeting changes as a response to the outcome of the review. So long as a product is supported and maintained, it must follow these requirements. Products can be excepted from evolution requirements only when they have a corresponding end-of-life plan that has been published to consumers, and that plan ends within the next year. A product with a long-term EOL plan (as most Operating Systems have, for example) is not exempt. The reasoning for this requirement is to ensure that an active product is not left to collect technical debt that will get out of control before it is retired. Summary Hopefully, the requirements here are self-explanatory and self-evident. Most of the time, they come naturally from approaching a project in good faith. Either way, following them will help us ensure that we not only produce the best possible software, but also continue to keep it well-maintained and innovative well into the future. If you have any questions about items in this document, please reach out to a lead or to the Architecture team.","title":"Product Lifecycle Requirements"},{"location":"Projects/product-lifecycle-requirements/#product-lifecycle-requirements","text":"As projects begin, develop, and mature, they eventually blossom into full-fledged products. Along the way, there are several important steps to take that help ensure the product turns out solid. Too frequently ignored, however, is the follow-through once the product has matured. Continuing to manage the lifecycle of a product, even once it\u2019s \u201cdone\u201d, is just as important to its quality, and often the quality-of-life of your team as well.","title":"Product Lifecycle Requirements"},{"location":"Projects/product-lifecycle-requirements/#table-of-contents","text":"Looking for the TL;DR ? Here are some shortcuts to common reference sections: Product Versioning Requirements Support and Maintenance Checklist Documentation Requirements Product Evolution Requirements Full Table of Contents: Product Lifecycle Requirements Table of Contents Lifecycle != SDLC What is a Product? The Structure of this Document Reading the Requirements Versioning and Operation Lifecycle Public Version vs. Internal Version Choosing Versioned or Rolling Versioned Products During Design During Initial Development Upon Initial Release Subsequent Versions Discontinuing a Product Rolling Products During Design During Initial Development Upon Initial Release Subsequent Versions Discontinuing a Product Responsibility, Accountability, and Ownership Maintenance and Support Checklist The Importance of Documentation Product Growth The Importance of Innovation The Importance of Evolution Evolution Requirements Summary","title":"Table of Contents"},{"location":"Projects/product-lifecycle-requirements/#lifecycle-sdlc","text":"While this document deals with the lifecycle of products at ATLAS, it is not a replacement for an understanding of Systems/Software Development Lifecycle (SDLC) approaches. It also does not prescribe an approach. Instead, it addresses how the company in particular develops and maintains its products. These requirements should be applicable to any SDLC process. This distinction is made because SDLC is a loaded term. Sometimes it can refer generically to any lifecycle approach, but often it refers to a very particular process that heavily promotes waterfall development. ATLAS is not a waterfall shop, and many aspects of the colloquial SDLC methodology don\u2019t fit well. As a result, the term \u201cSDLC\u201d will be avoided from here.","title":"Lifecycle != SDLC"},{"location":"Projects/product-lifecycle-requirements/#what-is-a-product","text":"It\u2019s easy to look at the front-end applications, which receive all the marketing and sales focus, and call those the products. From a business and sales perspective, that\u2019s probably true. Those are the products of the company . Engineering, however, has many more products than just those SKUs. This document uses the term product frequently and loosely, but what is a product? Within the context of these requirements, basically everything is a product. They apply to all efforts. While the \u201cproductness\u201d of some sub-modules or parts of larger applications might be debatable, assume that these requirements apply when in doubt.","title":"What is a Product?"},{"location":"Projects/product-lifecycle-requirements/#the-structure-of-this-document","text":"These requirements are broken into three major categories that encapsulate most of a product\u2019s lifecycle. The first section is about the operation of the product , specifically around managing the initial effort, ongoing development, and its relationship with its users throughout. The second section details matters of responsibility, accountability, and ownership of the product as it relates to the support and maintenance from both a business and technical level. Finally, the last section covers the growth, innovation, and upkeep of products\u2013how they\u2019re kept from stagnation and how technical debt is paid off.","title":"The Structure of this Document"},{"location":"Projects/product-lifecycle-requirements/#reading-the-requirements","text":"When interpreting the verbiage of the requirements, know that it was inspired by RFC 2119 . As a quick reference, requirements using must , shall , must not , and shall not are absolute. Should , should not , recommended , and not recommended are strongly encouraged but there may exist valid reasons to avoid them. May , may not , and optional are optional, but acknowledged as part of the problem space. In all cases, the letter of the item is less important than the spirit. Reach out to team leadership if there are questions around interpretation. Similarly, no matter how absolute the requirement in this document, nothing overrides the fact that business must continue to operate. These requirements are a tool to assist in cooperation and ongoing value-creation for our customers; they are not a book with which to beat others.","title":"Reading the Requirements"},{"location":"Projects/product-lifecycle-requirements/#versioning-and-operation-lifecycle","text":"The versioning and operational requirements laid down in this document are bisected into the categories of versioned products and rolling products . Neither of them necessarily reflect anything about the development or release methodology of the product itself, though obviously some practices tend in one direction or the other. Instead, the focus is on the two different sets of expectations that consumers of the products harbor. The requirements are guided by these expectations, and differ as a result. ATLAS uses a versioned approach most often for technical modules like code libraries, containers, and (rarely) some APIs. These are strongly versioned to provide a contract to their consumers. The contract serves as an interface guarantee; a statement that the product at version a.b.c does exactly what it does the same way every time, while version x.y.z may behave differently. On the other hand, ATLAS usually relies on a rolling approach for its front-end, customer-facing software. Here, the contract between developer and consumer is much softer. It\u2019s implied that in exchange for a live, hassle-free stream of updates, users receive new versions automatically and regularly\u2013usually without their explicit consent or action. These products still have versions, of course, but the version is not part of that developer/consumer contract. Instead, there is an understanding that the developers will do their due diligence to ensure smooth transitions between releases, since users cannot opt out. There are certainly crossovers between these domains. However, the important part is keeping in mind the consumer\u2019s expectations. One must design these into the product, as they will heavily affect its lifecycle.","title":"Versioning and Operation Lifecycle"},{"location":"Projects/product-lifecycle-requirements/#public-version-vs-internal-version","text":"It\u2019s important to note that there is a clear distinction between the consumer-facing (\u201cpublic\u201d) version number of a product and its internal version as the development team knows it. No matter the approach, the internal version of a product must always uniquely represent a specific build of it\u2013or, more accurately, a reproducible point-in-time instance of it. The public version of the product may be the same as the internal version, but rarely will. The requirements in this document almost exclusively cover public versions. The public version number of the product is representative of its feature set rather than any given point-in-time instance. Rolling products must not have a prominent public version number. As discussed in later sections, the public version number is tied to a particular release rather than a build instance, and always requires a developer\u2019s hand to maintain the contract implied by the version number. For example, if a product follows Semantic Versioning , there is meaning behind each part of the version number that must be upheld. There\u2019s another version indicator that can exist as well: a marketing version number. Using Windows as an example, Windows 10 is not (internally or externally) version 10.x.y.z of the OS software. The 10 is just part of the marketing. These indicators are of little concern to the development team, though there are a few considerations.","title":"Public Version vs. Internal Version"},{"location":"Projects/product-lifecycle-requirements/#choosing-versioned-or-rolling","text":"When starting a new project, choosing the operational lifecycle strategy is key to developing and maintaining it successfully. Making this decision can be challenging, but there are a few guidelines to help. Versioned : \u2795 Creates a stable interface for consumers to rely on \u2795 Gives the dev team an easy path to introduce breaking changes when necessary \u2795 Allows clear communication around support windows \u2796 Tends to slow the pace of releases \u2796 Tends to stagnate consumers Consumers will not upgrade unless they are forced or convinced Forcing consumers to upgrade should come with a clear communication of deprecation to all consumers, a generous timeline for them to switch, and (for critical products) an out-of-band window to support those that cannot or will not upgrade. Convincing them requires publishing detailed change notes and, sometimes, marketing and promoting the new version (even internally) \u2796 Can be a burden on maintenance Depending on the users of the product and your relationship with them, you may be forced to support many versions simultaneously \u2796 Disadvantageous for service-oriented products Some products are inherently versioned. Code libraries are a prime example. Organizations occasionally hybridize the two approaches\u2013for example, automatically releasing nightly \u201cunstable\u201d versions or only making the latest version available\u2013but the product is still consumed at a specific version. The consumer usually still has to seek out and upgrade versions themselves, and in those scenarios most consumers are encouraged to wait for full releases. Even versioned products with auto-updaters are still versioned. Rolling : \u2795 Allows for an uninterrupted flow of new features to consumers \u2795 Smaller, non-breaking bug fixes can be applied as needed, enabling prompt resolution of issues \u2795 Only a single \u201clive\u201d version to worry about \u2795 Tends to accelerate the pace of releases \u2796 Communication around support windows lacks clear points of delineation \u2796 A deft approach to change is required from developers Since there is no ability to opt-in or opt-out of changes, any planned breaking change must be clearly communicated to all consumers They must be given a generous timeline to adapt, sometimes requiring training Every upgrade, enhancement, new feature, and regression goes live for all consumers This necessitates a more thorough testing methodology and more careful design\u2013not necessarily bad things, but certainly time-sinks Leads to strategies such as blue/green deployments; these strategies add complexity and overhead It often leads to strange workarounds to avoid introducing breaking changes alongside new functionality These workarounds/hacks more easily become permanent, due to the lack of clear delineation around support windows, which can eventually (if unmanaged) create large amounts of technical debt \u2796 Tends to increase burden on support teams/documentation Consumers are likely to find themselves left behind if they step away from your product for a time Even consistently engaged users are more likely to miss new functionality without strong update communication \u2796 Cannot cleanly apply to some product types Most SaaS products (including web applications) use a rolling model. Even their APIs are rolling, though often they\u2019re hybridized. In the common hybrid model, the consumer declares a version in their request to specify the desired functionality and compatibility. However, they\u2019re still forced to use the current interface; the version request is a function of the software rather than a fundamental part of the contract. It\u2019s an inversion of control; the user must ask to be allowed to tap into compatibility with old versions, rather than the user driving which version is used and the developer trying to shepherd them forward.","title":"Choosing Versioned or Rolling"},{"location":"Projects/product-lifecycle-requirements/#versioned-products","text":"","title":"Versioned Products"},{"location":"Projects/product-lifecycle-requirements/#during-design","text":"The intended consumer audience for the product must be set. Remember: a consumer might be a customer user, an internal user, or even another development group. No matter who the audience is, treat them as your consumer. Consumer expectations for lifecycle must be defined. If you\u2019re here, you\u2019ve probably already done this. The release mechanism must be established. How will consumers get new versions? SLAs around support must be quantified. What expectations should be set around how long a given version will be supported? What is the plan for enterprise-level support of out-of-band versions (if applicable)? Versioning rules for the product must be declared. Either they are defined in the design, or a reference is provided to the standard scheme used. One should strongly prefer Semantic Versioning over other schemes. No matter the versioning scheme chosen, it must meet these criteria : Some indicator of major version and minor version must be available at a minimum. Further indicators, such as patch numbers and tags to indicate development status (e.g. 3.0.4-beta ), are allowed. Additional indicators (patch numbers, etc.) must have their meaning and rules clearly defined in the design, especially as they relate to the major and minor version. Incrementing the minor version must never introduce known breaking changes. Incrementing the major version should reset the minor version. In other words, the minor version should always be a subset of the major version. The version indicators must always be mathematically comparable first and lexically comparable second. The comparison must linearly indicate the release history of the product relative to other versions. This is to address a common error. Version 3.10 must always be a later version than 3.2 when comparing version numbers\u2013specifically, 8 minor versions later.","title":"During Design"},{"location":"Projects/product-lifecycle-requirements/#during-initial-development","text":"The parties responsible and accountable for the product\u2019s ongoing maintenance, development, and support must be established. Consumer-facing versioning is not important during initial development until the product is made available to consumers for the first time. Expectations should be set that the \u201cinterface contract\u201d does not begin until the product\u2019s initial full release. To clarify, in-development versions are likely to have breaking changes constantly. However, the responsibility is on the developers to ensure that whatever version consumers are likely to take dependencies on is the initial full release . In other words, don\u2019t hand consumers a mostly-functional version of the product and expect them to not start relying on it, no matter what the version number is. It\u2019s on you, the developer, to manage expectations. Internal versioning begins ASAP during initial development. The internal version indicator should not be prominently visible publicly. It must always be possible for maintainers/supporters to discover the internal version of the product in the wild (e.g. once deployed or installed somewhere). It is strongly encouraged to use a different versioning scheme for internal versions. Prefer ones that include a temporal element.","title":"During Initial Development"},{"location":"Projects/product-lifecycle-requirements/#upon-initial-release","text":"The initial release shall be version 1.0 (or equivalent in your scheme). The full interface contract is now in effect. Version indicators must now fully adhere to the versioning rules laid out in the design, including the initial release.","title":"Upon Initial Release"},{"location":"Projects/product-lifecycle-requirements/#subsequent-versions","text":"Changes in ownership of the ongoing support and maintenance must be continually well documented, and that ownership must always be clearly defined. Release versions shall be a concern of the release pipeline/process, and shall always reflect the consumer-facing (\u201cpublic\u201d) version indicator. No system shall automatically increment the major version of a product. Build systems shall not increment the major or minor version of a product. Only release processes can affect them. These requirements are in place to convey the meaning behind the version. No automated system is capable of determining what the versioning of the next iteration of the product should be. However, it is acceptable for a release system to, by default, increment a minor version. It should not be possible for a release to be created that generates an out-of-sequence, historically inaccurate, or repeat version number. All major and minor releases shall be communicated to consumers via appropriate means. New releases shall include change notes. These can be as simple as a summary of the whole update (\u201cfixed bugs with loading report options\u201d), or as complex as a full changelog. The format of these notes depends on the intended consumer of the product, but should tend toward being as informative as possible. Changing the versioning scheme for a product must occur only at new major versions. It is considered a breaking change. Major version releases should only be made when breaking changes are introduced. The development team shall support the entire current (latest) major version series. Example: if Product X is at version 3.4.1 and a consumer using version 3.1.8 finds a bug, this is a valid issue and must be supported. Note that \u201csupport\u201d does not mean that a special version 3.1.8-fix should be created. If the issue is already fixed in a later release, it is valid to ask the consumer to upgrade. If the issue still exists in the most recent release, it is valid to fix it in a future release. The development team shall not make the immediately previous major version series of the product unavailable. Example: Product X is at version 3.4.1 . The team then releases version 4.0.0 . It cannot make the 3.*.* series of the product unavailable. It could take the 2.*.* series offline if necessary. This requirement is in place to provide some level of security to consumers\u2019 downstream dependent systems (e.g. build pipelines). Although the 3.*.* series is still available in this example, availability does not necessarily imply support (see below). Versions prior to the initial release do not apply. There is one exception: when the software has ceased to function properly in uncorrectable ways outside of the dev team\u2019s control, it may be made unavailable. The development team shall support the most recent version of the immediately previous major version series of the product. For example, if the product is version 3.4 and then 4.0 is released, version 3.4 must continue to be supported. Version 3.3 may go unsupported. When version 5.0 releases, support may end for 3.4 . As deemed necessary by product leadership, this support can be sunset. Sunsetting the previous series is a double-edged sword: Setting a sunset date requires that support for the series continue through that date. This supersedes the requirement to only support the current and immediately previous series, so the team may end up supporting many series. A generous window must be given before end-of-life. Consumers must be notified of the sunset date using appropriate means. The product team may choose to support more versions than the minimum outlined here. If so, the policy must be documented alongside the versioning scheme. Fixes should almost always be made against the latest series. Additions shall always be made against the latest series. It is strongly discouraged to branch the product into two actively maintained series (like how Python 2 and Python 3 worked until 2020). However, in some rare and unusual cases, the product\u2019s leadership may choose to do this.","title":"Subsequent Versions"},{"location":"Projects/product-lifecycle-requirements/#discontinuing-a-product","text":"There is currently no set of requirements or guidelines for discontinuing a product. A process will need to be established before requirements can be created.","title":"Discontinuing a Product"},{"location":"Projects/product-lifecycle-requirements/#rolling-products","text":"These are often called \u201clive\u201d or \u201cservice\u201d products, and they represent a vast majority of the software consumers use today.","title":"Rolling Products"},{"location":"Projects/product-lifecycle-requirements/#during-design_1","text":"The intended consumer audience for the product must be set. Remember: a consumer might be a customer user, an internal user, or even another development group. No matter who the audience is, treat them as your consumer. Consumer expectations for lifecycle must be defined. If you\u2019re here, you\u2019ve probably already done this. The release mechanism must be established. How will consumers receive the live version? Any expectations or agreements around product features or interface details (especially relating to consumer design input, demands, legal agreements, or otherwise) must be well-documented so that future maintainers are aware. It should be decided what SLAs are needed around feature support and product/service availability. Since the consumers are not able to opt out of updating, features they may rely on must be protected by some kind of SLA, even if its only for internal accountability and confidence. Many rolling products are SaaS offerings; this implies that the product is only usable when the service is available and functioning as intended. Because of this, the product\u2019s availability must be protected by some kind of SLA, even if its only for internal reference and confidence.","title":"During Design"},{"location":"Projects/product-lifecycle-requirements/#during-initial-development_1","text":"The parties responsible and accountable for the product\u2019s ongoing maintenance and support must be established. A strategy for smoothly transitioning into breaking changes must be developed. Despite the rolling nature of the product, it is not valid to break functionality without ensuring minimal consumer disruption. This strategy includes: Software design accommodations for changing requirements (so that a transition plan can exist later) Communication plans How are consumers to learn of changes to the product? What are the impacts? What is the value for them? When is the change expected? An approach to gate delivery of features or changes, such as feature flags or blue/green deployments The dev team must have a plan for determining consumer acceptance (e.g. UAT). This is true for versioned products as well, but becomes especially important for rolling products in subsequent releases. A rolling product, when it must take dependencies, should prefer to take dependencies on versioned products. Versioned products provide a level of reliability that rolling products cannot. Depending on them allows your live rolling product to better isolate issues and remove variables during troubleshooting. Internal versioning begins ASAP during initial development. The internal version indicator should not be prominently visible publicly. It must always be possible for maintainers/supporters to discover the internal version of the live product. Rolling products are strongly encouraged to either include a temporal element in their internal version, or be able to easily track backwards into one. This is important to help establish a historical reference of versions that otherwise may not exist if, for example, the internal version is based entirely on an arbitrary incrementing ID.","title":"During Initial Development"},{"location":"Projects/product-lifecycle-requirements/#upon-initial-release_1","text":"The product shall not be referenced by a version indicator when communicating with consumers, with the exception of any marketing version applied to it. This includes technical consumers and internal (developer) consumers. This requirement exists to prevent internal versioning from becoming external versioning. Versioning a rolling product is a violation of its implicit contract with consumers and sets the wrong expectations with them. Changes, fixes, and feature additions can instead be referenced by release timeline or development milestone, depending on product leadership\u2019s preference.","title":"Upon Initial Release"},{"location":"Projects/product-lifecycle-requirements/#subsequent-versions_1","text":"The development team must not introduce breaking changes without generous warning to consumers. For breaking changes, a transition plan must be developed to ensure as seamless an experience as possible. It should follow the strategy designed during initial development. Changes in ownership of the ongoing support and maintenance must be continually well documented, and that ownership must always be clearly defined. Changes to the consumers\u2019 expectations of the product, agreements (legal or otherwise), and design input over time must be continually well documented. New releases containing new features, important fixes, or impactful changes shall coincide with change communication to the consumer. This can be as simple as a summary of the whole update (\u201cfixed bugs with loading report options\u201d), or as complex as a full changelog. The format of these notes depends on the intended consumer of the product, but should tend toward being as informative as possible. The team should continue to support deprecated features for a generous period of time. The window should be large enough that even consumers using the product in unintended ways, who act on elongated timelines, and who require some coercion are able to adapt before the feature goes away. Security issues are an exception; features found to suffer from a security vulnerability can be removed immediately as a result. If the feature was slated for removal anyway, it can remain absent. If not, it should be reintroduced once the issue has been addressed. One should still notify consumers when these changes are made. The team should track metrics around feature utilization, especially around features that are currently or will soon become deprecated. This data should be used to inform transition plans and better target enhancements. The team must continue to prioritize consumer acceptance (UAT) for each new release, especially those with new features. A strategy must exist for correcting when breaking regressions are accidentally introduced. Whatever the strategy, it must include a catch-all safety, such as a full rollback, that ensures the product becomes fully available in an accepted state again. This is because consumers do not have the ability to opt out of updates that happen to be non-functional; some guarantee of restoration of service is required. The development team must not provide differently-versioned instances of the same product. In other words, even if multiple instances of the product are live at a given time, all instances must be the same internal version. This only applies to production environments, of course. Exceptions to this can be made, but only by company leadership. Supporting multiple versions of a rolling product turns it into a versioned product (but without the positive attributes normally associated with them). Support and maintenance costs are an order of magnitude higher.","title":"Subsequent Versions"},{"location":"Projects/product-lifecycle-requirements/#discontinuing-a-product_1","text":"There is currently no set of requirements or guidelines for discontinuing a product. A process will need to be established before requirements can be created.","title":"Discontinuing a Product"},{"location":"Projects/product-lifecycle-requirements/#responsibility-accountability-and-ownership","text":"A product\u2019s lifecycle is about more than versioning new features and managing consumer expectations. The product must also be continually supported and maintained. It is not acceptable to have products that are unattended. Having a clear and complete understanding of who owns a product\u2019s lifecycle and what that ownership entails is critical to its continued success. Responsibility vs. Accountability It is important to differentiate between responsibility and accountability. The responsible party is the one tasked with the duty. The accountable party is the one held liable for the results. The two can be the same but aren\u2019t necessarily so.","title":"Responsibility, Accountability, and Ownership"},{"location":"Projects/product-lifecycle-requirements/#maintenance-and-support-checklist","text":"This section provides an easy checklist of maintenance and support questions that must be answered for every product. The answers to these questions must always be available in product documentation during all stages of its lifecycle. They help ensure everyone is aware of which team is responsible for the ongoing support and who is accountable for the product\u2019s continued success. The responsible and accountable party can shift over time, but it must always be clearly defined. Note that the separate pieces/services which make up a product may have separate answers to these questions . For example, a website product with a backing database might have one team monitoring the health of the site and another monitoring the health of the database. This is disadvantageous, but until a DevOps methodology is fully embraced, is sometimes required. Be sure that the separation is clearly denoted in documentation. Who is the product team? Who is the product owner? What roles make up the dev team and who is filling them? What other roles are dedicated to the product and who is assigned to them? Who is responsible/accountable for monitoring the product? Monitoring includes ensuring that the product is operating within acceptable parameters and meeting SLOs. It also includes proactive examination of metrics to anticipate future problems or concerns. Monitors are responsible for raising the alarm when the product deviates, or is forecasted to deviate, from its \u201cgreen\u201d state. Automated monitoring is encouraged, but a human must always be available\u2013either alongside the auto-monitoring or \u201con call\u201d\u2013to ensure its signals are observed and reacted to. The objective of the monitoring effort is to help ensure that the product continues to meet business needs at the real-time level. Some product types require no monitoring, for example if they are statically-delivered and \u201cpackaged\u201d applications, but this is becoming rare. Who is responsible/accountable for the customer support of the product? In other words, who engages with the consumers of the product around questions, issues, requests, and so on? This party must be assigned even when the consumer of the product is an internal audience. Who is responsible/accountable for the technical support of the product? In other words, who can answer technical questions about the workings of the product and its components? Who is responsible/accountable for the ongoing maintenance of the product infrastructure? These concerns include making sure database maintenance is performed, security patches are kept current, backups are valid and can be restored, log folders are cleaned, scaling is in line with demand, and so on. Who is responsible/accountable for the business alignment of the product? Almost always, this will be the product owner. The product owner will react to the business needs and express the alignment as stories and prioritization. Sometimes, a product owner does not exist. Even in this case, some party must be responsible for ensuring that the product moves forward and aligns with direction. The objective of this effort is to help ensure that the product continues to meet business objectives at a tactical and strategic level. Who is responsible/accountable for the technical upkeep and evolution (growth) of the product? The nature of entropy dictates that over time, all applications will decay unless they continue to evolve and, at minimum, keep up with their surrounding environment. This means that someone must be responsible for ensuring that the evolution of all supported products is represented in discussions around prioritization and value. Products left untouched for long periods of time accrue technical debt simply by existing. The knowledge necessary to support and maintain the product fades over time, and eventually even minor fixes become large efforts as the code must be relearned (often by someone new).","title":"Maintenance and Support Checklist"},{"location":"Projects/product-lifecycle-requirements/#the-importance-of-documentation","text":"The original development team of a product will not always be its shepherds. Even if the product doesn\u2019t change hands directly, the team itself will change over time. The only way to preserve the decisions made about the product\u2013whether during initial design, original development, or as part of on-going growth\u2013is to document them. There must be a centralized location for all documentation related to the product\u2019s design and development. PMO-based documentation (project submissions, etc.) are encouraged to be located here as well but not required. Confidential legal artifacts, financial artifacts, and (hopefully a very rare scenario) tightly-held company secrets should not live here. Documentation shall cover technical and business decisions made, the operation of the product, dependencies, assumptions, and other intangibles. Product docs shall be available for any member of any development team within the company to view. This allows a cross-pollination of information, removes variables around security and access (which saves Ops time), and encourages a sharing of ideas and collaboration. It also saves time when the team responsible for the product changes. There is a window during the initial design of the product where documentation can be less widely available, to avoid exposing incorrect information. However, the docs should be made available ASAP. Secrets should not live in centralized documentation. This is especially true for passwords, authentication tokens, keys, and so on. Instead, secrets should live in a secret store (such as a key vault). Documents shall be allowed to evolve and change alongside the product as necessary. However, this evolution should happen within a change-tracked system, so that historical versions of documents can be retrieved. This requirement exists to ensure that knowledge-sharing documents are not encouraged to freeze and become outdated. An exception: documents which must freeze in order to represent authoritative or process-based finality (such as forms approved as part of project submission) are allowed to do so. All currently supported versions of the product must be covered by the documentation, either within a single set or by multiple parallel sets.","title":"The Importance of Documentation"},{"location":"Projects/product-lifecycle-requirements/#product-growth","text":"All products that are to be supported for any significant amount of time must continue to grow. We cannot allow products to stagnate or rot, even those products which are \u201cfinished\u201d. This section discusses how products should continue to innovate and evolve over time, even if they are already providing stable business value.","title":"Product Growth"},{"location":"Projects/product-lifecycle-requirements/#the-importance-of-innovation","text":"Warning This section covers philosophy and approach rather than direct requirements. It is still very important; it provides context to the requirements that come in the next section. Here\u2019s a very curt statement: A stagnant product leaves money on the table. It may be blunt, but it\u2019s a core philosophy of successful software companies. If you have a product you\u2019re still supporting/selling/distributing, letting it languish is robbing the company of potential value. It may not be literal dollars, but there are many factors contributing to \u201cvalue\u201d which can be represented either directly or indirectly by money. Actual money, in new sales contracts, add-ons, or ad-hoc purchases Expense which could have been reduced Opportunity cost Customer/user faith, trust, and interest Competitive advantage Like all things, software is subject to entropy. A new product is fresh and exciting, even something as simple as a development library. There is inherent momentum behind a new release. As that momentum dies down, however, users become bored and disinterested. This is one reason why you often see products like Facebook make large changes to user experience even though the existing one worked \u201cjust fine\u201d\u2013change keeps the momentum going. It keeps users engaged and gives you the opportunity to introduce new features and functionality which might lead to greater value. It\u2019s reasonable to react to this philosophy with some negativity. \u201cWhy make changes to a product which is bringing value and risk bringing less value?\u201d There is some validity to this reaction, certainly. Not all markets react the same way to stagnant products. However, a stagnant product\u2019s value often has a peculiar curve, as demonstrated by this rough graph. Post-release, the product\u2019s value skyrockets to its peak after a warm-up period. From there, without any further releases, the value holds briefly before beginning to taper off. Eventually, the value will near zero. Obviously, this is not hard data based on any particular case study; it\u2019s an abstraction of the philosophy. Most products will deviate from this on the micro level, but at the macro level, experience holds this to be accurate more often than not. On the other hand, here\u2019s the product\u2019s value with additional releases that bring new functionality, a refreshed user experience, or even (less sustainably) important bug fixes. Again, these graphs are not hard data, but this approach has been proven time and again by various companies. Innovation is its own value, and often holds the key to unlocking the long-term potential of the product.","title":"The Importance of Innovation"},{"location":"Projects/product-lifecycle-requirements/#the-importance-of-evolution","text":"A central part of product ownership is being committed to its technical advancement and upkeep. Innovation is very important, but tacking new features onto a product without keeping the underlying infrastructure and technology up-to-date is like adding rooms onto your house when the foundation is cracking. You might be successful at first, but without addressing the underlying issue, the house is doomed. The importance of evolution is to not just patch up the foundation as it starts to fall apart, but to proactively improve and enhance it so that you can more easily innovate. Perhaps a more familiar metaphor is technical debt . Through sheer entropy, code left unimproved for long periods of time will accrue debt. Quick fixes and hasty bolt-ons will act as loans, creating more debt. Eventually, if you don\u2019t pay it down, it will begin to compound on itself until the burden of it is too much to repay. Often, you\u2019re left with what seems like only a single option: technical bankruptcy. To help ensure our products don\u2019t fall into this cycle, there are a few basic requirements. These apply to both product owners and to product teams. Some are common sense, while others may seem a little extreme. Just remember that one of the primary ways Engineering can help the business grow is through innovation, and that innovation is much cheaper and easier when the foundation stays strong.","title":"The Importance of Evolution"},{"location":"Projects/product-lifecycle-requirements/#evolution-requirements","text":"Advancing and upkeeping a product comes in various forms. Redesign, re-architecture, and refactoring are the most common varieties of evolution. These are technical challenges to be overcome. However, product owners and stakeholders have responsibilities here as well: reevaluation (of business value and direction) and revision (of product roadmap). You may have heard it said that refactoring isn\u2019t worthwhile, and therefore assume none of these efforts are beneficial. This is 100% false. These efforts can be incredibly lucrative, so long as they are focused on bringing additional value to the business. In other words, refactoring for the sake of it has no value; refactoring to pay off technical debt or increase speed to ROI is incredibly worthy. These are the requirements of all products: \u201cWhen in Rome\u201d only applies to code style differences, not to design and architecture. Efforts made in existing modules should attempt to bring them up to current standards. An example of a code style difference where \u201cWhen in Rome\u201d can apply: using ALL_CAPS convention for constants in C# code instead of PascalCase . An example of a design difference that should be corrected: logging to file when the standard is to log to stdout / stderr and allow a log aggregator to collect them. In this example, value is brought not just by reducing maintenance costs (in reducing the number of code paths that must be supported), but also in unifying logging. This makes solving issues easier, as the logs are more accessible and discoverable. Prioritize evolution efforts by value, and be able to identify the value of each decision. If you cannot articulate the value, it may not be worth doing. The product team should prefer to refactor mercilessly in place rather than take on large, risky, discrete refactoring efforts. The product team must be empowered to pay down technical debt. Product owners must recognize the value of evolution and should allow for it to be planned as part of feature design, or, where necessary, as discrete tasks. It is always cheaper, from an expense and opportunity perspective, to pay down debt ASAP. Ultimately, however, the product owner makes the call on how the product should align with business objectives, as reflected in their decisions managing the backlog. Or, to put it in layman\u2019s terms, sometimes evolution will be deprioritized for innovation, and that\u2019s fine. Technical debt is like financial debt: it can be paid off with little additional cost if well-managed. It only becomes a problem when the debt spirals out of control. However, if the debt does spiral out of control, it represents a huge loss of value for the company. The company\u2019s technical stakeholders will hold accountable anyone (including themselves) whose actions lead to out-of-control technical debt or technical bankruptcy. The product team (including the product owner) shall keep technical debt well-managed. Products shall be reviewed annually by an interest group to evaluate the design, infrastructural, and architectural state of the product. The objective of this review shall be: To assess technical debt To determine what actions need to be taken to rectify issues and improve foundations over the next year To realign the technology with business goals/direction To drive adoption of supported/platform technology and (sustainable) growth The interest group must include (or include representatives from): Director over the product Product owner Product team lead and/or technical lead Ops Data Architecture Architecture The review must occur early enough to allow for budgeting changes as a response to the outcome of the review. So long as a product is supported and maintained, it must follow these requirements. Products can be excepted from evolution requirements only when they have a corresponding end-of-life plan that has been published to consumers, and that plan ends within the next year. A product with a long-term EOL plan (as most Operating Systems have, for example) is not exempt. The reasoning for this requirement is to ensure that an active product is not left to collect technical debt that will get out of control before it is retired.","title":"Evolution Requirements"},{"location":"Projects/product-lifecycle-requirements/#summary","text":"Hopefully, the requirements here are self-explanatory and self-evident. Most of the time, they come naturally from approaching a project in good faith. Either way, following them will help us ensure that we not only produce the best possible software, but also continue to keep it well-maintained and innovative well into the future. If you have any questions about items in this document, please reach out to a lead or to the Architecture team.","title":"Summary"},{"location":"Projects/project-technical-guidelines/","text":"Project Technical Design Guidelines It\u2019s one thing for us to say that we need to be writing tests, implementing proper logging and monitoring, adhering to agile and DevOps, and so on. It\u2019s another entirely to actually do those things, or more precisely, to do them for every project consistently. To help us be consistent and follow through on our goal of writing more resilient, long-lasting software, this document provides a set of guidelines. These items are essential for new development, but they\u2019re also important for non-trivial efforts against existing products. Any design submission that does not meet these guidelines will be rejected\u2014though don\u2019t let that discourage you. This is a cooperative process, and not a set of laws or rules. The guidelines fall into these categories: Automated Testing CI and CD Monitoring Logging Patterns and Practice Infrastructure Documentation and Design Not every project will be compatible with every guideline, and no set of guidelines can tell you how to optimally write software . It\u2019s best to treat this as a sort of checklist you run through during the design phase of each project. To facilitate that, we\u2019ll begin with a series of summaries you can check off as you validate them. More in-depth discussion about each point is linked on each list item. Enable Checkboxes Clear & Disable Checkboxes Automated Testing - Summary Plan for testing from the very beginning (rule of thumb: 20% extra time) Design testable solutions and write testable code Fix \u201cuntestable\u201d code, or denote why you can\u2019t Test all public endpoints and critical paths Avoid tautological tests Avoid testing third-party or framework code, but test your integrations Shoot not for code coverage percentages but for 100% business coverage Test your code and infrastructure deployments CI and CD - Summary Keep modules granular so they can be integrated as painlessly as possible Use feature flags where possible to release more frequently Create reproducible and deterministic builds Use CI process as a sign off on code quality and integration Create reproducible, deterministic, and idempotent deployments Ensure the entire pipeline takes less than 10 minutes to execute Monitoring - Summary Measure all mathematically significant metrics Abstract as much monitoring logic out of your application as possible Take full advantage of time series data Establish and maintain SLAs on throughput and performance Alert meaningfully Create dashboards and stoplights to help identify problems before they become a problem Logging - Summary Log (almost) everything Put as little logging code within your application as possible Provide enough information in logs for them to be immediately actionable Rely on log levels, but ensure they\u2019re meaningful Carefully craft logs to be eminently searchable Remember that logs are not metrics, and metrics are not logs Patterns and Practice - Summary Apply SOLID principles, but don\u2019t be dogmatic Be monolithic when you need to be; be modular when it matters Consume services responsibly; provide responsible services Be creative where needed and formulaic to save time YAGNI Get WET before you worry about being DRY Expose configuration externally to minimize the need for recompilation and redeployment Avoid technical debt, and plan immediately to pay any you accrue off Infrastructure - Summary Express infrastructure requirements as code Make your application highly available, load balanced, fault tolerant, and scalable Consume resources responsibly Store secrets securely Have a back-out strategy for bad/failed deployments Plan for emergency deployment of critical services\u2014or document the lack of plan/capability Documentation and Design - Summary Create a design document that demonstrates how requirements will be satisfied Address many of the same concerns as this document in your design doc Record the intangibles for future reference to avoid tribal knowledge Keep design, requirements, and other project docs centralized and grouped together Treat API documentation as a first-class deliverable Write self-documenting code Automate doc creation and publishing \u2014 Automated Testing Automated testing is vital to the long-term success of any engineering effort, but it\u2019s especially critical for applications following an agile/DevOps methodology. Tests are how you help keep from breaking things when you move fast, and they increase confidence in deployments. The objective of releasing often isn\u2019t necessarily to catch bugs more quickly; it\u2019s to surface defects in general. Reducing the number of avoidable defects by eliminating easy bugs using automated testing means you can spend more time addressing the stuff that matters. Plan for Testing If you\u2019re not planning for testing, you won\u2019t write tests. This may sound obvious, but too many teams assume that testing is just part of development. Their estimates are usually inaccurate, and they spend time that otherwise could have been spent on tests just finishing the work instead. A good rule of thumb is to add 20% extra time to estimates for testing (and testing alone). Go through your team\u2019s usual methodology for estimation. Whether it\u2019s hours, days, story points, or whatever\u2014create your estimate as usual. Then, add 20% to that. This is not padding or fluff . This is the bare minimum time you can expect to spend on writing tests. If you expect that a task or project will require additional test-writing time, you are empowered to add more than just 20%. Avoid Influencing Estimates with Testing Time Be careful not to start adding time for testing until you\u2019ve settled on the estimate for the work. You want to avoid subconsciously influencing your estimate by the amount of testing time it would add. It is more important to estimate accurately and write good code (which means good tests) than it is to give management a lower number. Testable Designs and Testable Code This document cannot teach you how to design testable solutions, nor how to write testable code. These topics alone are worthy of entire books. What it can do is help explain what makes a solution or module testable, and then encourage you to learn and ask questions. Testable Design is Usually Modular Design Past a certain point of trivial simplicity, monolithic designs tend to be harder to test. Even if every code unit that makes up the system is perfectly testable (a goal which, itself, is more challenging in monolithic designs), the system as a whole is often not. It\u2019s only as good as the aggregate of its unit tests. Failures in a single module cause the entire system to be rejected, which drastically increases the impact of regressions. Monolithic code modules are, by definition, more complex. Complexity increases the likelihood of defects. By contrast, modular designs tend to be simpler (per module), which reduces the likelihood of defects. They also isolate impacts to single modules, meaning other fixes, features, and improvements are not held back by the one defective module. \u201cModular\u201d doesn\u2019t have to mean microservices. Often it will, but modularity can be achieved through several mechanisms: packages, libraries, sidecar services, and more. So long as a given module can be written , deployed , referenced , and maintained independently of other modules, modularity has been achieved. Don\u2019t take this to mean your modules can\u2019t take dependencies. They can, so long as those dependencies are themselves modular. Modularity is a \u201cturtles all the way down\u201d sort of thing. Testable Code is Always SOLID Code It is possible at the system level to design something testable without relying on modularity, especially when the system is trivial or itself a module. However, when it comes to testing code units , there is less flexibility. Without following SOLID principles fairly closely, it is inevitable that the code will become test-resistant. It may occur immediately or it might degrade over years of maintenance, but it will happen. Using a principled approach throughout the software\u2019s lifecycle, even during maintenance mode, will help stave off technical debt, omniscient dependencies, and even entropy. Sometimes, an application may seem so trivial or straightforward that \u201cgoing through the motions\u201d isn\u2019t worth it. But, it is , and to view these principles as rote practice is to miss the point (as we\u2019ll discuss more later in this document). It\u2019s always significantly faster to do it right in the beginning than to come back and re-do it later. The nature of software is to grow and morph over time. Unless you\u2019re 100% sure that you\u2019re writing a dead-end application that will never evolve, be copy/pasted from, referred to by future developers, or be called upon to suddenly scale, it\u2019s worth it. Example of Testable vs. Test-resistant Code We\u2019ve talked a lot about what makes something testable , but how can you tell the difference? Here\u2019s an example of the contrast between testable and test-resistant code. Don\u2019t focus too much on the underlying language (C#) or patterns\u2013those are only used to give context to the examples\u2013just focus on how you\u2019d write tests for this logic. /* Test-resistant Example */ /* Modules: SalesReportWebsite */ // ------ // Sales.cs // ------ // Assume SalesData is a POCO and that the referenced fields exist on it public class Sales { public SalesData GetSalesData() { var sales = /* Pretend this does something, like query a database or read a file */; return sales; } public decimal CalculateSalesFromOnline(SalesData sales) { return sales.Daily.Where(s => s.Source == \"Online\").Select(s => s.Dollars).Sum(); } } // ------ // SalesController.cs // ------ // Pretend this is a really simple ASP.NET MVC Controller public class SalesController : Controller { public SalesController() { } public Sales SalesLogic { get; } = new Sales(); /* ...other methods/actions... */ public string ReportOnlineSales() { var sales = SalesLogic.GetSalesData(); var salesFromOnline = SalesLogic.CalculateSalesFromOnline(sales); return $\"Sales from Online sources were ${salesFromOnline}.\"; } } /* Testable Example */ /* Modules: SalesContracts AcmeSalesReports - depends on SalesContracts SalesReportWebsite - depends on SalesContracts, AcmeSalesReports */ // ------ // ISalesQuery.cs - in SalesContracts.csproj // ------ // Assume SalesData is a POCO public interface ISalesQuery { SalesData GetSalesData(); } // ------ // ISalesCalculator.cs - in SalesContracts.csproj // ------ public interface ISalesCalculator { decimal CalculateSalesFromOnline(SalesData sales); } // ------ // SalesController.cs - in SalesReportWebsite.csproj // ------ // Pretend this is a really simple ASP.NET MVC-style Controller public class SalesController : Controller { public ISalesQuery SalesQuery { get; } public ISalesCalculator SalesCalc { get; } public SalesController(ISalesQuery salesQuery, ISalesCalculator salesCalc) { SalesQuery = salesQuery; SalesCalc = salesCalc; } /* ...other methods/actions... */ public string ReportOnlineSales() { var sales = SalesQuery.GetSalesData(); var salesFromOnline = SalesCalc.CalculateSalesFromOnline(sales); return $\"Sales from Online sources were ${salesFromOnline}.\"; } } // ------ // Startup.cs - in SalesReportWebsite.csproj // ------ // Pretend this is an application of the ASP.NET Core startup pattern public class Startup { /* ...ASP.NET Core boilerplate goes here... */ public void ConfigureServices(IServiceCollection services) { services.AddControllers(); // The prod implementations of ISalesQuery and ISalesCalculator live in AcmeSalesReports. // However, they're not shown here because who cares? // The point is that they could be anything that adheres to the interface contract. services.AddTransient<ISalesQuery, SomeSalesQueryImplementation>(); services.AddSingleton<ISalesCalculator, SomeSalesCalculatorImplementation>(); } } The \u201ctestable\u201d example is a very basic and imperfect implementation of dependency injection using the provider that comes with ASP.NET Core, but don\u2019t worry about that. Instead, consider how you would write tests for one vs. the other given what\u2019s shown here. In the first example: How would you write a test that doesn\u2019t actually connect to the database? How would you test that the generated report is correct when the backing data is live and could change? How could you test the generated report without breaking that test the instant a customer wants the sales calculation to change slightly? How could you assert that the report is in the correct format without worrying about the correctness of the calculation, and vice versa? In the second example, the answers to the above questions are trivial. When to Write Tests During Development Some engineers evangelize Test-driven Development (TDD), where tests are written before the logic. Others prefer to write tests after the fact. Both of these have flaws: TDD tends to promote over-modularized, trivial code with tautological tests, and testing after the fact often causes developers to miss obvious cases they may have otherwise remembered. Sometimes the best option is to write the test at the same time as you write the logic, but this requires greater discipline and isn\u2019t compatible with everyone\u2019s thought process. Instead, get in the habit of considering what is best for you at each unit level. Applied this way, each developer can work optimally without needing to enforce a methodology. Some people work best when they write the test first, as it allows them to build a \u201cskeleton\u201d of the logic ahead of time that focuses on business rules. Others keep the testing in mind as they write a unit, and then hammer out the test quickly after. A few programmers switch back and forth between the unit and the test, adding things to each cooperatively. The unit itself might dictate how tests are best written. Units with atomic outputs to assert or static algorithms to check are great candidates for test-first development. Units that require large amounts of input variation or conditional state manipulation often lend themselves well to side-by-side development with the tests. Units dealing with large schema-bound data objects or tight integrations with other services often come naturally to testing afterwards. The important part is to think about the unit before you begin writing it and decide at the time how best to write the test. Where to Go from Here If you\u2019re not used to writing testable code, it will take some time to adjust. Do some reading on the SOLID principles and practice applying them in code. Remember to decide how you\u2019re going to a test a unit before you even write it. Perhaps most importantly, if you\u2019re unsure whether a design or unit is testable, ask someone! It\u2019s usually easier to imagine the dependencies and side-effects of a method/design as someone that didn\u2019t write it. Fixing Untestable Code Often, you\u2019ll find yourself applying these guidelines to existing code instead of greenfield projects. When that happens, aside from the challenges of designing a solution, you\u2019ve also got to deal with whatever decisions came before. That usually involves some level of technical debt. Older code\u2013especially stuff written before these principles became commonplace\u2013tends to have piles of technical debt or, sometimes, just poor design. Trying to write tests against this stuff can seem impossible. However, there is no such thing as \u201cuntestable\u201d code. What you\u2019ve really got is test-resistant code. The difference might seem like an ideological exercise, but the important distinction is your mindset. You should be thinking about how to make the code more testable. Not all code can immediately be made testable, but that\u2019s not a roadblock. What steps can you take right now? How can the code begin to evolve into a testable state? To be clear, the correct answer is usually not to hack tests into test-resistant units. The objective is to either make the code more testable or to begin the initiative of making it more testable. Make a plan for how to get there and execute it. The value of good tests is too high to abandon them when the task seems daunting. Each and every developer is responsible for code quality and cleanliness, and each of us is empowered to design solutions. Having said that, this set of guidelines can\u2019t predict every scenario. It\u2019s possible, if you\u2019re time-limited and it\u2019s truly hideous code, there isn\u2019t enough time to even begin tackling the testability. Even in those cases, you should come up with a plan you (or someone) can pursue later, but what if you don\u2019t even have time to plan ? Regardless of the reason why, if you accept that you\u2019re unable to write tests, you must document the reasons why. Store them with the documentation (both technical and project) so that in the future, when someone comes back to the code you wrote, you\u2019ll save them some time having documented the issues you ran into. Testing Public Endpoints and Critical Paths When writing tests, the most important units to cover are those directly used to satisfy public endpoints (i.e. the API of your module). These are always business critical, by definition. This will intentionally lead you down a rabbit hole of testing, where you start asserting all of the conditions and dependencies that lead up to the endpoint. A critical path is logic where your application spends the majority of its time; or, more precisely, the paths where optimization efforts are most valuable as they reduce the overall execution time for a given operation. Most of the time, following the above guidance, you will end up covering your critical paths by necessity. However, great care should still be taken to ensure they\u2019re well tested. Optimizations are a frequent source of regressions, and being a critical path implies that regression would be highly detrimental. Avoiding Tautological Tests A tautological test is one where the test will always pass, usually because the unit and test are essentially equivalent. A rudimentary example in Python: def sum(a, b): return a + b def myTest(): assert(sum(3, 4) == 3 + 4) Notice that the test asserts sum(3, 4) by comparing it to 3 + 4 . Technically, this test is validating that addition is performed correctly in the sum function. However, the condition is only checked by essentially replicating the unit being tested. Even if the test were improved it would still be tautological: def sum(a, b): return a + b def myTest(): assert(sum(3, 4) == 7) The test can never fail, although in this version it\u2019s more because the test is asserting basic language functionality, which can be assumed to not fail. The nature of the sum unit is the problem rather than the test itself. Here\u2019s an example of a non-tautological test that is similar: def calculateGMROII(profit, avgInventoryCost): return profit / avgInventoryCost def myTest(): assert(calculateGMROII(100000, 25000) == 4) This isn\u2019t testing that Python is capable of dividing two numbers. It tests that the GMROII calculation behaves as expected. This is a business case which provides value, and a change in the algorithm for GMROII would likely impact downstream systems. Therefore, this is a useful, non-tautological test even though the calculation is relatively simple. Where Tautology Gets Tricky The examples above are trivial for the sake of demonstration. Most of the time, a test becomes tautological through less obvious means. Frequently, a test can appear perfectly valid until you \u201csolve\u201d it down like algebra. If the test solves down to (for example) 2x = x / 0.5 , you\u2019ve got a tautology. A more practical example: WEEK = 201801 TESTDATA = [{'week': WEEK, 'value': 123}, {'week': WEEK, 'value': 456}, {'week': WEEK, 'value': 789}] # This is the unit being tested def dataSlicer(dataLoaderCallback, week, skip, take): data = dataLoaderCallback() dataAtWeek = [d for d in data if d['week'] == week] return dataAtWeek[skip:skip+take] def test_dataLoader(): return TESTDATA def myTest(): sliceData = dataSlicer(test_dataLoader, WEEK, 2, 1) assert(len(sliceData) == 1) assert(sliceData[0] == {'week': WEEK, 'value': 789}) It is left as an exercise to the reader to determine why this one is tautological, but as a hint: simplify the test down until it becomes clearer. Avoiding Redundant Tests Against Third-party or Framework Code There is no value in writing a test against someone else\u2019s code in your test suite. For example, if there is a math library you\u2019re referencing, don\u2019t bother writing tests against its pow() function. If you\u2019re relying on an extension library to remove entities from an Entity Framework set, there\u2019s no need to test that the entity was actually removed. A module\u2019s tests belong with the module. No tests in your suite should be written against other modules. If you find a test case for another module, for example a scenario that causes unexpected output, you should attempt to add it to that module\u2019s suite. If that\u2019s not possible, don\u2019t worry about it. Write a test for your workaround instead, such that when the module updates and works correctly again, your workaround test will break. Speaking of testing code integrations, while there\u2019s no value in testing the third-party library, there is a ton of value in testing your units\u2019 integration with those libraries. A great example is where your logic must transform the output from a third-party source in some way. Don\u2019t bother testing that the third-party source returns the output as you expect; instead, test that your transformations behave as expected against all important possible outputs (including failures). Business Coverage vs. Code Coverage Many of the same shops that espouse TDD will enforce a 100% code coverage standard. In other words, all reachable code must be touched by a test in the suite. This dogmatic approach to testing leaves no room for interpretation or flexibility; often, builds will automatically be rejected if coverage falls below a certain percent. Speaking strictly about the dogmatism and zealotry, there is no value in code coverage-based testing standards. These shops are usually trying to substitute out high-quality engineers and smart people with rigid, rote practice, and it never * works. There is no substitute for good developers. True value comes from looking at business coverage . Business coverage is just making sure that your suite has tests covering all of your business cases, asserting your business rules, and testing that your value-generating logic continues to work as intended. The test spec is created by strong requirements and clear, visible design. Coverage is ensured through due diligence during development and careful peer review focused not just on code quality, but on business value as well. Difference Between Code Coverage and Business Coverage Code coverage only validates that the code units behave as expected. Business coverage is focused on ensuring that the application performs all business-valuable operations in the correct way. They frequently intersect but the mindset behind the two is very different. Calculating business coverage is not as easy as using a tool to spit out a percentage and automatically failing when it falls below a threshold, but it\u2019s infinitely more useful as a signpost of quality. As a result of its subjectivity, there can never be a simple numeric representation. It will instead fall to discipline and practice, which is one of the reasons high-quality engineers are important. Code coverage enthusiasts assume that a 100%-covered solution will by definition ensure 100% business coverage, but this couldn\u2019t be further from the truth. If it completely misses valuable business cases (because the units were not written to address them), it might as well be 0% covered. * Even when people say it works, it\u2019s probably not actually working. A team gets the results it incentivizes, so when it incentivizes code coverage, that\u2019s precisely what developers focus on\u2013to the detriment of code that does anything useful. Testing Deployments Just as important as unit-level and module-level testing is deployment-level testing. Testing a deployment means validating that the build/release pipeline is successful and adheres to infrastructure and architecture requirements. As discussed in the later section on CI/CD , the pipeline should be held to several standards. Deployment testing asserts those standards. For example, a deployment test for a service might validate that it deploys idempotently. Unlike other testing granularities, deployment testing requires a suitable environment. Cloud-first applications enjoy an advantage here, as infrastructure can be trivially acquired temporarily to test a pipeline and then disposed after the test. Thanks to products like Kubernetes, however, even private cloud applications can acquire and dispose of infrastructure programmatically. Test results are usually binary: absolute success or complete failure. Deployment tests, however, can sometimes fail due to transient errors inherent to Infrastructure-as-a-Service (IaaS) platforms. Whether it\u2019s Azure, Google, or vSphere, sometimes the infrastructure just cannot be created for unknown, unimportant, and temporary reasons. Because of this, it is not recommended to automate the testing of deployments. It should be scripted , but the success of a pipeline should never rely on the outcome of a deployment test. You may be asking, \u201cif the test shouldn\u2019t be automated, when should it be run?\u201d Deployment testing should be conducted before the module is promoted into a production branch (i.e. master ). Usually, infrastructure deployments into production are gated by a DevOps engineer anyway; it falls to them to execute the provided deployment tests. If the test does not pass, the DevOps engineer will not allow the module to pass. Thankfully, DevOps engineers should be the most knowledgeable source of information about how to write a deployment test, so cooperate with them to fix your infrastructure and/or write a valid test. CI and CD The principles of Continuous Integration (CI) and Continuous Delivery (CD) are vital to getting the maximum value out of our projects. Developers are often unable to accomplish work as fast as possible due to roadblocks or speed bumps in the release process. CI/CD are a central part of every flavor of agile methodology because they remove roadblocks and speed bumps; developers are enabled to move quickly and deploy often. In fact, some teams/projects will even choose to use Continuous Deployment to ensure their latest code is always released when it\u2019s available. For reference, here are some quick definitions: Continuous Integration : the practice of automatically validating that new commits from many authors integrate properly with the existing code base. In other words, CI is making sure that failures or flaws were not introduced when all commits have merged together. Often, it implies frequent merges to minimize the impact of drifting changes between branches over time. Continuous Delivery : the practice of being able to release changes as quickly as possible. Usually, this entails having an up-to-date release artifact ready to go live at all times. While not necessary, it\u2019s considered healthy to release as frequently as possible; this reduces the \u201cblast radius\u201d of the changeset. Continuous Deployment : a corollary to CD, this is the practice of taking the up-to-date release artifact generated by the CD process and automatically releasing it as soon as it\u2019s generated successfully. There is some debate over whether the automatic release must be into production . For the purposes of this document, assume that it must at least deploy into a production staging environment which can trivially become production. Granular Modules Integrate Easily This is a deceptively simple concept: more granular modules are easier to integrate. That statement has a dual meaning; it refers both to code integration and system integration. Module-level integration testing runs faster and, thanks to lower complexity, proves more decisive. System-level integration (i.e. deploying the module into a running environment) has a lower impact risk if the module is small. Tend toward more granular modules, where possible, to amplify these advantages\u2014though obviously balance it against the increase in system complexity. Feature Flags A common (and healthy) practice in CD is to release changesets as frequently as possible. As mentioned above, this reduces the possible impact of any single changeset. Another benefit, however, is the ability to get features into production to harden ASAP. As the saying goes, no plan survives contact with the enemy, so why not get the plan in front of them as soon as possible? Of course, not every infant feature is ready to be seen by users immediately. Even if there would be a ton of value in having developers test the feature in the production environment, users have a habit of forming an impression the first time they see a feature whether it\u2019s \u201cready\u201d or not. This is where feature flags come into play. They can allow you to toggle features in real-time, gate features by user (or other logic), and expose functionality internally without making it visible to the public, among other things. You get this while also getting to integrate and test the feature where it matters: in production. The implementation of feature flags is outside the scope of this document (though here is a decent reference for more information). Getting into the habit of using them and releasing features earlier will usually result in stronger features, better testing, and smoother user acceptance. This is especially true if you\u2019re able to run pilot programs or A/B testing with your feature flags. Reproducible and Deterministic Builds In order to reduce time-consuming troubleshooting on build/deploy pipeline issues, avoid unpredictable application regressions, and create higher confidence in releases, you should strive for all builds to be reproducible and deterministic. A reproducible build means that the set of conditions (operating environment, variables, code state, etc.) required to build a product can be replicated on demand, for any (reasonable) past or current version of the product. An example of a build that is not reproducible is one that relies on being built on machines that, externally to the pipeline, have been prepared with prerequisites. For example, if the pipeline does not install the .NET Core 3 SDK, but your build requires it, it is not reproducible. Perhaps a more volatile example: if the build relies on resources that could become permanently unavailable (say, downloading a library from someone\u2019s personal website), it is not reproducible. A deterministic build is one where, given that set of reproducible conditions, the build output will be the same every time. Creating a deterministic build can sometimes be tricky. Many modern package managers or similar concepts, like Docker image repositories, allow you to take dependencies not on exact versions but instead on minor, major, or even latest versions. Doing this causes your build to become non-deterministic. Some build pipelines take dependencies on time-sensitive or context-sensitive variables; these are non-deterministic as well. Important Most package managers support some form of lock file or similar mechanism that ensures your packages and all their dependences resolve to deterministic versions. You must take advantage of this mechanism. Too often, a package you depend on will take a loose dependency on another package. Since that indirect package could change without you changing anything in your direct dependencies, your build is no longer reproducible or deterministic. Locking the chain of dependencies resolves the issue as much as can reasonably be expected. Using Continuous Integration as a Quality Measure The more trust you can place into your CI process\u2019s ability to vet your changes, the more trust you can have in deploying. CI/CD is built on enhancing trust to the point of having faith a given deployment will succeed. Defects and regressions will always happen\u2014no process can catch them all\u2014but having confidence that the automated pipelines will take care of the obvious stuff removes a lot of anxiety. This is why your CI pipelines run the automated test suites for your projects, why you merge early and often, and why you perform CI at all. It helps increase confidence in the artifacts being generated and the work being done. You should make sure your CI process is doing everything (within reason) to assert the quality of your code, changes, and artifacts. Reproducible, Deterministic, Idempotent Deployments Much like your builds, your deployments should be reproducible and deterministic. For the most part, you can reference the section on builds and apply the same principles here. There are some differences, which this section will focus on. A reproducible deployment differs from a reproducible build only in that the environment you\u2019re reproducing is different. Rather than making sure the build occurs in the same environment, here you\u2019re ensuring that the infrastructure itself is reproducible. A deployment should be able to create its necessary infrastructure, dependencies, and so on. For example, if your application depends on its own RabbitMQ instance, your deployment must be able to reproduce that. A reproducible deployment leads into making a deterministic deployment. Assuming deterministic build artifacts, the resulting deployment should be the same every time. This means that deploying your artifact fresh to the platform should result in the exact same code, infrastructure, backing services/dependencies, networking, and so on, every time. Idempotency here means that, after the first time, deploying the same artifacts again will change nothing. The existing deployment will be unaffected. Without tools that help enforce idempotency (like Kubernetes and its declarative YAML format), it can be difficult to achieve it for your pipelines. Idempotency extends further to the individual components of each deployment. For example, imagine you have executed a deployment pipeline which creates a RabbitMQ instance, two services, and one load balancer in Kubernetes. Then, a minor code change to one service causes a new artifact to be created. When you execute the pipeline with this new artifact, only the changed service should be affected. If you end up with two RabbitMQ instances, four services, and two load balancers\u2026 that\u2019s a problem. Faster Pipelines Equals More Productivity It probably goes without saying, but if your pipelines are fast, you can respond more quickly to issues. No matter how much testing we automate, how often we test deployments, or how diligent our developers are in peer review, sometimes stuff slips through the cracks. Imagine a build/release pipeline that takes 45 minutes to run. Any error, even something so trivial you can correct it in literally one second, requires a minimum of 45 minutes to fix. Imagine how long it takes when it\u2019s something serious or you need to release multiple times. As a rule of thumb, keep CI/CD pipelines under 10 minutes of execution time. It\u2019s fine if they run longer when releasing to an empty environment (i.e. releasing for the first time), but given that the components should be idempotent , later executions should be much quicker. Monitoring Without being able to measure what\u2019s going on in a system, you\u2019re stuck taking shots in the dark when issues crop up. Even worse, you only know issues are happening once they\u2019ve already caused some kind of impact. In complex systems, it can take tens of minutes\u2013sometimes hours\u2013to find which subsystem is even having the issue before you can start debugging. It\u2019s especially nasty when the issue isn\u2019t a wholesale failure, but a transient error, throughput issue, performance problem, or similar functional-but-bad state. Good monitoring is the key to solving the visibility issues that many teams face in production systems. Measure all Mathematically Significant Metrics When in doubt, it\u2019s better to over-measure than to under-measure. You should record every metric your application deals in. Whether it\u2019s CPU usages, memory used/free, number of threads, time required, message throughputs, number of files processed in the current operation, or even network utilization, you should be recording it one way or another. Mathematically Significant? A mathematically significant metric is a number that has meaning in comparison or contrast to other numbers, and whose value scales by some mathematical function with variables in the developer\u2019s control. Usually, these are obvious metrics: number of operations per second, CPU usage, count of I/O waits, and so on. The distinction is made to contrast these metrics against \u201csoft\u201d measurements like, for example, a user satisfaction rating. A user satisfaction rating is something that can be measured , but only using an arbitrary scale and without the ability to directly affect the outcomes. The objective of recording all these metrics is to be able to draw conclusions about the state and performance of the system, both the system as a whole and its subsystems. We need these insights both real-time and historically. Imagine a dashboard showing a line graph of your application\u2019s message throughput next to a pie chart of the breakdowns of the types of work sitting in the queue. Beside that is a bar chart showing current throughput per type vs. average throughput per type at this time. You\u2019d be able to quickly pinpoint potential issues in an instant, rather than having to identify that a problem exists and then run a bunch of queries or aggregate a bunch of logs to gather the metrics yourself. Obviously, not every metric in that imaginary dashboard must necessarily come from your application, but for it to exist, all applications must be dutifully reporting metrics. When deciding what to measure, there are some bare minimum metrics that every application should be recording, shown here along with some examples. Utilizations CPU, RAM, disk, network, files, threads, processes, etc. Throughputs Number of messages processed, number of operations performed, number of service calls made, number of records read, etc. Workloads Number of messages waiting, number of files yet to process, count of jobs currently running, etc. Times Time required to complete an operation, time elapsed between heartbeats, time spent on reading files, time spent on waiting for a database, time spent waiting on network communication, uptime, etc. Recording a metric, as discussed below in the Abstract Metric Collection section, should be trivial to your application in terms of resource cost and logic required. Some of these minimum metrics are taken care of for you automatically, depending on the application platform. For example, Kubernetes takes care of measuring most of your application\u2019s resource utilizations in that environment (but not necessarily all). If your application uses RabbitMQ as a message broker, it can be configured to report metrics automatically as well. Value in Non-Mathematical Metrics Despite the focus in this section on mathematically significant metrics, there can certainly be value in other metrics which aren\u2019t as \u201chard\u201d. However, those metrics should be measured outside the scope of your project\u2019s system monitoring effort. Their collection might, for example, be part of the application\u2019s business requirements rather than a system-level architectural necessity. It\u2019s important that you consider your monitoring carefully even when various platforms provide assistance. This guidance takes care not to prescribe any particular solutions because only you, the developer, know best what\u2019s appropriate for your project. Only you can know which metrics you should record that might not be provided by your infrastructure. Abstracting Metric Collection and Dispatch It may seem a little counter-intuitive to recommend measuring everything , then immediately say your application should do as little measuring as possible. The key phrase, however, is your application . This means that every effort should be made to collect metrics about the application outside of the app\u2019s logic. If the logic must expose a metric (for example, because the data can only be recorded by the logic), it should do so plainly, in a way that does not take dependencies on specific mechanisms, infrastructures, or frameworks. Another process, such as a Kubernetes sidecar or metric collection agent like Telegraf, should be responsible for the collection and dispatch of those metrics. Separating the collection and delivery of metrics from the recording allows the application to remain na\u00efve and operate in many different environments without needing to change. The underlying dispatch of metrics can change live without impacting anything. The recording of metrics won\u2019t potentially have repercussions throughout the application if something fails; the collector can simply pick up and resume when it recovers. Taking Full Advantage of Time Series Data An amazing way to create measurable metrics is to record state artifacts at a time interval. Viewing the various state details of an application over time\u2013for example, sampling and recording the number of items in queue every second\u2013can expose tons of insight into the behavior of the system. Assuming you\u2019ve properly abstracted your metric collection , you should be able to trivially record details at a fairly quick interval without causing harm to the system. You want to be able to extrapolate meaning from the data both in realtime and historically simply by aggregating/partitioning the data differently. Let\u2019s look at CPU utilization on a Windows workstation as an example. Viewing it realtime in Task Manager tells you how loaded the CPU is right now and what impacts to expect in the near term. Viewing it over the past 24 hours in Resource Monitor informs you on what your busiest hours are, which helps you plan your future days better. In both cases, the CPU usage is sampled at an interval and recorded; the only difference is the zoom level and aggregation of the samples. Establishing and Maintaining SLAs on Metrics The biggest goal of the monitoring effort, aside from system visibility, is the ability to define and assert success for a system. Being able to look meaningfully at the monitoring and alerts for a system and say, definitively, \u201cthe system is operating successfully,\u201d has immeasurable value. It can: Increase confidence around the product/system Reduce stress on the team and business Quickly locate where problems actually lie and fix them directly Alleviate conflation between different definitions of success Note To clarify the different definitions of success used here: a product\u2019s success can almost never be 100% determined by how well it satisifed its SLAs and how good its metrics look. A system\u2019s success can be shown much more mathematically. Defining success requires the creation of SLAs. The Distributed Architecture Strategy document goes into much greater detail about the creation and maintenance of SLAs. It is strongly recommended to read that entire document, but especially the section on SLAs and metrics. The SLAs you create for your system are not necessarily user-facing SLAs. Those are usually driven more by the business and sales pipelines, though they often strongly influence your internal SLAs. SLAs can only be created and enforced around good metrics. Even the most basic SLA in the world, uptime percent, can\u2019t be upheld without metrics around service uptimes. Since raw uptime isn\u2019t really valuable\u2013if one replica out of 100 is up, the service is technically still \u201cup\u201d\u2013you need to define an uptime of scale instead. Establishing an uptime of scale necessitates defining what scale means for your system. By the time you\u2019ve followed this thought process through, you\u2019ll have determined several metrics to record and items to monitor. Alerting Meaningfully Most systems alert too much. Developers and operators often err on the side of caution and choose fairly low thresholds to alert on, which leads to an overabundance of noisy alerts that may or may not actually represent a problem. Consumers begin to ignore the alerts, meaning that actual, valuable signals get lost in the ether. This isn\u2019t a theoretical problem; it\u2019s one that has proven itself historically. You\u2019ve seen it or heard of it: e-mail alerts getting automatically shuffled by Outlook rules into folders full of thousands and thousands of unread signals, most of which have less-than-zero value. Make no mistake, however. Those alerts which really do have meaning are astoundingly valuable. This becomes especially true as the complexities of systems, architectures, and ecosystems increase. Being able to get meaningful signals from the various parts of a distributed application (or even a monolithic one) is vital to keeping systems healthy. In turn, this keeps the customers happy. Important This section is primarily discussing infrastructure-, system-, and architecture-level alerts. As covered in the System-Level Alerts vs. Business-Level Alerts sub-section, there is a strong distinction between alerts that are a byproduct of your application vs. those that are part of your application\u2019s business logic. Tips for Alerting Distinguishing what makes an alert a useful and meaningful signal can sometimes be tricky, but here are a few tips: An alert must be actionable. If you aren\u2019t sure whether something qualifies for alerting, ask someone else what they would do if they received the alert. Since alerts should be actionable, if they can\u2019t tell you what they would do to address the alert, it might not be a valuable alert. As a rule of thumb, when a system has crossed into its yellow or red states (see Dashboards and Stoplights ), a signal should be raised. However, there must be a \u201ccooldown\u201d on this signal to prevent it from over-alerting when the system rapidly cycles between states. Alerts should usually be based on metrics. Specifically, a good signal is when an important metric is deviating significantly from its steady state. Example: a queue-based message processing application has items in the queue, but its throughput is 0 over the most recent window of time. Alerts should contain the right information needed to convey the impact. In other words, you should strive to make the alert self-explanatory, without needing to dig in further to understand the problem. Some examples: Spline reticulator reticulated 100 splines This is only a useful alert if you happen to know that 100 is too many splines . Spline reticulator reticulated 100 splines which exceeds the threshold of 25. This is better. It indicates: The reticulation exceeded a threshold, which is why there was an alert. The magnitude of the excess was 100 vs. 25\u2013a potential indicator of severity or urgency. Spline reticulator reticulated 100 splines which exceeds the threshold of 25. This often indicates that the foo map is too complex or that the bar graph has too many baz nodes. This is even better. However, this much insight won\u2019t always be achievable or even desirable depending on the source of the alerts. Just as important as the metric information is some ability to correlate the alert to the system(s) it\u2019s based on. Ensure alerts are making it to the right people without that list going stale. An alert could be perfect, concise, and informative, but if it doesn\u2019t end up in the right places, it might as well not exist. This often means making sure alerts are going to places abstracted from individual people. For example, alerting into a team-wide or division-wide Slack channel or emailing alerts to a distribution group that represents the concerned parties helps ensure that the right people see them no matter who may come and go within the company. Don\u2019t be afraid to lean on alerting frameworks for help. Getting the right quantity and quality from your alerts can be tough. These are solved problems; there\u2019s no reason to bang your head against the wall. Grafana, for example, has a decent alerting rules engine built-in which is able to directly pull metrics from time-series databases like InfluxDB. Application Logic Should (Almost) Never Alert Application logic should almost never create a system-level alert directly. Only in very rare scenarios should it even be able to do so. Your code should be handling errors gracefully (remember Postel\u2019s Law ), so the sort of explosions that you would need to signal should be infrastructural\u2013things that the application platform would alert on instead. The responsibility of observing the system for critical states lies outside of your application, for the most part. If an error is so severe that it cannot be handled, it\u2019s usually best to just crash. For the same reasons that your application should be recording metrics plainly, so too should it be failing plainly. The application platform and DevOps ecosystem must be robust enough to create the alerts around frequent application crashes\u2013after all, you can\u2019t predict every reason your app might go down. Let\u2019s look at a practical scenario. Pretend your application is a report runner; its purpose is to receive messages to run reports, run the reports, and write the output to a storage location. As an example, the app might crash if it hits a defined retry limit on trying to access its critical storage for the results. The application cannot continue when it\u2019s unable to write the results of its operations. In this example, crashing may even fix the error, depending on the application platform and storage medium. Transient errors or temporary mount disruptions can sometimes be corrected with an application restart. That said, there are rare exceptions where your application may need to alert directly. Use your best judgement, given the guidance of this document and your team. Most of the time, if you feel like your application must alert, it\u2019s because the alerts fall into the category discussed in the next sub-section. System-Level Alerts vs. Business-Level Alerts Most of the discussion in this section has been around metric-based, system-level alerting. Part of the reason your application shouldn\u2019t be doing its own alerting is the implication that it must therefore be concerning itself with consuming its own metrics and/or taking a dependency on an alerting mechanism. This goes against the guidance in the Abstracting Metric Collection section. However, there is a type of alerting that doesn\u2019t fit the same mold: business-level alerting . Unlike the infrastructure-level signals this section has focused on so far, business-level alerts are driven by business rules and requirements. These alerts are not a byproduct of your application, they are your application . You must be very careful to distinguish these alert types, because they are very different use cases. As an example, pretend your application is part of an ETL platform. Another application has extracted the data; yours is responsible for asserting the completeness of that data\u2013as in, whether the source system provided all of the necessary data, based on a business definition of completeness. When your application sends an alert that the data is likely not complete, that alert is the output of your application. It is not the same kind of alerting as, say, signaling that the application is using 100% of its allocated RAM. The audience is different, the delivery mechanism is probably different, and the content is different. These are very likely two separate systems, serving two separate purposes. Business-level alerts are fine, so long as they fall within the requirements and design of the application. Many of the same tips in this section could probably apply to them (e.g. they should be actionable, based on metrics, etc.) but there is much more room for flexibility here. Whatever is determined to provide value to the user is exactly how much alerting your application logic should do when it\u2019s business-level alerting. Dashboards and Stoplights Important Please read the Distributed Architecture Strategy section on Stoplights before proceeding! During the design of any system, one of the first-class items should be a well-defined collection of dashboards designed to assist support and maintenance out of the gate. Using the dashboards, one should be able to answer many questions about the operation of the system and quickly, proactively diagnose issues. Similarly, the collection of metrics and states that will define your steady state must be defined during the design effort. These SLO\u2019s will help you define your stoplights. What conditions must be met for it to qualify as green ? What conditions cause it to be in a red state? (For guidance to others) what does a yellow state usually mean about the system? This at-a-glance discoverability has incalculable value, and cannot be anything less than a primary concern of every effort. It\u2019s far too easy to put dashboards and stoplights aside when the project is on a time crunch or trying to run with a thin crew, believing that the system will run fine without them. Perhaps it will, for a bit, but any production system that\u2019s intended to last will eventually go down. Every minute that system is down, it\u2019s costing you (either directly in revenue, or indirectly in damage to customer and stakeholder trust), and you\u2019ll wish you had all the tools available to correct the problem ASAP. Logging Keeping clean, informative logs is one of the most important things developers can do to make a system more maintainable. However, it is perhaps one of the most challenging things to get \u201cright\u201d, second only to recording metrics. Too many logs (or too verbose) just leads to unreadable noise. Too few (or too sparse) and there are gaps in information and history that prevent issues from being traced. This section should help define a sweet spot of useful logs that become a reliable source of troubleshooting and auditing assistance. Log (Almost) Everything \u2026 Minimize Application Logging Code \u2026 Actionable Log Data \u2026 Meaningful Log Levels Most logging libraries support a concept of log levels\u2014such as trace , debug , info , warning , and error ; or sometimes represented as numbers\u2014to help categorize and often thin out the amount of logs being written or consumed at a given time. Given that you should log almost everything , having a somewhat-standard way to denote the intention and importance of a log message is very useful. Otherwise, the number of logs can become overwhelming. It is strongly recommended that you rely upon log levels in your logging implementation. However, unless the levels have well-established meaning, their intended use of reducing the amount of work one must do while spelunking through the logs is eliminated. Here are a few guidelines for making log levels meaningful in your application: Make sure the log level of your application is configurable. Most of the time, the lowest levels of your logging are very verbose. The trace and debug levels, for example, often spit out several messages for every related info event. That highly granular detail is not usually necessary for a system running in production. It\u2019s wise to make the log level configurable such that the verbose logs are not written unless the application is specially configured to output them. Often, dev and QA environments will leave the verbose logging on. Production environments might turn it on occasionally if detailed troubleshooting is required. In the documentation, establish what types of events and messages should fall into which levels. The primary cause of meaningless log levels is when developers don\u2019t understand what each level should be used for, so information ends up poorly categorized. As a default, use this: trace is used for very fine-grained debugging information that often goes as deep as emitting a log when a method is called or even when specific lines of code are about to be executed. Obviously not every method or every line of code should log to the trace log, but important or impactful ones can be. Ex: \"parseJsonForMessage() called\" debug often represents critical state changes, important logical entry points or exit points, or data dumps that might be useful for debugging later. Ex: \"Begin parsing JSON for message 1234\" info is usually the lowest active logging level by default, and represents audit information, events and messages of interest, and other messages that help viewers understand standard process flow. Ex: \"Received message 1234\" warning is for events or messages that are slightly problematic or potentially concerning, but not immediately erroneous. However, as they often indicate future errors, they are quite important. error is straightforward: it denotes errors. An error may not necessarily be fatal (though it\u2019s very useful to make fatal ones obvious), but any time an erroneous state is encountered it should be logged. Non-fatal errors in user input/state and warnings often overlap in concern. It can be helpful to treat non-fatal user issues as warnings to help system-level errors stand out. Ex: \"Message 1234 was larger than the maximium size of 1024 KB and could not be processed.\" Ensure that whatever method is used for digging through log files makes the log level visible and searchable\u2014whether it\u2019s something as simple as flat text files or as complex as Splunk. For example, flat text logs should include the log level in the message so that advanced text editors like VS Code, Sublime Text, or Notepad++ can highlight and find it within the log. Ex: 2020-12-08T16:56:35.9319564-06:00 Info: Received message 1234 Ex: 2020-12-08T16:56:35.9319564-06:00 Error: Message 1234 was larger than the maximum size of 1024 KB and could not be processed.\" Obviously, be consistent with how log levels are treated and what sorts of messages fall into which levels. Searchable Logs \u2026 Logs Are Not Metrics, Metrics Are Not Logs \u2026 Patterns and Practice Apply SOLID principles, but don\u2019t be dogmatic Be monolithic when you need to be; be modular when it matters Consume services responsibly; provide responsible services Be creative where needed and formulaic to save time YAGNI Get WET before you worry about being DRY Expose configuration externally to minimize the need for recompilation and redeployment Avoid technical debt, and plan immediately to pay any you accrue off Infrastructure Express infrastructure requirements as code Make your application highly available, load balanced, fault tolerant, and scalable Consume resources responsibly Store secrets securely Have a back-out strategy for bad/failed deployments Plan for emergency deployment of critical services\u2014or document the lack of plan/capability Documentation and Design Create a design document that demonstrates how requirements will be satisfied Address many of the same concerns as this document in your design doc Record the intangibles for future reference to avoid tribal knowledge Keep design, requirements, and other project docs centralized and grouped together Treat API documentation as a first-class deliverable Write self-documenting code Automate doc creation and publishing","title":"Project Technical Design Guidelines"},{"location":"Projects/project-technical-guidelines/#project-technical-design-guidelines","text":"It\u2019s one thing for us to say that we need to be writing tests, implementing proper logging and monitoring, adhering to agile and DevOps, and so on. It\u2019s another entirely to actually do those things, or more precisely, to do them for every project consistently. To help us be consistent and follow through on our goal of writing more resilient, long-lasting software, this document provides a set of guidelines. These items are essential for new development, but they\u2019re also important for non-trivial efforts against existing products. Any design submission that does not meet these guidelines will be rejected\u2014though don\u2019t let that discourage you. This is a cooperative process, and not a set of laws or rules. The guidelines fall into these categories: Automated Testing CI and CD Monitoring Logging Patterns and Practice Infrastructure Documentation and Design Not every project will be compatible with every guideline, and no set of guidelines can tell you how to optimally write software . It\u2019s best to treat this as a sort of checklist you run through during the design phase of each project. To facilitate that, we\u2019ll begin with a series of summaries you can check off as you validate them. More in-depth discussion about each point is linked on each list item. Enable Checkboxes Clear & Disable Checkboxes","title":"Project Technical Design Guidelines"},{"location":"Projects/project-technical-guidelines/#automated-testing-summary","text":"Plan for testing from the very beginning (rule of thumb: 20% extra time) Design testable solutions and write testable code Fix \u201cuntestable\u201d code, or denote why you can\u2019t Test all public endpoints and critical paths Avoid tautological tests Avoid testing third-party or framework code, but test your integrations Shoot not for code coverage percentages but for 100% business coverage Test your code and infrastructure deployments","title":"Automated Testing - Summary"},{"location":"Projects/project-technical-guidelines/#ci-and-cd-summary","text":"Keep modules granular so they can be integrated as painlessly as possible Use feature flags where possible to release more frequently Create reproducible and deterministic builds Use CI process as a sign off on code quality and integration Create reproducible, deterministic, and idempotent deployments Ensure the entire pipeline takes less than 10 minutes to execute","title":"CI and CD - Summary"},{"location":"Projects/project-technical-guidelines/#monitoring-summary","text":"Measure all mathematically significant metrics Abstract as much monitoring logic out of your application as possible Take full advantage of time series data Establish and maintain SLAs on throughput and performance Alert meaningfully Create dashboards and stoplights to help identify problems before they become a problem","title":"Monitoring - Summary"},{"location":"Projects/project-technical-guidelines/#logging-summary","text":"Log (almost) everything Put as little logging code within your application as possible Provide enough information in logs for them to be immediately actionable Rely on log levels, but ensure they\u2019re meaningful Carefully craft logs to be eminently searchable Remember that logs are not metrics, and metrics are not logs","title":"Logging - Summary"},{"location":"Projects/project-technical-guidelines/#patterns-and-practice-summary","text":"Apply SOLID principles, but don\u2019t be dogmatic Be monolithic when you need to be; be modular when it matters Consume services responsibly; provide responsible services Be creative where needed and formulaic to save time YAGNI Get WET before you worry about being DRY Expose configuration externally to minimize the need for recompilation and redeployment Avoid technical debt, and plan immediately to pay any you accrue off","title":"Patterns and Practice - Summary"},{"location":"Projects/project-technical-guidelines/#infrastructure-summary","text":"Express infrastructure requirements as code Make your application highly available, load balanced, fault tolerant, and scalable Consume resources responsibly Store secrets securely Have a back-out strategy for bad/failed deployments Plan for emergency deployment of critical services\u2014or document the lack of plan/capability","title":"Infrastructure - Summary"},{"location":"Projects/project-technical-guidelines/#documentation-and-design-summary","text":"Create a design document that demonstrates how requirements will be satisfied Address many of the same concerns as this document in your design doc Record the intangibles for future reference to avoid tribal knowledge Keep design, requirements, and other project docs centralized and grouped together Treat API documentation as a first-class deliverable Write self-documenting code Automate doc creation and publishing","title":"Documentation and Design - Summary"},{"location":"Projects/project-technical-guidelines/#-","text":"","title":"---"},{"location":"Projects/project-technical-guidelines/#automated-testing","text":"Automated testing is vital to the long-term success of any engineering effort, but it\u2019s especially critical for applications following an agile/DevOps methodology. Tests are how you help keep from breaking things when you move fast, and they increase confidence in deployments. The objective of releasing often isn\u2019t necessarily to catch bugs more quickly; it\u2019s to surface defects in general. Reducing the number of avoidable defects by eliminating easy bugs using automated testing means you can spend more time addressing the stuff that matters.","title":"Automated Testing"},{"location":"Projects/project-technical-guidelines/#plan-for-testing","text":"If you\u2019re not planning for testing, you won\u2019t write tests. This may sound obvious, but too many teams assume that testing is just part of development. Their estimates are usually inaccurate, and they spend time that otherwise could have been spent on tests just finishing the work instead. A good rule of thumb is to add 20% extra time to estimates for testing (and testing alone). Go through your team\u2019s usual methodology for estimation. Whether it\u2019s hours, days, story points, or whatever\u2014create your estimate as usual. Then, add 20% to that. This is not padding or fluff . This is the bare minimum time you can expect to spend on writing tests. If you expect that a task or project will require additional test-writing time, you are empowered to add more than just 20%. Avoid Influencing Estimates with Testing Time Be careful not to start adding time for testing until you\u2019ve settled on the estimate for the work. You want to avoid subconsciously influencing your estimate by the amount of testing time it would add. It is more important to estimate accurately and write good code (which means good tests) than it is to give management a lower number.","title":"Plan for Testing"},{"location":"Projects/project-technical-guidelines/#testable-designs-and-testable-code","text":"This document cannot teach you how to design testable solutions, nor how to write testable code. These topics alone are worthy of entire books. What it can do is help explain what makes a solution or module testable, and then encourage you to learn and ask questions.","title":"Testable Designs and Testable Code"},{"location":"Projects/project-technical-guidelines/#testable-design-is-usually-modular-design","text":"Past a certain point of trivial simplicity, monolithic designs tend to be harder to test. Even if every code unit that makes up the system is perfectly testable (a goal which, itself, is more challenging in monolithic designs), the system as a whole is often not. It\u2019s only as good as the aggregate of its unit tests. Failures in a single module cause the entire system to be rejected, which drastically increases the impact of regressions. Monolithic code modules are, by definition, more complex. Complexity increases the likelihood of defects. By contrast, modular designs tend to be simpler (per module), which reduces the likelihood of defects. They also isolate impacts to single modules, meaning other fixes, features, and improvements are not held back by the one defective module. \u201cModular\u201d doesn\u2019t have to mean microservices. Often it will, but modularity can be achieved through several mechanisms: packages, libraries, sidecar services, and more. So long as a given module can be written , deployed , referenced , and maintained independently of other modules, modularity has been achieved. Don\u2019t take this to mean your modules can\u2019t take dependencies. They can, so long as those dependencies are themselves modular. Modularity is a \u201cturtles all the way down\u201d sort of thing.","title":"Testable Design is Usually Modular Design"},{"location":"Projects/project-technical-guidelines/#testable-code-is-always-solid-code","text":"It is possible at the system level to design something testable without relying on modularity, especially when the system is trivial or itself a module. However, when it comes to testing code units , there is less flexibility. Without following SOLID principles fairly closely, it is inevitable that the code will become test-resistant. It may occur immediately or it might degrade over years of maintenance, but it will happen. Using a principled approach throughout the software\u2019s lifecycle, even during maintenance mode, will help stave off technical debt, omniscient dependencies, and even entropy. Sometimes, an application may seem so trivial or straightforward that \u201cgoing through the motions\u201d isn\u2019t worth it. But, it is , and to view these principles as rote practice is to miss the point (as we\u2019ll discuss more later in this document). It\u2019s always significantly faster to do it right in the beginning than to come back and re-do it later. The nature of software is to grow and morph over time. Unless you\u2019re 100% sure that you\u2019re writing a dead-end application that will never evolve, be copy/pasted from, referred to by future developers, or be called upon to suddenly scale, it\u2019s worth it.","title":"Testable Code is Always SOLID Code"},{"location":"Projects/project-technical-guidelines/#example-of-testable-vs-test-resistant-code","text":"We\u2019ve talked a lot about what makes something testable , but how can you tell the difference? Here\u2019s an example of the contrast between testable and test-resistant code. Don\u2019t focus too much on the underlying language (C#) or patterns\u2013those are only used to give context to the examples\u2013just focus on how you\u2019d write tests for this logic. /* Test-resistant Example */ /* Modules: SalesReportWebsite */ // ------ // Sales.cs // ------ // Assume SalesData is a POCO and that the referenced fields exist on it public class Sales { public SalesData GetSalesData() { var sales = /* Pretend this does something, like query a database or read a file */; return sales; } public decimal CalculateSalesFromOnline(SalesData sales) { return sales.Daily.Where(s => s.Source == \"Online\").Select(s => s.Dollars).Sum(); } } // ------ // SalesController.cs // ------ // Pretend this is a really simple ASP.NET MVC Controller public class SalesController : Controller { public SalesController() { } public Sales SalesLogic { get; } = new Sales(); /* ...other methods/actions... */ public string ReportOnlineSales() { var sales = SalesLogic.GetSalesData(); var salesFromOnline = SalesLogic.CalculateSalesFromOnline(sales); return $\"Sales from Online sources were ${salesFromOnline}.\"; } } /* Testable Example */ /* Modules: SalesContracts AcmeSalesReports - depends on SalesContracts SalesReportWebsite - depends on SalesContracts, AcmeSalesReports */ // ------ // ISalesQuery.cs - in SalesContracts.csproj // ------ // Assume SalesData is a POCO public interface ISalesQuery { SalesData GetSalesData(); } // ------ // ISalesCalculator.cs - in SalesContracts.csproj // ------ public interface ISalesCalculator { decimal CalculateSalesFromOnline(SalesData sales); } // ------ // SalesController.cs - in SalesReportWebsite.csproj // ------ // Pretend this is a really simple ASP.NET MVC-style Controller public class SalesController : Controller { public ISalesQuery SalesQuery { get; } public ISalesCalculator SalesCalc { get; } public SalesController(ISalesQuery salesQuery, ISalesCalculator salesCalc) { SalesQuery = salesQuery; SalesCalc = salesCalc; } /* ...other methods/actions... */ public string ReportOnlineSales() { var sales = SalesQuery.GetSalesData(); var salesFromOnline = SalesCalc.CalculateSalesFromOnline(sales); return $\"Sales from Online sources were ${salesFromOnline}.\"; } } // ------ // Startup.cs - in SalesReportWebsite.csproj // ------ // Pretend this is an application of the ASP.NET Core startup pattern public class Startup { /* ...ASP.NET Core boilerplate goes here... */ public void ConfigureServices(IServiceCollection services) { services.AddControllers(); // The prod implementations of ISalesQuery and ISalesCalculator live in AcmeSalesReports. // However, they're not shown here because who cares? // The point is that they could be anything that adheres to the interface contract. services.AddTransient<ISalesQuery, SomeSalesQueryImplementation>(); services.AddSingleton<ISalesCalculator, SomeSalesCalculatorImplementation>(); } } The \u201ctestable\u201d example is a very basic and imperfect implementation of dependency injection using the provider that comes with ASP.NET Core, but don\u2019t worry about that. Instead, consider how you would write tests for one vs. the other given what\u2019s shown here. In the first example: How would you write a test that doesn\u2019t actually connect to the database? How would you test that the generated report is correct when the backing data is live and could change? How could you test the generated report without breaking that test the instant a customer wants the sales calculation to change slightly? How could you assert that the report is in the correct format without worrying about the correctness of the calculation, and vice versa? In the second example, the answers to the above questions are trivial.","title":"Example of Testable vs. Test-resistant Code"},{"location":"Projects/project-technical-guidelines/#when-to-write-tests-during-development","text":"Some engineers evangelize Test-driven Development (TDD), where tests are written before the logic. Others prefer to write tests after the fact. Both of these have flaws: TDD tends to promote over-modularized, trivial code with tautological tests, and testing after the fact often causes developers to miss obvious cases they may have otherwise remembered. Sometimes the best option is to write the test at the same time as you write the logic, but this requires greater discipline and isn\u2019t compatible with everyone\u2019s thought process. Instead, get in the habit of considering what is best for you at each unit level. Applied this way, each developer can work optimally without needing to enforce a methodology. Some people work best when they write the test first, as it allows them to build a \u201cskeleton\u201d of the logic ahead of time that focuses on business rules. Others keep the testing in mind as they write a unit, and then hammer out the test quickly after. A few programmers switch back and forth between the unit and the test, adding things to each cooperatively. The unit itself might dictate how tests are best written. Units with atomic outputs to assert or static algorithms to check are great candidates for test-first development. Units that require large amounts of input variation or conditional state manipulation often lend themselves well to side-by-side development with the tests. Units dealing with large schema-bound data objects or tight integrations with other services often come naturally to testing afterwards. The important part is to think about the unit before you begin writing it and decide at the time how best to write the test.","title":"When to Write Tests During Development"},{"location":"Projects/project-technical-guidelines/#where-to-go-from-here","text":"If you\u2019re not used to writing testable code, it will take some time to adjust. Do some reading on the SOLID principles and practice applying them in code. Remember to decide how you\u2019re going to a test a unit before you even write it. Perhaps most importantly, if you\u2019re unsure whether a design or unit is testable, ask someone! It\u2019s usually easier to imagine the dependencies and side-effects of a method/design as someone that didn\u2019t write it.","title":"Where to Go from Here"},{"location":"Projects/project-technical-guidelines/#fixing-untestable-code","text":"Often, you\u2019ll find yourself applying these guidelines to existing code instead of greenfield projects. When that happens, aside from the challenges of designing a solution, you\u2019ve also got to deal with whatever decisions came before. That usually involves some level of technical debt. Older code\u2013especially stuff written before these principles became commonplace\u2013tends to have piles of technical debt or, sometimes, just poor design. Trying to write tests against this stuff can seem impossible. However, there is no such thing as \u201cuntestable\u201d code. What you\u2019ve really got is test-resistant code. The difference might seem like an ideological exercise, but the important distinction is your mindset. You should be thinking about how to make the code more testable. Not all code can immediately be made testable, but that\u2019s not a roadblock. What steps can you take right now? How can the code begin to evolve into a testable state? To be clear, the correct answer is usually not to hack tests into test-resistant units. The objective is to either make the code more testable or to begin the initiative of making it more testable. Make a plan for how to get there and execute it. The value of good tests is too high to abandon them when the task seems daunting. Each and every developer is responsible for code quality and cleanliness, and each of us is empowered to design solutions. Having said that, this set of guidelines can\u2019t predict every scenario. It\u2019s possible, if you\u2019re time-limited and it\u2019s truly hideous code, there isn\u2019t enough time to even begin tackling the testability. Even in those cases, you should come up with a plan you (or someone) can pursue later, but what if you don\u2019t even have time to plan ? Regardless of the reason why, if you accept that you\u2019re unable to write tests, you must document the reasons why. Store them with the documentation (both technical and project) so that in the future, when someone comes back to the code you wrote, you\u2019ll save them some time having documented the issues you ran into.","title":"Fixing Untestable Code"},{"location":"Projects/project-technical-guidelines/#testing-public-endpoints-and-critical-paths","text":"When writing tests, the most important units to cover are those directly used to satisfy public endpoints (i.e. the API of your module). These are always business critical, by definition. This will intentionally lead you down a rabbit hole of testing, where you start asserting all of the conditions and dependencies that lead up to the endpoint. A critical path is logic where your application spends the majority of its time; or, more precisely, the paths where optimization efforts are most valuable as they reduce the overall execution time for a given operation. Most of the time, following the above guidance, you will end up covering your critical paths by necessity. However, great care should still be taken to ensure they\u2019re well tested. Optimizations are a frequent source of regressions, and being a critical path implies that regression would be highly detrimental.","title":"Testing Public Endpoints and Critical Paths"},{"location":"Projects/project-technical-guidelines/#avoiding-tautological-tests","text":"A tautological test is one where the test will always pass, usually because the unit and test are essentially equivalent. A rudimentary example in Python: def sum(a, b): return a + b def myTest(): assert(sum(3, 4) == 3 + 4) Notice that the test asserts sum(3, 4) by comparing it to 3 + 4 . Technically, this test is validating that addition is performed correctly in the sum function. However, the condition is only checked by essentially replicating the unit being tested. Even if the test were improved it would still be tautological: def sum(a, b): return a + b def myTest(): assert(sum(3, 4) == 7) The test can never fail, although in this version it\u2019s more because the test is asserting basic language functionality, which can be assumed to not fail. The nature of the sum unit is the problem rather than the test itself. Here\u2019s an example of a non-tautological test that is similar: def calculateGMROII(profit, avgInventoryCost): return profit / avgInventoryCost def myTest(): assert(calculateGMROII(100000, 25000) == 4) This isn\u2019t testing that Python is capable of dividing two numbers. It tests that the GMROII calculation behaves as expected. This is a business case which provides value, and a change in the algorithm for GMROII would likely impact downstream systems. Therefore, this is a useful, non-tautological test even though the calculation is relatively simple.","title":"Avoiding Tautological Tests"},{"location":"Projects/project-technical-guidelines/#where-tautology-gets-tricky","text":"The examples above are trivial for the sake of demonstration. Most of the time, a test becomes tautological through less obvious means. Frequently, a test can appear perfectly valid until you \u201csolve\u201d it down like algebra. If the test solves down to (for example) 2x = x / 0.5 , you\u2019ve got a tautology. A more practical example: WEEK = 201801 TESTDATA = [{'week': WEEK, 'value': 123}, {'week': WEEK, 'value': 456}, {'week': WEEK, 'value': 789}] # This is the unit being tested def dataSlicer(dataLoaderCallback, week, skip, take): data = dataLoaderCallback() dataAtWeek = [d for d in data if d['week'] == week] return dataAtWeek[skip:skip+take] def test_dataLoader(): return TESTDATA def myTest(): sliceData = dataSlicer(test_dataLoader, WEEK, 2, 1) assert(len(sliceData) == 1) assert(sliceData[0] == {'week': WEEK, 'value': 789}) It is left as an exercise to the reader to determine why this one is tautological, but as a hint: simplify the test down until it becomes clearer.","title":"Where Tautology Gets Tricky"},{"location":"Projects/project-technical-guidelines/#avoiding-redundant-tests-against-third-party-or-framework-code","text":"There is no value in writing a test against someone else\u2019s code in your test suite. For example, if there is a math library you\u2019re referencing, don\u2019t bother writing tests against its pow() function. If you\u2019re relying on an extension library to remove entities from an Entity Framework set, there\u2019s no need to test that the entity was actually removed. A module\u2019s tests belong with the module. No tests in your suite should be written against other modules. If you find a test case for another module, for example a scenario that causes unexpected output, you should attempt to add it to that module\u2019s suite. If that\u2019s not possible, don\u2019t worry about it. Write a test for your workaround instead, such that when the module updates and works correctly again, your workaround test will break. Speaking of testing code integrations, while there\u2019s no value in testing the third-party library, there is a ton of value in testing your units\u2019 integration with those libraries. A great example is where your logic must transform the output from a third-party source in some way. Don\u2019t bother testing that the third-party source returns the output as you expect; instead, test that your transformations behave as expected against all important possible outputs (including failures).","title":"Avoiding Redundant Tests Against Third-party or Framework Code"},{"location":"Projects/project-technical-guidelines/#business-coverage-vs-code-coverage","text":"Many of the same shops that espouse TDD will enforce a 100% code coverage standard. In other words, all reachable code must be touched by a test in the suite. This dogmatic approach to testing leaves no room for interpretation or flexibility; often, builds will automatically be rejected if coverage falls below a certain percent. Speaking strictly about the dogmatism and zealotry, there is no value in code coverage-based testing standards. These shops are usually trying to substitute out high-quality engineers and smart people with rigid, rote practice, and it never * works. There is no substitute for good developers. True value comes from looking at business coverage . Business coverage is just making sure that your suite has tests covering all of your business cases, asserting your business rules, and testing that your value-generating logic continues to work as intended. The test spec is created by strong requirements and clear, visible design. Coverage is ensured through due diligence during development and careful peer review focused not just on code quality, but on business value as well. Difference Between Code Coverage and Business Coverage Code coverage only validates that the code units behave as expected. Business coverage is focused on ensuring that the application performs all business-valuable operations in the correct way. They frequently intersect but the mindset behind the two is very different. Calculating business coverage is not as easy as using a tool to spit out a percentage and automatically failing when it falls below a threshold, but it\u2019s infinitely more useful as a signpost of quality. As a result of its subjectivity, there can never be a simple numeric representation. It will instead fall to discipline and practice, which is one of the reasons high-quality engineers are important. Code coverage enthusiasts assume that a 100%-covered solution will by definition ensure 100% business coverage, but this couldn\u2019t be further from the truth. If it completely misses valuable business cases (because the units were not written to address them), it might as well be 0% covered. * Even when people say it works, it\u2019s probably not actually working. A team gets the results it incentivizes, so when it incentivizes code coverage, that\u2019s precisely what developers focus on\u2013to the detriment of code that does anything useful.","title":"Business Coverage vs. Code Coverage"},{"location":"Projects/project-technical-guidelines/#testing-deployments","text":"Just as important as unit-level and module-level testing is deployment-level testing. Testing a deployment means validating that the build/release pipeline is successful and adheres to infrastructure and architecture requirements. As discussed in the later section on CI/CD , the pipeline should be held to several standards. Deployment testing asserts those standards. For example, a deployment test for a service might validate that it deploys idempotently. Unlike other testing granularities, deployment testing requires a suitable environment. Cloud-first applications enjoy an advantage here, as infrastructure can be trivially acquired temporarily to test a pipeline and then disposed after the test. Thanks to products like Kubernetes, however, even private cloud applications can acquire and dispose of infrastructure programmatically. Test results are usually binary: absolute success or complete failure. Deployment tests, however, can sometimes fail due to transient errors inherent to Infrastructure-as-a-Service (IaaS) platforms. Whether it\u2019s Azure, Google, or vSphere, sometimes the infrastructure just cannot be created for unknown, unimportant, and temporary reasons. Because of this, it is not recommended to automate the testing of deployments. It should be scripted , but the success of a pipeline should never rely on the outcome of a deployment test. You may be asking, \u201cif the test shouldn\u2019t be automated, when should it be run?\u201d Deployment testing should be conducted before the module is promoted into a production branch (i.e. master ). Usually, infrastructure deployments into production are gated by a DevOps engineer anyway; it falls to them to execute the provided deployment tests. If the test does not pass, the DevOps engineer will not allow the module to pass. Thankfully, DevOps engineers should be the most knowledgeable source of information about how to write a deployment test, so cooperate with them to fix your infrastructure and/or write a valid test.","title":"Testing Deployments"},{"location":"Projects/project-technical-guidelines/#ci-and-cd","text":"The principles of Continuous Integration (CI) and Continuous Delivery (CD) are vital to getting the maximum value out of our projects. Developers are often unable to accomplish work as fast as possible due to roadblocks or speed bumps in the release process. CI/CD are a central part of every flavor of agile methodology because they remove roadblocks and speed bumps; developers are enabled to move quickly and deploy often. In fact, some teams/projects will even choose to use Continuous Deployment to ensure their latest code is always released when it\u2019s available. For reference, here are some quick definitions: Continuous Integration : the practice of automatically validating that new commits from many authors integrate properly with the existing code base. In other words, CI is making sure that failures or flaws were not introduced when all commits have merged together. Often, it implies frequent merges to minimize the impact of drifting changes between branches over time. Continuous Delivery : the practice of being able to release changes as quickly as possible. Usually, this entails having an up-to-date release artifact ready to go live at all times. While not necessary, it\u2019s considered healthy to release as frequently as possible; this reduces the \u201cblast radius\u201d of the changeset. Continuous Deployment : a corollary to CD, this is the practice of taking the up-to-date release artifact generated by the CD process and automatically releasing it as soon as it\u2019s generated successfully. There is some debate over whether the automatic release must be into production . For the purposes of this document, assume that it must at least deploy into a production staging environment which can trivially become production.","title":"CI and CD"},{"location":"Projects/project-technical-guidelines/#granular-modules-integrate-easily","text":"This is a deceptively simple concept: more granular modules are easier to integrate. That statement has a dual meaning; it refers both to code integration and system integration. Module-level integration testing runs faster and, thanks to lower complexity, proves more decisive. System-level integration (i.e. deploying the module into a running environment) has a lower impact risk if the module is small. Tend toward more granular modules, where possible, to amplify these advantages\u2014though obviously balance it against the increase in system complexity.","title":"Granular Modules Integrate Easily"},{"location":"Projects/project-technical-guidelines/#feature-flags","text":"A common (and healthy) practice in CD is to release changesets as frequently as possible. As mentioned above, this reduces the possible impact of any single changeset. Another benefit, however, is the ability to get features into production to harden ASAP. As the saying goes, no plan survives contact with the enemy, so why not get the plan in front of them as soon as possible? Of course, not every infant feature is ready to be seen by users immediately. Even if there would be a ton of value in having developers test the feature in the production environment, users have a habit of forming an impression the first time they see a feature whether it\u2019s \u201cready\u201d or not. This is where feature flags come into play. They can allow you to toggle features in real-time, gate features by user (or other logic), and expose functionality internally without making it visible to the public, among other things. You get this while also getting to integrate and test the feature where it matters: in production. The implementation of feature flags is outside the scope of this document (though here is a decent reference for more information). Getting into the habit of using them and releasing features earlier will usually result in stronger features, better testing, and smoother user acceptance. This is especially true if you\u2019re able to run pilot programs or A/B testing with your feature flags.","title":"Feature Flags"},{"location":"Projects/project-technical-guidelines/#reproducible-and-deterministic-builds","text":"In order to reduce time-consuming troubleshooting on build/deploy pipeline issues, avoid unpredictable application regressions, and create higher confidence in releases, you should strive for all builds to be reproducible and deterministic. A reproducible build means that the set of conditions (operating environment, variables, code state, etc.) required to build a product can be replicated on demand, for any (reasonable) past or current version of the product. An example of a build that is not reproducible is one that relies on being built on machines that, externally to the pipeline, have been prepared with prerequisites. For example, if the pipeline does not install the .NET Core 3 SDK, but your build requires it, it is not reproducible. Perhaps a more volatile example: if the build relies on resources that could become permanently unavailable (say, downloading a library from someone\u2019s personal website), it is not reproducible. A deterministic build is one where, given that set of reproducible conditions, the build output will be the same every time. Creating a deterministic build can sometimes be tricky. Many modern package managers or similar concepts, like Docker image repositories, allow you to take dependencies not on exact versions but instead on minor, major, or even latest versions. Doing this causes your build to become non-deterministic. Some build pipelines take dependencies on time-sensitive or context-sensitive variables; these are non-deterministic as well. Important Most package managers support some form of lock file or similar mechanism that ensures your packages and all their dependences resolve to deterministic versions. You must take advantage of this mechanism. Too often, a package you depend on will take a loose dependency on another package. Since that indirect package could change without you changing anything in your direct dependencies, your build is no longer reproducible or deterministic. Locking the chain of dependencies resolves the issue as much as can reasonably be expected.","title":"Reproducible and Deterministic Builds"},{"location":"Projects/project-technical-guidelines/#using-continuous-integration-as-a-quality-measure","text":"The more trust you can place into your CI process\u2019s ability to vet your changes, the more trust you can have in deploying. CI/CD is built on enhancing trust to the point of having faith a given deployment will succeed. Defects and regressions will always happen\u2014no process can catch them all\u2014but having confidence that the automated pipelines will take care of the obvious stuff removes a lot of anxiety. This is why your CI pipelines run the automated test suites for your projects, why you merge early and often, and why you perform CI at all. It helps increase confidence in the artifacts being generated and the work being done. You should make sure your CI process is doing everything (within reason) to assert the quality of your code, changes, and artifacts.","title":"Using Continuous Integration as a Quality Measure"},{"location":"Projects/project-technical-guidelines/#reproducible-deterministic-idempotent-deployments","text":"Much like your builds, your deployments should be reproducible and deterministic. For the most part, you can reference the section on builds and apply the same principles here. There are some differences, which this section will focus on. A reproducible deployment differs from a reproducible build only in that the environment you\u2019re reproducing is different. Rather than making sure the build occurs in the same environment, here you\u2019re ensuring that the infrastructure itself is reproducible. A deployment should be able to create its necessary infrastructure, dependencies, and so on. For example, if your application depends on its own RabbitMQ instance, your deployment must be able to reproduce that. A reproducible deployment leads into making a deterministic deployment. Assuming deterministic build artifacts, the resulting deployment should be the same every time. This means that deploying your artifact fresh to the platform should result in the exact same code, infrastructure, backing services/dependencies, networking, and so on, every time. Idempotency here means that, after the first time, deploying the same artifacts again will change nothing. The existing deployment will be unaffected. Without tools that help enforce idempotency (like Kubernetes and its declarative YAML format), it can be difficult to achieve it for your pipelines. Idempotency extends further to the individual components of each deployment. For example, imagine you have executed a deployment pipeline which creates a RabbitMQ instance, two services, and one load balancer in Kubernetes. Then, a minor code change to one service causes a new artifact to be created. When you execute the pipeline with this new artifact, only the changed service should be affected. If you end up with two RabbitMQ instances, four services, and two load balancers\u2026 that\u2019s a problem.","title":"Reproducible, Deterministic, Idempotent Deployments"},{"location":"Projects/project-technical-guidelines/#faster-pipelines-equals-more-productivity","text":"It probably goes without saying, but if your pipelines are fast, you can respond more quickly to issues. No matter how much testing we automate, how often we test deployments, or how diligent our developers are in peer review, sometimes stuff slips through the cracks. Imagine a build/release pipeline that takes 45 minutes to run. Any error, even something so trivial you can correct it in literally one second, requires a minimum of 45 minutes to fix. Imagine how long it takes when it\u2019s something serious or you need to release multiple times. As a rule of thumb, keep CI/CD pipelines under 10 minutes of execution time. It\u2019s fine if they run longer when releasing to an empty environment (i.e. releasing for the first time), but given that the components should be idempotent , later executions should be much quicker.","title":"Faster Pipelines Equals More Productivity"},{"location":"Projects/project-technical-guidelines/#monitoring","text":"Without being able to measure what\u2019s going on in a system, you\u2019re stuck taking shots in the dark when issues crop up. Even worse, you only know issues are happening once they\u2019ve already caused some kind of impact. In complex systems, it can take tens of minutes\u2013sometimes hours\u2013to find which subsystem is even having the issue before you can start debugging. It\u2019s especially nasty when the issue isn\u2019t a wholesale failure, but a transient error, throughput issue, performance problem, or similar functional-but-bad state. Good monitoring is the key to solving the visibility issues that many teams face in production systems.","title":"Monitoring"},{"location":"Projects/project-technical-guidelines/#measure-all-mathematically-significant-metrics","text":"When in doubt, it\u2019s better to over-measure than to under-measure. You should record every metric your application deals in. Whether it\u2019s CPU usages, memory used/free, number of threads, time required, message throughputs, number of files processed in the current operation, or even network utilization, you should be recording it one way or another. Mathematically Significant? A mathematically significant metric is a number that has meaning in comparison or contrast to other numbers, and whose value scales by some mathematical function with variables in the developer\u2019s control. Usually, these are obvious metrics: number of operations per second, CPU usage, count of I/O waits, and so on. The distinction is made to contrast these metrics against \u201csoft\u201d measurements like, for example, a user satisfaction rating. A user satisfaction rating is something that can be measured , but only using an arbitrary scale and without the ability to directly affect the outcomes. The objective of recording all these metrics is to be able to draw conclusions about the state and performance of the system, both the system as a whole and its subsystems. We need these insights both real-time and historically. Imagine a dashboard showing a line graph of your application\u2019s message throughput next to a pie chart of the breakdowns of the types of work sitting in the queue. Beside that is a bar chart showing current throughput per type vs. average throughput per type at this time. You\u2019d be able to quickly pinpoint potential issues in an instant, rather than having to identify that a problem exists and then run a bunch of queries or aggregate a bunch of logs to gather the metrics yourself. Obviously, not every metric in that imaginary dashboard must necessarily come from your application, but for it to exist, all applications must be dutifully reporting metrics. When deciding what to measure, there are some bare minimum metrics that every application should be recording, shown here along with some examples. Utilizations CPU, RAM, disk, network, files, threads, processes, etc. Throughputs Number of messages processed, number of operations performed, number of service calls made, number of records read, etc. Workloads Number of messages waiting, number of files yet to process, count of jobs currently running, etc. Times Time required to complete an operation, time elapsed between heartbeats, time spent on reading files, time spent on waiting for a database, time spent waiting on network communication, uptime, etc. Recording a metric, as discussed below in the Abstract Metric Collection section, should be trivial to your application in terms of resource cost and logic required. Some of these minimum metrics are taken care of for you automatically, depending on the application platform. For example, Kubernetes takes care of measuring most of your application\u2019s resource utilizations in that environment (but not necessarily all). If your application uses RabbitMQ as a message broker, it can be configured to report metrics automatically as well. Value in Non-Mathematical Metrics Despite the focus in this section on mathematically significant metrics, there can certainly be value in other metrics which aren\u2019t as \u201chard\u201d. However, those metrics should be measured outside the scope of your project\u2019s system monitoring effort. Their collection might, for example, be part of the application\u2019s business requirements rather than a system-level architectural necessity. It\u2019s important that you consider your monitoring carefully even when various platforms provide assistance. This guidance takes care not to prescribe any particular solutions because only you, the developer, know best what\u2019s appropriate for your project. Only you can know which metrics you should record that might not be provided by your infrastructure.","title":"Measure all Mathematically Significant Metrics"},{"location":"Projects/project-technical-guidelines/#abstracting-metric-collection-and-dispatch","text":"It may seem a little counter-intuitive to recommend measuring everything , then immediately say your application should do as little measuring as possible. The key phrase, however, is your application . This means that every effort should be made to collect metrics about the application outside of the app\u2019s logic. If the logic must expose a metric (for example, because the data can only be recorded by the logic), it should do so plainly, in a way that does not take dependencies on specific mechanisms, infrastructures, or frameworks. Another process, such as a Kubernetes sidecar or metric collection agent like Telegraf, should be responsible for the collection and dispatch of those metrics. Separating the collection and delivery of metrics from the recording allows the application to remain na\u00efve and operate in many different environments without needing to change. The underlying dispatch of metrics can change live without impacting anything. The recording of metrics won\u2019t potentially have repercussions throughout the application if something fails; the collector can simply pick up and resume when it recovers.","title":"Abstracting Metric Collection and Dispatch"},{"location":"Projects/project-technical-guidelines/#taking-full-advantage-of-time-series-data","text":"An amazing way to create measurable metrics is to record state artifacts at a time interval. Viewing the various state details of an application over time\u2013for example, sampling and recording the number of items in queue every second\u2013can expose tons of insight into the behavior of the system. Assuming you\u2019ve properly abstracted your metric collection , you should be able to trivially record details at a fairly quick interval without causing harm to the system. You want to be able to extrapolate meaning from the data both in realtime and historically simply by aggregating/partitioning the data differently. Let\u2019s look at CPU utilization on a Windows workstation as an example. Viewing it realtime in Task Manager tells you how loaded the CPU is right now and what impacts to expect in the near term. Viewing it over the past 24 hours in Resource Monitor informs you on what your busiest hours are, which helps you plan your future days better. In both cases, the CPU usage is sampled at an interval and recorded; the only difference is the zoom level and aggregation of the samples.","title":"Taking Full Advantage of Time Series Data"},{"location":"Projects/project-technical-guidelines/#establishing-and-maintaining-slas-on-metrics","text":"The biggest goal of the monitoring effort, aside from system visibility, is the ability to define and assert success for a system. Being able to look meaningfully at the monitoring and alerts for a system and say, definitively, \u201cthe system is operating successfully,\u201d has immeasurable value. It can: Increase confidence around the product/system Reduce stress on the team and business Quickly locate where problems actually lie and fix them directly Alleviate conflation between different definitions of success Note To clarify the different definitions of success used here: a product\u2019s success can almost never be 100% determined by how well it satisifed its SLAs and how good its metrics look. A system\u2019s success can be shown much more mathematically. Defining success requires the creation of SLAs. The Distributed Architecture Strategy document goes into much greater detail about the creation and maintenance of SLAs. It is strongly recommended to read that entire document, but especially the section on SLAs and metrics. The SLAs you create for your system are not necessarily user-facing SLAs. Those are usually driven more by the business and sales pipelines, though they often strongly influence your internal SLAs. SLAs can only be created and enforced around good metrics. Even the most basic SLA in the world, uptime percent, can\u2019t be upheld without metrics around service uptimes. Since raw uptime isn\u2019t really valuable\u2013if one replica out of 100 is up, the service is technically still \u201cup\u201d\u2013you need to define an uptime of scale instead. Establishing an uptime of scale necessitates defining what scale means for your system. By the time you\u2019ve followed this thought process through, you\u2019ll have determined several metrics to record and items to monitor.","title":"Establishing and Maintaining SLAs on Metrics"},{"location":"Projects/project-technical-guidelines/#alerting-meaningfully","text":"Most systems alert too much. Developers and operators often err on the side of caution and choose fairly low thresholds to alert on, which leads to an overabundance of noisy alerts that may or may not actually represent a problem. Consumers begin to ignore the alerts, meaning that actual, valuable signals get lost in the ether. This isn\u2019t a theoretical problem; it\u2019s one that has proven itself historically. You\u2019ve seen it or heard of it: e-mail alerts getting automatically shuffled by Outlook rules into folders full of thousands and thousands of unread signals, most of which have less-than-zero value. Make no mistake, however. Those alerts which really do have meaning are astoundingly valuable. This becomes especially true as the complexities of systems, architectures, and ecosystems increase. Being able to get meaningful signals from the various parts of a distributed application (or even a monolithic one) is vital to keeping systems healthy. In turn, this keeps the customers happy. Important This section is primarily discussing infrastructure-, system-, and architecture-level alerts. As covered in the System-Level Alerts vs. Business-Level Alerts sub-section, there is a strong distinction between alerts that are a byproduct of your application vs. those that are part of your application\u2019s business logic.","title":"Alerting Meaningfully"},{"location":"Projects/project-technical-guidelines/#tips-for-alerting","text":"Distinguishing what makes an alert a useful and meaningful signal can sometimes be tricky, but here are a few tips: An alert must be actionable. If you aren\u2019t sure whether something qualifies for alerting, ask someone else what they would do if they received the alert. Since alerts should be actionable, if they can\u2019t tell you what they would do to address the alert, it might not be a valuable alert. As a rule of thumb, when a system has crossed into its yellow or red states (see Dashboards and Stoplights ), a signal should be raised. However, there must be a \u201ccooldown\u201d on this signal to prevent it from over-alerting when the system rapidly cycles between states. Alerts should usually be based on metrics. Specifically, a good signal is when an important metric is deviating significantly from its steady state. Example: a queue-based message processing application has items in the queue, but its throughput is 0 over the most recent window of time. Alerts should contain the right information needed to convey the impact. In other words, you should strive to make the alert self-explanatory, without needing to dig in further to understand the problem. Some examples: Spline reticulator reticulated 100 splines This is only a useful alert if you happen to know that 100 is too many splines . Spline reticulator reticulated 100 splines which exceeds the threshold of 25. This is better. It indicates: The reticulation exceeded a threshold, which is why there was an alert. The magnitude of the excess was 100 vs. 25\u2013a potential indicator of severity or urgency. Spline reticulator reticulated 100 splines which exceeds the threshold of 25. This often indicates that the foo map is too complex or that the bar graph has too many baz nodes. This is even better. However, this much insight won\u2019t always be achievable or even desirable depending on the source of the alerts. Just as important as the metric information is some ability to correlate the alert to the system(s) it\u2019s based on. Ensure alerts are making it to the right people without that list going stale. An alert could be perfect, concise, and informative, but if it doesn\u2019t end up in the right places, it might as well not exist. This often means making sure alerts are going to places abstracted from individual people. For example, alerting into a team-wide or division-wide Slack channel or emailing alerts to a distribution group that represents the concerned parties helps ensure that the right people see them no matter who may come and go within the company. Don\u2019t be afraid to lean on alerting frameworks for help. Getting the right quantity and quality from your alerts can be tough. These are solved problems; there\u2019s no reason to bang your head against the wall. Grafana, for example, has a decent alerting rules engine built-in which is able to directly pull metrics from time-series databases like InfluxDB.","title":"Tips for Alerting"},{"location":"Projects/project-technical-guidelines/#application-logic-should-almost-never-alert","text":"Application logic should almost never create a system-level alert directly. Only in very rare scenarios should it even be able to do so. Your code should be handling errors gracefully (remember Postel\u2019s Law ), so the sort of explosions that you would need to signal should be infrastructural\u2013things that the application platform would alert on instead. The responsibility of observing the system for critical states lies outside of your application, for the most part. If an error is so severe that it cannot be handled, it\u2019s usually best to just crash. For the same reasons that your application should be recording metrics plainly, so too should it be failing plainly. The application platform and DevOps ecosystem must be robust enough to create the alerts around frequent application crashes\u2013after all, you can\u2019t predict every reason your app might go down. Let\u2019s look at a practical scenario. Pretend your application is a report runner; its purpose is to receive messages to run reports, run the reports, and write the output to a storage location. As an example, the app might crash if it hits a defined retry limit on trying to access its critical storage for the results. The application cannot continue when it\u2019s unable to write the results of its operations. In this example, crashing may even fix the error, depending on the application platform and storage medium. Transient errors or temporary mount disruptions can sometimes be corrected with an application restart. That said, there are rare exceptions where your application may need to alert directly. Use your best judgement, given the guidance of this document and your team. Most of the time, if you feel like your application must alert, it\u2019s because the alerts fall into the category discussed in the next sub-section.","title":"Application Logic Should (Almost) Never Alert"},{"location":"Projects/project-technical-guidelines/#system-level-alerts-vs-business-level-alerts","text":"Most of the discussion in this section has been around metric-based, system-level alerting. Part of the reason your application shouldn\u2019t be doing its own alerting is the implication that it must therefore be concerning itself with consuming its own metrics and/or taking a dependency on an alerting mechanism. This goes against the guidance in the Abstracting Metric Collection section. However, there is a type of alerting that doesn\u2019t fit the same mold: business-level alerting . Unlike the infrastructure-level signals this section has focused on so far, business-level alerts are driven by business rules and requirements. These alerts are not a byproduct of your application, they are your application . You must be very careful to distinguish these alert types, because they are very different use cases. As an example, pretend your application is part of an ETL platform. Another application has extracted the data; yours is responsible for asserting the completeness of that data\u2013as in, whether the source system provided all of the necessary data, based on a business definition of completeness. When your application sends an alert that the data is likely not complete, that alert is the output of your application. It is not the same kind of alerting as, say, signaling that the application is using 100% of its allocated RAM. The audience is different, the delivery mechanism is probably different, and the content is different. These are very likely two separate systems, serving two separate purposes. Business-level alerts are fine, so long as they fall within the requirements and design of the application. Many of the same tips in this section could probably apply to them (e.g. they should be actionable, based on metrics, etc.) but there is much more room for flexibility here. Whatever is determined to provide value to the user is exactly how much alerting your application logic should do when it\u2019s business-level alerting.","title":"System-Level Alerts vs. Business-Level Alerts"},{"location":"Projects/project-technical-guidelines/#dashboards-and-stoplights","text":"Important Please read the Distributed Architecture Strategy section on Stoplights before proceeding! During the design of any system, one of the first-class items should be a well-defined collection of dashboards designed to assist support and maintenance out of the gate. Using the dashboards, one should be able to answer many questions about the operation of the system and quickly, proactively diagnose issues. Similarly, the collection of metrics and states that will define your steady state must be defined during the design effort. These SLO\u2019s will help you define your stoplights. What conditions must be met for it to qualify as green ? What conditions cause it to be in a red state? (For guidance to others) what does a yellow state usually mean about the system? This at-a-glance discoverability has incalculable value, and cannot be anything less than a primary concern of every effort. It\u2019s far too easy to put dashboards and stoplights aside when the project is on a time crunch or trying to run with a thin crew, believing that the system will run fine without them. Perhaps it will, for a bit, but any production system that\u2019s intended to last will eventually go down. Every minute that system is down, it\u2019s costing you (either directly in revenue, or indirectly in damage to customer and stakeholder trust), and you\u2019ll wish you had all the tools available to correct the problem ASAP.","title":"Dashboards and Stoplights"},{"location":"Projects/project-technical-guidelines/#logging","text":"Keeping clean, informative logs is one of the most important things developers can do to make a system more maintainable. However, it is perhaps one of the most challenging things to get \u201cright\u201d, second only to recording metrics. Too many logs (or too verbose) just leads to unreadable noise. Too few (or too sparse) and there are gaps in information and history that prevent issues from being traced. This section should help define a sweet spot of useful logs that become a reliable source of troubleshooting and auditing assistance.","title":"Logging"},{"location":"Projects/project-technical-guidelines/#log-almost-everything","text":"\u2026","title":"Log (Almost) Everything"},{"location":"Projects/project-technical-guidelines/#minimize-application-logging-code","text":"\u2026","title":"Minimize Application Logging Code"},{"location":"Projects/project-technical-guidelines/#actionable-log-data","text":"\u2026","title":"Actionable Log Data"},{"location":"Projects/project-technical-guidelines/#meaningful-log-levels","text":"Most logging libraries support a concept of log levels\u2014such as trace , debug , info , warning , and error ; or sometimes represented as numbers\u2014to help categorize and often thin out the amount of logs being written or consumed at a given time. Given that you should log almost everything , having a somewhat-standard way to denote the intention and importance of a log message is very useful. Otherwise, the number of logs can become overwhelming. It is strongly recommended that you rely upon log levels in your logging implementation. However, unless the levels have well-established meaning, their intended use of reducing the amount of work one must do while spelunking through the logs is eliminated. Here are a few guidelines for making log levels meaningful in your application: Make sure the log level of your application is configurable. Most of the time, the lowest levels of your logging are very verbose. The trace and debug levels, for example, often spit out several messages for every related info event. That highly granular detail is not usually necessary for a system running in production. It\u2019s wise to make the log level configurable such that the verbose logs are not written unless the application is specially configured to output them. Often, dev and QA environments will leave the verbose logging on. Production environments might turn it on occasionally if detailed troubleshooting is required. In the documentation, establish what types of events and messages should fall into which levels. The primary cause of meaningless log levels is when developers don\u2019t understand what each level should be used for, so information ends up poorly categorized. As a default, use this: trace is used for very fine-grained debugging information that often goes as deep as emitting a log when a method is called or even when specific lines of code are about to be executed. Obviously not every method or every line of code should log to the trace log, but important or impactful ones can be. Ex: \"parseJsonForMessage() called\" debug often represents critical state changes, important logical entry points or exit points, or data dumps that might be useful for debugging later. Ex: \"Begin parsing JSON for message 1234\" info is usually the lowest active logging level by default, and represents audit information, events and messages of interest, and other messages that help viewers understand standard process flow. Ex: \"Received message 1234\" warning is for events or messages that are slightly problematic or potentially concerning, but not immediately erroneous. However, as they often indicate future errors, they are quite important. error is straightforward: it denotes errors. An error may not necessarily be fatal (though it\u2019s very useful to make fatal ones obvious), but any time an erroneous state is encountered it should be logged. Non-fatal errors in user input/state and warnings often overlap in concern. It can be helpful to treat non-fatal user issues as warnings to help system-level errors stand out. Ex: \"Message 1234 was larger than the maximium size of 1024 KB and could not be processed.\" Ensure that whatever method is used for digging through log files makes the log level visible and searchable\u2014whether it\u2019s something as simple as flat text files or as complex as Splunk. For example, flat text logs should include the log level in the message so that advanced text editors like VS Code, Sublime Text, or Notepad++ can highlight and find it within the log. Ex: 2020-12-08T16:56:35.9319564-06:00 Info: Received message 1234 Ex: 2020-12-08T16:56:35.9319564-06:00 Error: Message 1234 was larger than the maximum size of 1024 KB and could not be processed.\" Obviously, be consistent with how log levels are treated and what sorts of messages fall into which levels.","title":"Meaningful Log Levels"},{"location":"Projects/project-technical-guidelines/#searchable-logs","text":"\u2026","title":"Searchable Logs"},{"location":"Projects/project-technical-guidelines/#logs-are-not-metrics-metrics-are-not-logs","text":"\u2026","title":"Logs Are Not Metrics, Metrics Are Not Logs"},{"location":"Projects/project-technical-guidelines/#patterns-and-practice","text":"Apply SOLID principles, but don\u2019t be dogmatic Be monolithic when you need to be; be modular when it matters Consume services responsibly; provide responsible services Be creative where needed and formulaic to save time YAGNI Get WET before you worry about being DRY Expose configuration externally to minimize the need for recompilation and redeployment Avoid technical debt, and plan immediately to pay any you accrue off","title":"Patterns and Practice"},{"location":"Projects/project-technical-guidelines/#infrastructure","text":"Express infrastructure requirements as code Make your application highly available, load balanced, fault tolerant, and scalable Consume resources responsibly Store secrets securely Have a back-out strategy for bad/failed deployments Plan for emergency deployment of critical services\u2014or document the lack of plan/capability","title":"Infrastructure"},{"location":"Projects/project-technical-guidelines/#documentation-and-design","text":"Create a design document that demonstrates how requirements will be satisfied Address many of the same concerns as this document in your design doc Record the intangibles for future reference to avoid tribal knowledge Keep design, requirements, and other project docs centralized and grouped together Treat API documentation as a first-class deliverable Write self-documenting code Automate doc creation and publishing","title":"Documentation and Design"}]}